# Thesis
## Algorithm-based development of an instrument to measure self-perceived data literacy

The increasing relevance of competent and critical handling of data in society not only makes it 
possible to record this competence, but also makes the self-perception of this competence increasingly important.
Previous approaches considered this competence primarily against the specific background of individual target
groups, jobs or roles (Cui et al., 2023). In addition, only a few explicitly refer to the general 
population (Cui et al., 2023; Carmi et al., 2020). In view of the various theoretical approaches, 
there is a need for a uniform definition of data literacy in order to create comparability.

Our aim is therefore to derive a holistic definition based on these approaches and to develop a 
questionnaire for self-perception of one's own data literacy. To this end, the decisive factors from previous 
definitions and operationalizations in various disciplines are brought together. Cognitive interviews are 
conducted iteratively to create and refine the items. The items are then selected using algorithm-based
item selection. The facets of data literacy are tested for factorial, discriminant, convergent and congruent
incremental validity in order to promote a differentiated understanding of the construct. 
Construct and criterion validity are tested using correlations and hierarchical regression analyses,
while cross-validation checks the robustness of the instrument.
Based on a cross-sectional online survey, we will examine a representative sample from the general population. 

Limitations result from the cross-sectional design and the heuristic item reduction, which prevent causal 
conclusions and limit predictive validity. The heterogeneous nature of the construct makes the development
of a global instrument and understanding of all participants difficult.
The self-assessment questionnaire shall enable a holistic assessment of perceived competence for further studies, 
for example by comparing self-assessment and actual performance.

## Where am I - and what is missing:

I am currently conducting cognitive interviews on the items in order to adjust and better assess 
their selectivity and difficulty. I have also decided to include additional circular items for each 
factor in the item pool in order to reduce any self-rating biases. I would also like to pre-register 
my work soon before I start collecting data. The aim is not to have the entire instrument ready at the end, 
but the three core factors. I deliberately leave out two factors that are more difficult and where I think 
CAT is a good way to work on them afterwards. Therefore, I am designing a small set of items for these two factors, 
just to see how a homogeneous part of the sample performs on these factors, and perhaps to use this data as 
preliminary results to work on further after this study.

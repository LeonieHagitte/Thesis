---
title: "Development of a German Self-Rating Instrument for Data Literacy"
subtitle: "An Algorithm-based Approach to Scale Development"
author: "Leonie Hagitte"
format:
  revealjs: 
    theme: [default, custom.scss]
    slide-number: c/t
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Colloquium presentation by Leonie Hagitte"
    title-slide-attributes:
      data-background-image: cover.png
      data-background-size:  contain 
      data-background-opacity: ".2"
---

## Outline

\

-   Abstract
-   Background
-   Method & Planned Analysis
-   Open Questions
-   References

## Abstract

## What is Data Literacy?

\

::: scriptsize
::: columns
::: {.column width="60%"}
**Risdale et al. 2015:**\
Data literacy is the ability to collect, manage, evaluate, and apply data, in a critical manner.\
\
**Carmi et al. 2020:**\
Data citizenship that includes data thinking, data doing, and data participation. This framework highlights the importance of citizens' critical understanding and proactive daily engagement with data.
:::

::: {.column width="40%"}
![](DataLiteracySchüller.png){.lightbox}
:::
:::

**Wolff et al. 2016:**\
Data literacy is the ability to ask and answer real-world questions using large and small datasets through an inquiry process, while considering the ethical use of data. It involves core practical and creative skills, and the capacity to expand knowledge of specialized data handling techniques based on objectives. These skills include selecting, cleaning, analyzing, visualizing, critiquing, and interpreting data, as well as communicating insights and using data in design processes.
:::

## Conceptual Integration
\

::: scriptsize
[*Conceptual integration, is a methodological approach used to consolidate diverse perspectives and create a unified understanding of a concept that lacks a universally agreed-upon definition. This process involves examining various definitions of the concept from existing literature and extracting shared elements to formulate a comprehensive and inclusive definition.*]{.highlight}
:::
\

::: small

- Cui et al. 2023 summarized  good number of literature on the topic - Reporting what facets are incorporated in the several data literacy definitions used.

- I started creating a frequency table to get at the shared factors with the idea of finding a universal definition via conceptual integration
:::

## Data Literacy

::::{.columns}

:::{.column}
::: normalsize
I arrived at five core facets that are also very prominent in the most definitions in the literature (Cui et al., 2023).\
\
:::
:::

:::{.column}
![](DL Measurement Model.png){.lightbox}
:::
::::
::: small
I further divided them into [“consumer”]{.highlight} facets ([Comprehension,Evaluation & Integration]{.highlight}), that are relevant for nearly every person in society, from citizens up.\
And [“producer”]{.highlight} facets ([Communication & Statistics]{.highlight}), that are mainly relevant for people, actively working with data. 
:::
## Nomological Net

\
![](DL_Nomological_Net.png){.lightbox}

## Research Question

\
*Does the proposed set of items effectively capture the latent factor structure of self-rated data literacy, and can the created scale be considered a reliable and valid measure of this construct?*\
\

::: small
-   *Hypothesis 1: The proposed set of items will demonstrate a good model fit.*
-   *Hypothesis 2: The structure of the initial analysis will also be found in a different sample.*
-   *Hypothesis 3: The final questionnaire will show good discriminant validity.*
:::

## Methods & Planned Analysis

\

::: normalsize
::: columns
::: {.column width="50%"}
[Study Design]{.highlight}\
Item Creation\
Item Selection\
Validation
:::

::: {.column .scriptsize width="50%"}
-   Cross-sectional online survey
-   Sample from the general population
    -   Ca. 700 participants
-   Self-rated data literacy scale
-   Demographic questions
-   Additional validation measures
:::
:::
:::

## Methods & Planned Analysis

\

::: normalsize
::: columns
::: {.column width="50%"}
Study Design\
[Item Creation]{.highlight}\
Item Selection\
Validation
:::

::: {.column .scriptsize width="50%"}
-   Literature review to create items
-   Cognitive interviews to refine potential items
    -   71 items total
:::
:::
:::

## Methods & Planned Analysis

\

::: normalsize
::: columns
::: {.column width="50%"}
Study Design\
Item Creation\
[Item Selection]{.highlight}\
Validation
:::

::: {.column .scriptsize width="50%"}
-   Quantitative survey
    -   Data Literacy items
    -   SWE-IV-16 (Behm, 2018)
    -   NFC-K (Beißert et al., 2015)
    -   ICT-SC25, general domain (Schauffel et al., 2021)
    -   BFI-10, openness & conscientiousness (Rammstedt et al., 2014)
-   Algorithm-based item selection
:::
:::
:::

## Methods & Planned Analysis

\

::: normalsize
::: columns
::: {.column width="50%"}
Study Design\
Item Creation</span>\
Item Selection\
[Validation]{.highlight}
:::

::: {.column .scriptsize width="50%"}
-   Construct validity
    -   Confirmatory factor analysis (CFA)
    -   Correlation analyses with related constructs
-   Crossvalidation
:::
:::
:::

## Poweranalyse 
\

::: small

- Similar literature had sample sizes of N=500 up to N=1000 could be found (Algner & Lorenz, 2022; Remmert et al.,2022; Schneider et al.,2024).
- But highly dependent on the factor structure!
:::

## Poweranalyse 

::: normalsize
Poweranalyse with ‘semPower’(Moshagen & Bader, 2023)
:::

```{r, echo=TRUE}
library(semPower)
powerLI <- semPower.powerLI(
  type = 'a-priori', alpha = .05, power = .80,
  comparison = 'configural',
  nullEffect = 'metric',
  nIndicator = c(4, 4, 4, 4, 4),
  loadM = c(.6, .7, .6, .7, .7),
  autocorResiduals = TRUE
)
```
::: scriptsize
- This has also been done with simulation of several model estimations, via the argument 'simulatedPower = TRUE'
:::

## Poweranalyse 

```{r}
#| label: Summary Power Analysis
#| tbl-cap: "Example"
#| tbl-subcap: 
#|   - "table"
#|   - "figure" 
#| layout-ncol: 2
summary(powerLI)
```

## Item Creation 
\

::: normalsize
Iterative Cognitive Interviews
:::
::: small
- I conducted 10 Interviews mainly with people from the general public  
- To examine whether participants can easily understand and interpret items the way they were intended—which aids in identifying concrete suggestions for improving items and response options (Hughes, 2004).  
- helpful to identify issues with items and test revised versions until a good match between each item’s intent and participants’ interpretations is achieved(Scott et al., 2021).  
- helped also to refine items that ask for "critical incidents"
:::

## Survey
\

::: small
Each participant will answer 38 items of the 71, that are randomly selected. To answer the items, respondents indicate their agreement on a five-point Likert scale (1 = "strongly disagree", 2 = "somewhat disagree", 3 = "neither agree nor disagree", 4 = "somewhat agree", 5 = "strongly agree”) with a “don’t know” option. 
\

- 16 items of the SWE-IV-16 scale  
- 4 items of the NFC-K scale  
- 5 items general items of the ICT Self-Concept Scale (ICT-SC25)  
- 2 openness items & 2 conscientiousness items from the BFI-10 scale  
- 3 instructed response items and 1 seriousness check item  
- demographic questions (Age, gender, education, occupation...)  
:::

## Algorithm based Item selection
::: small
\
'Stuart' can construct subsets from a pool of items by using ant-colony- optimization, genetic algorithms, brute force, or random sampling. (Schultze, 2022) 

- Classical approaches: items are evaluated within the overall item pool and are then often selected based on their individual properties (e.g. difficulty, discrimination, item-scale-correlations).
- Automatic approach: opposite perspective- repeatedly estimating CFAs for a multitude of possible item-combinations.
  - A pool of items with some constraints and the goal to find the one combination that best fits the suggested         purpose of the final scale, not based on individual properties
- I want to optimise for model fit and reliability (RMSEA, SRMR, CFI & McDonalds ω) as well as variability in the difficulty of items.

:::

## Ant Colonization Algorithm
\

::: small
- Most commonly used meta-heuristic used in automated item-selection
- ACOs are based on observations of foraging ants connecting the food source with the nest (e.g.: Deneubourg et al., 1983).
- Find the most effective path between the two by leaving pheromones, making it more attractive to following ants
- Applied to item selection - an ant chooses the “path” between different items, leaving a trail of "pheromones".
- The probabilistic decision running from one item to the next, makes it unlikely that same solutions come up twice
- often chosen items will become more likely with every turn, because they are part of good solutions (Schultze, 2017)

:::

## Genetic Algorithms
\

::: small
- GAs are based on Darwinian evolution principles – selection, crossover, mutation and survival of the fittest.
- The algorithm selects random subsamples of items as an individual solution. Good Solutions survive and procreate.
- For the next generation, two item sets are combined to produce a "child"
- "Mutation" adds a degree of randomness to this process.
- repeated until the algorithm converges into a solution that is dominant
:::

## How to Inform the Algorithm?

[Some questions need to be answered by the researchers!]{.highlight}

::: small 
- Which item belongs where?
  - deriving a structure of factors from the item pool from theory. 
- What should my scale look like?
  - how many items is the scale supposed to have? 
- What is a “good” scale?
  - What are important content properties of the scale? 
:::

## Analysis under Construction

```{r, eval=FALSE, echo=TRUE}
library(lavaan)
library(stuart)
split <- holdout(data, prop = 0.5, grouping = NULL, seed = 2024, determined = NULL)
lapply(split, nrow) #check size of sample

gene(
  data,                 # Your data frame containing the observed variables
  factor.structure,     # A list defining the factor structure of your model
  capacity = NULL,      # Optional, capacity constraint for factors
  item.weights = NULL,  # Optional, initial weights for each indicator
  item.invariance = "congeneric",  # Specify the type of invariance assumption for items
  repeated.measures = NULL,        # Optional, specify repeated measures design
  grouping = NULL,                 # Optional, specify grouping variable for multigroup analysis
  group.invariance = "strict",     # Specify the type of invariance assumption for multigroup analysis
  comparisons = NULL,              # Optional, specify comparisons for multigroup analysis
  auxiliary = NULL,                # Optional, specify auxiliary variables
  use.order = FALSE,               # Whether to use order constraints
  software = "lavaan",             # Specify the SEM software to be used
  cores = NULL,                    # Number of CPU cores to be used for parallel processing
  objective = NULL,                # Optional, specify a custom objective function
  ignore.errors = FALSE,           # Whether to ignore errors during fitness evaluation
  burnin = 5,                      # Number of generations for burn-in phase
  generations = 256,               # Total number of generations
  individuals = 64,                # Number of individuals in each generation
  selection = "tournament",        # Selection method for genetic algorithm
  selection.pressure = NULL,       # Pressure for tournament selection
  elitism = NULL,                  # Rate of elitism
  reproduction = 0.5,               # Proportion of individuals reproduced in each generation
  mutation = 0.05,                  # Mutation rate
  mating.index = 0,                 # Index of mating type
  mating.size = 0.25,               # Proportion of mating pool size
  mating.criterion = "similarity",  # Criterion for selecting mates
  immigration = 0,                  # Proportion of immigrants in each generation
  convergence.criterion = "geno.between",  # Convergence criterion
  tolerance = NULL,                 # Tolerance for convergence
  reinit.n = 1,                     # Number of reinitializations
  reinit.criterion = convergence.criterion,  # Criterion for reinitialization
  reinit.tolerance = NULL,          # Tolerance for reinitialization
  reinit.prop = 0.75,               # Proportion of population reinitialized
  schedule = "run",                 # Schedule for genetic algorithm
  analysis.options = NULL,          # Additional options for analysis
  suppress.model = FALSE,           # Whether to suppress model output
  seed = NULL,                      # Seed for random number generation
  filename = NULL                   # Optional, filename for saving results
)

kfold(
  type,
  k = 5,
  max.invariance = "strict",
  seed = NULL,
  seeded.search = TRUE,
  ...,
  remove.details = TRUE
)

```

## Data Collection Factor 1{.scrollable}

::::{.columns}
:::{.column}
![](F1_1.png){width=425}
![](F1_3.png){width=400}
:::
:::{.column}
![](F1_2.png){width=410}

![](F1_4.png){width=400}
:::

::::
## Data Collection Factor 2{.scrollable}

::::{.columns}

:::{.column}
![](F2_1.png)

![](F2_2.png)
![](F2_5.png)

:::

:::{.column}
![](F2_3.png)
![](F2_4.png)
![](F2_6.png)
:::

::::

## Data Collection Factor 3{.scrollable}
::::{.columns}

:::{.column}
![](F3_1.png)
![](F3_2.png)
:::
:::{.column}
![](F3_3.png)
:::

::::
## Data Collection Factor 4{.scrollable}
::::{.columns}

:::{.column}
![](F4_1.png)
![](F4_2.png)
:::
:::{.column}
![](F4_3.png)
:::

::::
## Data Collection Factor 5{.scrollable}
::::{.columns}

:::{.column}

![](F5_1.png)
![](F5_2.png)
![](F5_3.png)
![](52_4.png)
![](F5_5.png)
![](F5_7.png)
:::
:::{.column}
![](F5_8.png)
![](F5_9.png)
![](F5_10.png)
![](F5_11.png)
:::

::::
## Validation Analyses
::: small
- Construct validity is evaluated via CFA and correlation analyses with related constructs
- I expect a moderate to high positive correlation with the SWE-IV-16 (Behm, 2018)
- I expect a moderate to high positive correlation with the the ICT-SC25 (Schauffel et al., 2021)
- I expect a small to moderate positive correlation with the NFC-K (Beißert et al., 2015)
- I expect a small to moderate positive correlation with the the BFI-10 (Rammstedt et al., 2014)
- sample will be divided into k subsets using the ‘holdout' function in ‘stuart’
- For crossvalidation the ‘kfold’ function in ‘stuart’, assesses also the invariance
:::

## Technical Stuff just for Reference
\

::: small
- I want to make this as adherent to Open Science practices as possible (and practice them more).
  - Thus, i preregistered at Zenodo: https://doi.org/10.5281/zenodo.11196495
  - I use the reproducibility workflow (Peikert et al.,2021)/ the "repro" package of Aaron Peikert
  - I write the Thesis in a GitHub repository, it is created as pdf file, as well as a gh-webpage

:::

## Questions
\

::: small
- How would you approach the data collection - balancing the sample - target group?
- ACO or GA ?
- How to rate "don't know" option - especially difficult?
- Are four items per factor to few?
- What is a good model?
  - For what would you optimise or look for? 
- What data pre-processing would you consider?
- "normal" crossvalidation or kfold crossvalidation?
:::

## References {.scrollable}

::: tiny
-   Algner, M., & Lorenz, T. (2022). You’re Prettier When You Smile: Construction and Validation of a Questionnaire to Assess Microaggressions Against Women in the Workplace. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.809862\
-   Behm, T. (2018). SWE-IV-16. Skala zur Erfassung der Informationsverhaltensbezogenen Selbstwirksamkeitserwartung \[Verfahrensdokumentation, Fragebogen deutsche und englische Version (SES-IB-16)\]. In Leibniz-Institut für Psychologie (ZPID) (Hrsg.), Open Test Archive. Trier: ZPID. 
https://doi.org/10.23668/psycharchives.4598\
-   Beißert, H., Köhler, M., Rempel, M., & Beierlein, C. (2015). Deutschsprachige Kurzskala zur Messung des Konstrukts Need for Cognition NFC-K. Zusammenstellung sozialwissenschaftlicher Items und Skalen (ZIS). https://doi.org/10.6102/zis230
-   Bless, H., Wänke, M., Bohner, G., Fellhauer, R. F., et al. (1994). Need for Cognition: Eine Skala zur Erfassung von Engagement und Freude bei Denkaufgaben \[Need for cognition: A scale measuring engagement and happiness in cognitive tasks\]. Zeitschrift für Sozialpsychologie, 25(2), 147–154.\
-   Brand-Gruwel, S., Wopereis, I., & Walraven, A. (2009). A descriptive model of information problem solving while using internet. Computers & Education, 53(4), 1207–1217. https://doi.org/10.1016/j.compedu.2009.06.004\
-   Cacioppo, J. T., & Petty, R. E. (1982). The need for cognition. Journal of Personality and Social Psychology, 42(1), 116–131. https://doi.org/10.1037/0022-3514.42.1.116\
-   Carmi, E., Yates, S. J., Lockley, E., & Pawluczuk, A. (2020). Data citizenship: Re- thinking data literacy in the age of disinformation, misinformation, and mal- information. Internet Policy Review, 9(2). https://doi.org/10.14763/2020.2.1481\
-   Cui, Y., Chen, F., Lutsyk, A., Leighton, J., & Cutumisu, M. (2023). Data liter- acy assessments: A systematic literature review. Assessment in Education: Principles, Policy & Practice, 30, 1–21. https://doi.org/10.1080/0969594X.2023\
-   Hu, L., & Bentler, P. (1999). Cutoff criteria for fit indexes in covariance structure analysis: conventional criteria versus new alternatives. Structural Equation Modeling, 6(1), 1–55. https://doi.org/10.1080/10705519909540118\
-   Meredith, W. (1993). Measurement invariance, factor analysis and factorial invariance. Psychometrika, 58(4), 525–543. https://doi.org/10.1007/BF02294825\
-   Moshagen, M., & Bader, M. (2023). Package ‘semPower’ (Version 1.0.0) \[Manual\]. CRAN. https://github.com/moshagen/semPower\
-   Rammstedt, B., & John, O. P. (2007). Measuring personality in one minute or less: A 10-item short version of the Big Five Inventory in English and German. Journal of Research in Personality, 41(1), 203–212. https://doi.org/10.1016/j.jrp.2006.02.001\
-   Rammstedt, B., Kemper, C. J., Klein, M. C., Beierlein, C., & Kovaleva, A. (2014). Big Five Inventory (BFI-10). Zusammenstellung sozialwissenschaftlicher Items und Skalen (ZIS). https://doi.org/10.6102/zis76\
-   Remmert, N., Schmidt, K. M. B., Mussel, P., Hagel, M. L., & Eid, M. (2022). The Berlin Misophonia Questionnaire Revised (BMQ-R): Development and validation of a symptom-oriented diagnostical instrument for the measurement of misophonia. PLoS ONE, 17(6), e0269428. https://doi.org/10.1371/journal.pone.0269428\
-   Rosseel, Y. (2012). lavaan: an R package for structural equation modeling. Journal of Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02\
-   Schauffel, N., Schmidt, I., Peiffer, H., & Ellwart, T. (2021). ICT Self-Concept Scale (ICT-SC25). Zusammenstellung sozialwissenschaftlicher Items und Skalen (ZIS). https://doi.org/10.6102/zis308_exz\
-   Schmalbach B, Irmer J, Schultze M (2019). *ezCutoffs: Fit Measure Cutoffs in SEM*. R package version 1.0.1, https://CRAN.R-project.org/package=ezCutoffs\
-   Schneider, J., Striebing, C., Hochfeld, K., & Lorenz, T. (2024). Establishing Circularity: Development and Validation of the Circular Work Value Scale (CWVS). Frontiers in Psychology, 15. https://doi.org/10.3389/fpsyg.2024.1296282\
-   Schultze, M. (2020). stuart: Subtests Using Algorithmic Rummaging Techniques. R-Package. Available online at: https://cran.r-project.org/web/packages/stuart/index.html
:::

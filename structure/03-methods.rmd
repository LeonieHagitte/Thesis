# Methods

## 2. Stage Two: Item Selection and Construct Validity
   - Conduct a quantitative survey including the original item pool, demographics, and validation measures.
   - Utilize automated item selection procedures to reduce the item pool.
   - Utilize an algorithm to select items from the original item pool and develop the final version of the scale.
   - Use an algorithm implemented in the R package "stuart" with predefined datasets split into training and test datasets.
   - Evaluate solutions against an objective function consisting of model fit criteria and composite reliability.
   - Evaluate model fit using standard recommendations proposed by Hu and Bentler (1999), including $\chi^2$ significance testing   and fit indices.
   - ez cutoffs. ; discussing dynamic fit indices
   - Hypothesize relationships between the newly created measure and related constructs.
   - Use distinct but conceptually similar instruments for validation purposes.
   - Cross-validate findings using a split-sample approach?
   - Evaluate solutions against an objective function consisting of model fit criteria and composite reliability.
   - Validate findings using k-fold cross-validation with the dataset?
   - Conduct confirmatory factor analysis (CFA) with the R package "lavaan."

## 3. Stage Three: External and Construct Validity Testing 
   - Perform bivariate correlation analyses with relevant related constructs to establish external validity.
   - Investigate construct validity through multiple regression analysis, controlling for other variables.
   - Discuss specific hypotheses regarding associations between the measure and related constructs.

## Item Creation  
A literature review was done to create items and then ten cognitive interviews were held to refine those potential items. The interviews were administered iteratively to refine the items every time a bit more. We will treat the first 25 participants like a pilot, to check for potential problems in the survey.

## Rationale for Measurement Model
One decision that needs to be done by the researcher before the item selection with 'stuart', is the design of the measurement model, or how many items per factor the final scale should have.
This scale will be created as parsimonious as possible, while ensuring that there is a real model fit to be estimated. So the decision in this case was of statistical nature, while with three items per factor, the model would have been just identified, four items per factor is the most parsimonious choice, where there is already a real model fit, that can be assessed in the end.

## Sample 

The participants are recruited on several online social media platforms. The participation is voluntary. We conducted a-priori power analysis to determine the necessary sample size for the structural equation modelling. We used the ‘semPower’ package in R (Moshagen & Bader, 2023) and also took a look into studies with similar goals and methods. The power analysis gave an analytical estimate for N=645, and a simulated estimate N=613, for the respective measurement model. In the literature sample sizes of N=500 up to N=1000 could be found (Algner & Lorenz, 2022; Remmert et al.,2022; Schneider et al.,2024). So the optimal sample size, we are aiming at, lies somewhere between those numbers.

Participants have to be of legal age, to be included in the study. Furthermore, attention check questions are included (three instructed response items and one seriousness check item, at the end) within the survey to assess participants' attentiveness. Participants who fail to correctly answer two out of the four attention check questions will be excluded from the analysis.

The sample for this study comprised XXX participants (M=, SD=). Within the sample, XXX% identified as female, XXX% as male, and xxx% did not identify with binary gender categories. All participants were aged 18 and above. Regarding education, all participants exhibited a [insert educational level- specifying the range or types of educational levels observed in the sample]. Among the participants, n= reported higher knowledge on items x, x, x, leading to their selection for an additional set of items as a preliminary survey for factors four and five.
The study encompassed every sector within the occupational classification (Bundesagentur für Arbeit, 2020), ensuring comprehensive representation. Conducted in German, the participation in the study was entirely voluntary, with no external incentives provided. The recruitment of participants was carried out through a combination of personal and professional networks, along with outreach on various online social media platforms.

Our study sample serves as a focal point for comparison against the demographic landscape of the general public in Germany. In 2022, the mean age of the German population was 44.6 years, with 45,457,000 individuals engaged in employment. Educational backgrounds varied (XXX), and for gender distribution, the split was nearly 50/50 (41,616,473 males and 42,816,197 females) according to the Statistisches Bundesamt (source: https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Bevoelkerungsstand/Tabellen/liste-zensus-geschlecht-staatsangehoerigkeit.html#651186).

## Open Science Standards 
This project uses the reproducibility workflow proposed by @Peikert2021. Docker and renv work together to create a reproducible and portable
environment. Docker captures the complete software stack, while renv focuses on
managing R package dependencies and providing a clear documentation of the R
package environment. This combination ensures that your analysis can be easily
reproduced and shared with others in a reliable and transparent manner.
The study was preregistered at Zenodo (DOI:10.5281/zenodo.11196495).

## Procedure
A cross-sectional online survey is used to examine a sample from the general population. Participants complete the Self-perceived Data Literacy Scale alongside demographic questions and additional validation measures. Survey questions of each measurement are randomized for each participant to minimize order effects and response biases. To shorten the overall length of the assessment the questions in each factor of the data literacy questionnaire are randomly selected for each participant. That way each participant only answers half of the possible items, the other half are planned missings. 

## Instruments  
### Measuring Data Literacy
On Data Literacy the participants will be asked to answer 71
items. Each participant will answer 38 items of the 71, that are randomly selected.
To answer the items, respondents indicate their agreement on a five-point Likert
scale (1 = "strongly disagree", 2 = "somewhat disagree", 3 = "neither agree nor
disagree", 4 = "somewhat agree", 5 = "strongly agree”) with a “don’t know”
option.

### Measuring Information Literacy
The SWE-IV-16 (Behm, 2018) assesses the self-efficacy beliefs
of adolescents and adults in their ability to engage in information behaviour. This
questionnaire measures the process model of information-related problem-solving
(Brand-Gruwel et al., 2009). In this study this construct is used as a proxy for information literacy.
It consists of 16 statements addressing self-assessed
abilities in searching for and evaluating information, as well as managing
information searches effectively. Each statement begins with "When I search for
information on a topic or a specific question..." and respondents indicate their
agreement on a five-point Likert scale (1 = "strongly disagree", 2 = "somewhat
disagree", 3 = "neither agree nor disagree", 4 = "somewhat agree", 5 = "strongly
agree”). The total scale value is computed as the arithmetic
mean of the items, which may be inverted if necessary. Calculation of the total value
requires valid responses to at least 12 of the 16 items.

### Measuring Need for Cognition
The NFC-K (Beißert et al., 2015) is a tool used to assess the
NFC through four items, which represent two facets: "engagement" and “joy". The
NFC-K is measured with a seven-point response scale, ranging from "strongly
disagree" (1) to "strongly agree" (7), with a "neither" option in the middle. The
German version of the scale is adapted from the original English scale by Cacioppo
and Petty (1982) and translated by Bless et al. (1994). To determine an individual's NFC score, a mean value
(scale value) is computed from the four raw score points of the responses. The
resulting mean values range between 1 and 7.

### Measuring Technology Competency
To assess self-perceived competence in using information and
communication technology (ICT), the five general items of the ICT-SC25 (Schauffel
et al., 2021) will be used. The ICT-SC25 is a scale consisting of 25 items designed to
assess self-perceived competence in using information and communication
technology. It is available in both German (ICT-SC25g) and English (ICT-SC25e). The
scale measures general and domain-specific ICT competence, including
communication, processing and storing, content generation, safe application, and
problem-solving skills. Items are measured using a six-point fully-labeled Likert-type
rating scale ranging from strongly disagree (1) to strongly agree (6). Researchers can choose to 
utilise either the entire scale or individual subscales based on their specific research objectives. The ICT-SC25g/e
is applicable for both manifest and latent analysis. Manifest scale scores for the ICT-
SC25g/e are calculated separately for each subscale by computing the unweighted
mean score of the items within each subscale (Schauffel et al., 2021).

### Measuring Openness and Conscientiousness
The BFI-10 (Rammstedt et al., 2014) will be used to assess
personality based on the five-factor model. Only the items on openness and
conscientiousness were assessed.
The items are answered on a five-point rating scale from "strongly disagree" (1) to
"strongly agree" (5). To measure the respondent's individual traits on the five
personality dimensions, the responses to the two items for each dimension are
averaged. First, the negatively worded item is recoded (items 1, 3, 4, 5, and 7), then
the mean value is calculated for each dimension from both the recoded and non-
recoded items. The values for the five dimensions range from 1 to 5 (see
Rammstedt, 2007 for reference values)

## Algorithm based Item selection  
The items will be selected via algorithms implemented in the stuart package in R [QUELLE].'Stuart' can construct subsets from a pool of items by using ant-colony- optimization, genetic algorithms, brute force, or random sampling (Schultze, 2022).
In classical approaches items are evaluated within the overall item pool and are then often selected based on their individual properties (e.g. difficulty, discrimination, item-scale-correlations).The automated approach of 'stuart' takes the opposite perspective, of meta heuristics, by repeatedly estimating CFAs for a multitude of possible item-combinations[QUELLE]. Thus, a pool of items with some constraints and the goal to find the one combination that best fits the suggested purpose of the final scale is estimated[QUELLE].
I want to optimise the data literacy self rating scale for model fit and reliability (RMSEA, SRMR, CFI & McDonalds $\omega$) as well as variability in the difficulty of items.

I use the genetic algorithm of stuart for this. Genetic algorithms are based on Darwinian evolution principles – selection, crossover, mutation and survival of the fittest[QUELLE]. The algorithm selects random subsamples of items as an individual solution. Good Solutions survive and procreate. The items with the lowest fittness are erased from the generation. For the next generation, two item sets are combined to produce a "child". Furthermore, "Mutation" adds a degree of randomness to this process. This progress is repeated until the algorithm converges into a solution that is dominant[QUELLE].






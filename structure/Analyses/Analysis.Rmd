---
title: "Analysis"
author: "Leonie"
date: "2024-04-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# to clear environment while working
rm(list=ls()) 

usethis::use_git_config(
  user.name = "LeonieHagitte",
  user.email = "leonie.hagitte@student.hu-berlin.de",
  init.defaultBranch = "devl")

```

# Reproducibility 
```{r}
if(!requireNamespace("remotes"))
install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
repro::automate() #creating a Dockerfile.    ####################### BUG ######
repro::use_docker()
repro::use_gha_docker() #GitHub action (GHA) is a cloud service that runs software when certain events trigger it.Add a GitHub Action to build the required Docker image with

# Install or update 'renv' package from CRAN if needed
# Load 'renv' package
library(renv)

# Initialize 'renv' in your project
renv::init()

# Activate the project-specific 'renv' environment
renv::activate()

# Install required packages for your project
#renv::install()

# Update renv.lock file
renv::snapshot()

```

# Model specification
```{r}
model <- "
# Measurement model
y1 =~ x1 + x2 + x3 + x4
y2 =~ x5 + x6 + x7 + x8
y3 =~ x9 + x10 + x11 + x12
y4 =~ x13 + x14 + x15 + x16
y5 =~ x17 + x18 + x19 + x20

# Factor variances
y1 ~~ y1
y2 ~~ y2
y3 ~~ y3
y4 ~~ y4
y5 ~~ y5

# Factor correlations
y1 ~~ y2
y1 ~~ y3
y1 ~~ y4
y1 ~~ y5
y2 ~~ y3
y2 ~~ y4
y2 ~~ y5
y3 ~~ y4
y3 ~~ y5
y4 ~~ y5
"

```

# Data Preparation
## Data enreading
```{r}
library(dplyr)
library(mice)
library(readr)

dataMA <- read_csv("/Users/leo/Desktop/Programming stuff/Thesis/structure/Analyses/DataMA.csv")
View(dataMA)

# Convert all columns to character to ensure consistent replacement
df <- data.frame(lapply(dataMA, as.character))

# Replace "AO01" etc with respective numeric values across all columns
df[] <- lapply(df, function(x) gsub("AO01", "1", x))
df[] <- lapply(df, function(x) gsub("AO02", "2", x))
df[] <- lapply(df, function(x) gsub("AO03", "3", x))
df[] <- lapply(df, function(x) gsub("AO04", "4", x))
df[] <- lapply(df, function(x) gsub("AO05", "5", x))
df[] <- lapply(df, function(x) gsub("AO06", "6", x))
df[] <- lapply(df, function(x) gsub("AO07", "7", x))
df[] <- lapply(df, function(x) gsub("AO08", "8", x))
df[] <- lapply(df, function(x) gsub("AO09", "9", x))
df[] <- lapply(df, function(x) gsub("AO10", "10", x))

# Convert columns back to numeric if all values in a column can be converted
df <- data.frame(lapply(df, function(x) {
  converted <- suppressWarnings(as.numeric(x))
  if(all(!is.na(converted))) converted else x
}))

# Rename column "ICT.ATC3." to "ATC3"
names(df)[names(df) == "ICT.ATC3."] <- "ATC3"

```

## Attention checks
```{r}
# Calculate the number of wrong answers for each person

df$wrong_answers <- 0
df$wrong_answers <- df$wrong_answers + (df$ATC1 != 1)
df$wrong_answers <- df$wrong_answers + (df$ATC2 != 0)
df$wrong_answers <- df$wrong_answers + (df$ATC3 != 5)
# df$wrong_answers <- df$wrong_answers + (df$ATC4 != 2)

# Filter IDs of people who failed more than two items
failed_ids <- df$id[df$wrong_answers > 2]

# Display the IDs
failed_ids

# 46, 92, 275, 283, 419, 512, 588, 599, 613, 663, 668, 713

# Count the number of people who failed more than two items, ignoring NA values
num_failed_more_than_two <- sum(df$wrong_answers > 2, na.rm = TRUE)

# Display the count
num_failed_more_than_two
```

## Filter for stubborn people 
```{r}
# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Bei der ersten Kontrollfrage (für die Qualitätssicherheit) dachte ich, es wäre eine Fangfrage & habe NICHT das gedrückt, was ich sollte… ansonsten habe ich alles nach bestem Wissen und Gewissen ausgefüllt ????????????"]

# Display the ID
person_id #id=23

# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Die Fragen ohne Inhalt mit Klickanweisung waren bizarr. Hintergrund unverständlich und nicht erläutert. Deshalb mit „weiß nicht“ beantwortet"]

# Display the ID
person_id #id=71

## Very long Comment - I couldnt find with the original code, i had used up to this point.##
# Defining a pattern that captures the essence of the comment while allowing for some variation
# For simplicity, this example uses a shorter, distinctive part of the comment
# Adjust the pattern as needed to be specific enough to uniquely identify the comment
pattern <- "Die Umfrage war sehr erhellend.*menschlichen Mitteln.*Doktor Faust"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id=275

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Fragen irritiert. Ich habe sie nicht wie gefordert beantwortet, da mir nicht transparent war, wozu dies wirklich dient. Es fühlte sich mehr wie ein Experiment an: Machen die Leute, was man ihnen sagt?"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids # id=419

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Habe die Frage.*nicht verstanden und immer irgendetwas gedrückt.*"

# Use grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, ignore.case = TRUE, perl = TRUE)

# Use the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id= 760

# Excluding stubborn people from former failed id list

# 46, 92, 283, 512, 588, 599, 613, 663, 668, 713
# leaves 10 people to exclude

```
```{r}
# Define the IDs to exclude
ids_to_exclude <- c(46, 92, 283, 512, 588, 599, 613, 663, 668, 713,796)

# id 796 is to be excluded, because the person indicated an age of 17
# Filter out the rows with the specified IDs and save the result to a new dataframe
df2 <- df[!df$id %in% ids_to_exclude, ]

# Display the first few rows of the new dataframe to verify
head(df2)
```

```{r}
# Convert 'lastpage' to numeric
df2$lastpage <- as.numeric(df2$lastpage)

# Inspect unique values of 'lastpage' after conversion
unique(df2$lastpage)

# Exclude rows where 'lastpage' is smaller than 6
df3 <- df2[df2$lastpage >= 6, ]

# This code updates df3 to keep only the rows where lastpage doesnt contain NA values.

# Check if 'lastpage' column exists in df3
if("lastpage" %in% names(df3)) {
  # If it exists, filter out NA values from 'lastpage'
  df3 <- df3[!is.na(df3$lastpage), ]
} else {
  cat("Column 'lastpage' does not exist in df3\n")
}

```

```{r, echo=FALSE}
# Recode columns 9 to 109 as numeric
df3[, 9:109] <- lapply(df3[, 9:109], as.numeric)

# Check the structure of the first few columns to confirm the change
str(df3[, 9:109])
```

```{r}
# Subset df3 to include only columns up to column 119
df3 <- df3[, 1:119]

# Check the structure of the modified dataframe to confirm the change
str(df3)

# Exclude columns 2, 4, 5, 6, 7, 8 and keep only up to column 119
df3 <- df3[, -c(2, 4, 5, 6, 7, 8)]

# Exclude the ATCs
df3 <- df3[, !(names(df3) %in% c("ATC1", "ATC2"))]

```

```{r}

```

```{r}

```

```{r}

```

# checking for the Pattern of Missings
Although it may not be necessary, because the missings are planned, random missings, one could check for the pattern of the missing data. 

## NAs FIMLn
```{r}
df_dl <- df3[, 3:71]
# Load the lavaan package
library(lavaan)

# Model specification
model <- '
# Measurement model
y1 =~ F1F1 + F1F2 + F1F3 + F1F4 + F1F5 + F1F6 + F1F7 + F1F8 + F1F9 + F1F10 + F1F11 + F1F12 + F1F13 + F1F14
y2 =~ F2F1 + F2F2 + F2F3 + F2F4 + F2F5 + F2F6 + F2F7 + F2F8 + F2F9 + F2F10 + F2F11 + F2F12 + F2F13 + F2F14 + F2F15 + F2F16 + F2F17 + F2F18 + F2F19 + F2F20
y3 =~ F3F1 + F3F2 + F3F3 + F3F4 + F3F5 + F3F6 + F3F7 + F3F8 + F3F9 
y4 =~ F4F1 + F4F2 + F4F3 + F4F4 + F4F5 + F4F6 + F4F7 + F4F8 
y5 =~ F5F1 + F5F2 + F5F3 + F5F4 + F5F5 + F5F6 + F5F7 + F5F8 + F5F9 + F5F10 + F5F11 + F5F12 + F5F13 + F5F14 + F5F15 + F5F16 + F5F17 + F5F18 

# Factor variances
y1 ~~ y1
y2 ~~ y2
y3 ~~ y3
y4 ~~ y4
y5 ~~ y5

# Factor correlations
y1 ~~ y2
y1 ~~ y3
y1 ~~ y4
y1 ~~ y5
y2 ~~ y3
y2 ~~ y4
y2 ~~ y5
y3 ~~ y4
y3 ~~ y5
y4 ~~ y5
'

# Fit the model using FIML for missing data
fit <- sem(model, data=df_dl, missing="fiml", em.h1.iter.max=10000)
# fixed.x = F. FIML works by estimating the relationships of the variables with each other and requires estimating the means and variances of the variables. If fixed.x = T (the default), then the variances and covariances are fixed and are based on the existing sample values and are not estimated.
# Summary of the fit
summary(fit)
```

## or using mice?
```{r}
imputedData <- mice(df_dl, m=5, method='pmm', maxit=50)
completedData <- complete(imputedData, 1)
```



# Poweranalyse
```{r}
install.packages('semPower')
devtools::install_github('martscht/stuart/stuart', ref = 'develop')
library(semPower)
library(stuart)
library(lavaan)
```
 
# Poweranalyse - SemPower mit Model
```{r}
powerMI <- semPower.powerMI(
  type = 'a-priori',
  comparison = 'configural',
  nullEffect = 'metric',
  nIndicator = c(4, 4, 4, 4, 4), # Corrected to a single vector
  loadM = list(.5, .6), # Assuming baseline loadings for two groups
  tau = list(rep(0.0, 20), # Assuming intercepts for 20 indicators in the first group
             rep(seq(.1, .5, .1), each = 4)), # Assuming intercepts for 20 indicators in the second group, with increments
  alpha = .05, 
  power = .80,
  N = list(.80, .20) # Assuming 80/20 sized groups for split
)
# Show summary
summary(powerMI)

```
## Results Printed
semPower: A priori power analysis
                                     
 F0                        0.033897  
 RMSEA                     0.058221  
 Mc                        0.983194  
                                     
 df                        20        
 Required Num Observations 622       
                           (498, 125)
                                     
 Critical Chi-Square       31.41043  
 NCP                       21.05165  
 Alpha                     0.050000  
 Beta                      0.197753  
 Power (1 - Beta)          0.802247  
 Implied Alpha/Beta Ratio  0.252841  


# Poweranalyse - SemPower ohne Model für Sample Split
```{r}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 155)
summary(ap)
```
semPower: A priori power analysis
                                   
 F0                        0.387500
 RMSEA                     0.050000
 Mc                        0.823864
                                   
 df                        155     
 Required Num Observations 128     
                                   
 Critical Chi-Square       185.0523
 NCP                       49.21250
 Alpha                     0.050000
 Beta                      0.199807
 Power (1 - Beta)          0.800193
 Implied Alpha/Beta Ratio  0.250242


## Descriptive Analyses
```{r}
# packages
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)

# Create a subset of df3 with columns 104 to 112
demographics <- df3[, 102:111]

```

## Sample
```{r}
# gender (2 = male, female = 1, 3 = divers)
round(prop.table(table(demographics$Gender))*100, 1)

# year of birth
range(demographics$Age, na.rm = TRUE)
round(mean(demographics$Age, na.rm = TRUE), 2)
round(sd(demographics$Age, na.rm = TRUE), 2)

```

```{r}
# prepare subsets for my variables
describe_age <- demographics %>% dplyr::select(starts_with("Age"))
describe_gender <- demographics %>% dplyr::select(starts_with("Gender"))
describe_education <- demographics %>% dplyr::select(starts_with("Education"))
describe_study <- demographics %>% dplyr::select(starts_with("Study"))
describe_degree <- demographics %>% dplyr::select(starts_with("Degree"))
describe_workfilter <- demographics %>% dplyr::select(starts_with("Workfilter"))
describe_worktime <- demographics %>% dplyr::select(starts_with("Worktime"))
describe_occupation <- demographics %>% dplyr::select(starts_with("Occupation"))

```

```{r}
# create table for age
table_age <- data.frame(colMeans(is.na(describe_age)))
colnames(table_age) <- "Age"
rownames(table_age) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")

# create table for EURO-D
table_eurod <- data.frame(colMeans(is.na(describe_eurod)))
colnames(table_eurod) <- "EURO-D"
rownames(table_eurod) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")

# create table for finances
table_finances <- data.frame(colMeans(is.na(describe_finances)))
colnames(table_finances) <- "Finances"
rownames(table_finances) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")

# combine tables
table_missings <- data.frame(table_age, table_eurod, table_finances, check.names = FALSE)
# round
table_missings <- table_missings %>% mutate_if(is.numeric, round, digits = 2) * 100
# add % symbol
table_missings <- table_missings %>% 
  mutate(across(everything(), ~paste0(., "%")))
table_missings # print for HTML

```

```{r}
# for Age
table_descriptive_age <- round(psych::describe(describe_age), 2)
rownames(table_descriptive_age) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")
table_descriptive_age # print for HTML

# for EURO-D
table_descriptive_eurod <- round(psych::describe(describe_eurod), 2)
rownames(table_descriptive_eurod) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")
table_descriptive_eurod # print for HTML

# for finances
table_descriptive_finances <- round(psych::describe(describe_finances), 2)
rownames(table_descriptive_finances) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")
table_descriptive_finances # print for HTML

```

## Heatmap
```{r}
# for EURO-D + finances over time
  # create correlation matrix
both_correlations <- data_wide %>% dplyr::select(starts_with("eurod"), starts_with("finances"))
  # rename variables for clarity
both_correlations <- both_correlations %>% rename(
  "EURO-D (wave 4)" = eurod.4,
  "EURO-D (wave 5)" = eurod.5,
  "EURO-D (wave 6)" = eurod.6,
  "EURO-D (wave 7)" = eurod.7,
  "EURO-D (wave 8)" = eurod.8,
  "Finances (wave 4)" = finances.4,
  "Finances (wave 5)" = finances.5,
  "Finances (wave 6)" = finances.6,
  "Finances (wave 7)" = finances.7,
  "Finances (wave 8)" = finances.8)
  # get correlations
both_matrix <- round(corr.test(both_correlations, use = "pairwise")$r, 2)
  # only use upper part of matrix
both_matrix_lower_part <- function(both_matrix){
  both_matrix[lower.tri(both_matrix)]<- NA
  return(both_matrix)}
both_matrix_up <- both_matrix_lower_part(both_matrix)
  # reduce size of correlation matrix/long format
both_matrix_melted <- melt(both_matrix_up, na.rm = TRUE)
  # plotting heatmap 
both_heatmap <- ggplot(data = both_matrix_melted, aes(Var2, Var1, fill = value)) +
 ggtitle("Correlations - EURO-D + Finances") +
 geom_tile(color = "white") +
 scale_fill_gradient2(low = "blue", high = "red", mid = "white",
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelations") +
  theme_minimal() + 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1), axis.text.y = element_text(size = 8), plot.title = element_text(size = 12), legend.position = "right", legend.title = element_text(size = 8)) +
 coord_fixed()
  # add correlation coefficients
heatmap_final <- both_heatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.3))
heatmap_final # print for HTML

# check p-values
both_matrix_p <- round(corr.test(both_correlations, use = "pairwise")$p, 5)
both_matrix_p # print for HTML

```

```{r}

```

```{r}

```

```{r}

```

## Filter for the partial completes
```{r}
# Splitting df3 into two datasets based on the "lastpage" variable
df4 <- subset(df3, lastpage == 13)
df_partial <- subset(df3, lastpage != 13)

```

# Stuart - Item selection
## Sample split
```{r}
library(stuart)
#, 0.4525
split <- holdout(df4, prop = 0.5475, grouping = NULL, seed = 2024, determined = NULL)
lapply(split, nrow) #check size of sample

# Accessing the split datasets
df_train <- split$calibrate
df_test <- split$validate

# Remove the 'determined' column by setting it to NULL
df_train$determined <- NULL
df_test$determined <- NULL

# adding the partially completed ones to the df train
df_train <- rbind(df_train, df_partial)

# exclude everything exept the dl items
df_train <- df_train[,3:71]

```

## Objective-Function
```{r}
library(stuart)
data(df_dl)

# Define the theoretical nu matrix 
theoretical_nu <- matrix(c(5, 4, 2, 1,  # Factor 1 items
                      5, 4, 2, 1,  # Factor 2 items
                      5, 4, 2, 1,  # Factor 3 items
                      5, 4, 2, 1,  # Factor 4 items
                      5, 4, 2, 1), # Factor 5 items
                    nrow = 4, byrow = TRUE,
                    dimnames = list(c("Item1", "Item2", "Item3", "Item4"),
                                    c("Factor1", "Factor2", "Factor3", "Factor4", "Factor5")))

# Convert the matrix to a vector
nu_vector <- c(theoretical_nu)

# Calculate the standard deviation of the vector
ideal_sd <- sd(nu_vector)

# Print the ideal standard deviation
print(ideal_sd) #1.62

# Use 'ideal_sd' in my objective function
#ordinal data
obj <- function(chisq, df, pvalue,rmsea.scaled, srmr, cfi.scaled, crel, nu) {
  out1 = 0.5-(0.5/(1 + exp(- 100 * (rmsea.scaled-.05))))
  out2 = 0.5-(0.5/(1 + exp(- 100 * (srmr-.05))))
  out3 = (1/(1 + exp(- 100 * (cfi.scaled-.95))))
  out4 = -(sd(nu) - 1.62)^2 + 1.62^2 # Quadratic function peaking at 'ideal_sd'
  out = (out1 + out2 + out3 + out4)/4
  return(out)                                
}

```

```{r}

fs<-list(Comprehension=c('F1F1','F1F2','F1F3','F1F4','F1F5','F1F6','F1F7','F1F8',
                           'F1F9','F1F10','F1F11','F1F12','F1F13','F1F14'),
           Evaluation=c('F2F1','F2F2','F2F3','F2F4','F2F5','F2F6','F2F7','F2F8',
                        'F2F9','F2F10','F2F11','F2F12','F2F13','F2F14','F2F15',
                        'F2F16','F2F17','F2F18','F2F19','F2F20'),
           Integration=c('F3F1','F3F2','F3F3','F3F4','F3F5','F3F6','F3F7','F3F8','F3F9'),
           Communication=c('F4F1','F4F2','F4F3','F4F4','F4F5','F4F6','F4F7','F4F8'),
           Statistics=c('F5F1','F5F2','F5F3','F5F4','F5F5','F5F6','F5F7','F5F8',
                        'F5F9','F5F10','F5F11','F5F12','F5F13','F5F14','F5F15',
                        'F5F16','F5F17','F5F18')
                           )
```

## Genetic Algorithm
### Kfold-Crossvalidation
```{r}

############ k-Folding #########################

# erster der holdout wird an kfold weitergegeben

kfold_sel <- kfold('gene', k = 3, #gene #how to arrive at number of folds?
  data = df_train, 
  factor.structure = fs,
  max.invariance = "scalar", ## randomly sampled aus derselben population - höher schon besser/ realistisch 
  capacity = 4, 
  seed = 2024, #what does the "capacity" do?
  seeded.search = TRUE,
  remove.details = TRUE,
  cores = 1) #what does the "cores" do?
#########################################################
summary(sel)

inspect(sel$final, 'est')
inspect(sel$final, 'fit')

lavaan::summary(sel$final, standardized = TRUE) # adapt final as name ggf
summary(selection)

lavaan::inspect(selection$final, 'est')$nu
```

```{r}
library(stuart)
# mmas freier in der spezifikation, variabler
# kfold & genetic loop paar mal laufen lassen, für stabile ergebnisse

gene(
  df_dl,                 # Your data frame containing the observed variables
  factor.structure <- fs, # A list defining the factor structure of your model
  capacity = 4,      # Optional, capacity constraint for factors
  auxiliary = NULL,                # Optional, specify auxiliary variables
  software = "lavaan",             # Specify the SEM software to be used
  cores = 1,                    # Number of CPU cores to be used for parallel processing
  objective = obj,                # Optional, specify a custom objective function
  ignore.errors = FALSE,           # Whether to ignore errors during fitness evaluation
  burnin = 5,                      # Number of generations for burn-in phase
  generations = 256,               # Total number of generations
  individuals = 64,                # Number of individuals in each generation
  selection = "tournament",        # Selection method for genetic algorithm
  selection.pressure = NULL,       # Pressure for tournament selection
  elitism = NULL,                  # Rate of elitism
  reproduction = 0.5,               # Proportion of individuals reproduced in each generation
  mutation = 0.05,                  # Mutation rate
  mating.index = 0,                 # Index of mating type
  mating.size = 0.25,               # Proportion of mating pool size
  mating.criterion = "similarity",  # Criterion for selecting mates
  immigration = 0,                  # Proportion of immigrants in each generation
  convergence.criterion = "geno.between",  # Convergence criterion
  tolerance = NULL,                 # Tolerance for convergence
  reinit.n = 1,                     # Number of reinitializations
  reinit.criterion = convergence.criterion,  # Criterion for reinitialization
  reinit.tolerance = NULL,          # Tolerance for reinitialization
  reinit.prop = 0.75,               # Proportion of population reinitialized
  schedule = "run",                 # Schedule for genetic algorithm
  analysis.options = NULL,          # Additional options for analysis
  suppress.model = FALSE,           # Whether to suppress model output
  seed = 2024,                      # Seed for random number generation
  filename = itemselection                   # Optional, filename for saving results
)


```

## Objective-Function ideas
```{r}
# threshhold for reliability
# correlations?
# variance in item difficulty - via intercepts?? -> intercepts  matrix in lavaan 
# "meanstructure" ?

# Define a matrix for item intercepts per factor
# Assuming a structure of 4 factors with 4 items each for simplicity
# Mu?
# interceptsMatrix
# int.ov 

fixedobjective(
  criteria = c("rmsea", "srmr", "crel"), #rel ohne"c" für einzelne faktoren 
  add = c("chisq", "df", "pvalue"),
  side = NULL, #can this be specified for certain values?
  scale = 1,
  matrices = NULL,
  fixed = NULL, #weitere function einfügen - if-then function etc. subreliabilitäten 
  comparisons = NULL
)

# matritzen mit objectivematrices function
# theoretisch festlegen, oder kombinatorisch/technisch festlegen? Random draws für fit indices? 
```


  # Normalize or scale avg_variance if necessary to align with the scale of out1, out2, out3
  # This is a placeholder; adjust scaling based on your specific needs
  out4 = avg_variance / 10 # Example scaling, adjust as needed



```{r}

# Crossvalidation
crossvalidate(sel, split)

```

```{r}

```

```{r}

```

```{r}

```

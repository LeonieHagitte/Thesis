---
title: "Analysis"
author: "Leonie"
date: "2024-04-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# to clear environment while working
rm(list=ls()) 

usethis::use_git_config(
  user.name = "LeonieHagitte",
  user.email = "leonie.hagitte@student.hu-berlin.de",
  init.defaultBranch = "devl")

```

```{r, echo=FALSE}
library(renv)
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)
library(dplyr)
library(mice)
library(readr)
library(lavaan)
library(semPower)
library(stuart)
library(reproducibleRchunks)

```

# Reproducibility 
```{r}
if(!requireNamespace("remotes"))
install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
repro::automate() #creating a Dockerfile.    ####################### BUG ######
repro::use_docker()
repro::use_gha_docker() #GitHub action (GHA) is a cloud service that runs software when certain events trigger it.Add a GitHub Action to build the required Docker image with

# Install or update 'renv' package from CRAN if needed
# Load 'renv' package
library(renv)

# Initialize 'renv' in your project
renv::init()

# Activate the project-specific 'renv' environment
renv::activate()

# Install required packages for your project
#renv::install()

# Update renv.lock file
renv::snapshot()

```

# Model specification
```{r}
model <- "
# Measurement model
y1 =~ x1 + x2 + x3 + x4
y2 =~ x5 + x6 + x7 + x8
y3 =~ x9 + x10 + x11 + x12
y4 =~ x13 + x14 + x15 + x16
y5 =~ x17 + x18 + x19 + x20

# Factor variances
y1 ~~ y1
y2 ~~ y2
y3 ~~ y3
y4 ~~ y4
y5 ~~ y5

# Factor correlations
y1 ~~ y2
y1 ~~ y3
y1 ~~ y4
y1 ~~ y5
y2 ~~ y3
y2 ~~ y4
y2 ~~ y5
y3 ~~ y4
y3 ~~ y5
y4 ~~ y5
"

```

# Data Preparation
## Data enreading
```{r}
library(dplyr)
library(mice)
library(readr)

dataMA <- read_csv("/Users/leo/Desktop/Programming stuff/Thesis/structure/Analyses/DataMA.csv")
View(dataMA)

# Convert all columns to character to ensure consistent replacement
df <- data.frame(lapply(dataMA, as.character))

# Replace "AO01" etc with respective numeric values across all columns
df[] <- lapply(df, function(x) gsub("AO01", "1", x))
df[] <- lapply(df, function(x) gsub("AO02", "2", x))
df[] <- lapply(df, function(x) gsub("AO03", "3", x))
df[] <- lapply(df, function(x) gsub("AO04", "4", x))
df[] <- lapply(df, function(x) gsub("AO05", "5", x))
df[] <- lapply(df, function(x) gsub("AO06", "6", x))
df[] <- lapply(df, function(x) gsub("AO07", "7", x))
df[] <- lapply(df, function(x) gsub("AO08", "8", x))
df[] <- lapply(df, function(x) gsub("AO09", "9", x))
df[] <- lapply(df, function(x) gsub("AO10", "10", x))

# Convert columns back to numeric if all values in a column can be converted
df <- data.frame(lapply(df, function(x) {
  converted <- suppressWarnings(as.numeric(x))
  if(all(!is.na(converted))) converted else x
}))

# Rename column "ICT.ATC3." to "ATC3"
names(df)[names(df) == "ICT.ATC3."] <- "ATC3"

```

## Attention checks
```{r}
# Calculate the number of wrong answers for each person

df$wrong_answers <- 0
df$wrong_answers <- df$wrong_answers + (df$ATC1 != 1)
df$wrong_answers <- df$wrong_answers + (df$ATC2 != 0)
df$wrong_answers <- df$wrong_answers + (df$ATC3 != 5)
# df$wrong_answers <- df$wrong_answers + (df$ATC4 != 2)

# Filter IDs of people who failed more than two items
failed_ids <- df$id[df$wrong_answers > 2]

# Display the IDs
failed_ids

# 46, 92, 275, 283, 419, 512, 588, 599, 613, 663, 668, 713

# Count the number of people who failed more than two items, ignoring NA values
num_failed_more_than_two <- sum(df$wrong_answers > 2, na.rm = TRUE)

# Display the count
num_failed_more_than_two
```

## Filter for stubborn people 
```{r}
# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Bei der ersten Kontrollfrage (für die Qualitätssicherheit) dachte ich, es wäre eine Fangfrage & habe NICHT das gedrückt, was ich sollte… ansonsten habe ich alles nach bestem Wissen und Gewissen ausgefüllt ????????????"]

# Display the ID
person_id #id=23

# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Die Fragen ohne Inhalt mit Klickanweisung waren bizarr. Hintergrund unverständlich und nicht erläutert. Deshalb mit „weiß nicht“ beantwortet"]

# Display the ID
person_id #id=71

## Very long Comment - I couldnt find with the original code, i had used up to this point.##
# Defining a pattern that captures the essence of the comment while allowing for some variation
# For simplicity, this example uses a shorter, distinctive part of the comment
# Adjust the pattern as needed to be specific enough to uniquely identify the comment
pattern <- "Die Umfrage war sehr erhellend.*menschlichen Mitteln.*Doktor Faust"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id=275

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Fragen irritiert. Ich habe sie nicht wie gefordert beantwortet, da mir nicht transparent war, wozu dies wirklich dient. Es fühlte sich mehr wie ein Experiment an: Machen die Leute, was man ihnen sagt?"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids # id=419

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Habe die Frage.*nicht verstanden und immer irgendetwas gedrückt.*"

# Use grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, ignore.case = TRUE, perl = TRUE)

# Use the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id= 760

# Excluding stubborn people from former failed id list

# 46, 92, 283, 512, 588, 599, 613, 663, 668, 713
# leaves 10 people to exclude

```
```{r}
# Define the IDs to exclude
ids_to_exclude <- c(46, 92, 283, 512, 588, 599, 613, 663, 668, 713,796)

# id 796 is to be excluded, because the person indicated an age of 17
# Filter out the rows with the specified IDs and save the result to a new dataframe
df2 <- df[!df$id %in% ids_to_exclude, ]

# Display the first few rows of the new dataframe to verify
head(df2)
```

```{r}
# Convert 'lastpage' to numeric
df2$lastpage <- as.numeric(df2$lastpage)

# Inspect unique values of 'lastpage' after conversion
unique(df2$lastpage)

# Exclude rows where 'lastpage' is smaller than 6
df3 <- df2[df2$lastpage >= 6, ]

# This code updates df3 to keep only the rows where lastpage doesnt contain NA values.

# Check if 'lastpage' column exists in df3
if("lastpage" %in% names(df3)) {
  # If it exists, filter out NA values from 'lastpage'
  df3 <- df3[!is.na(df3$lastpage), ]
} else {
  cat("Column 'lastpage' does not exist in df3\n")
}

```

```{r, echo=FALSE}
# Recode columns 9 to 109 as numeric
df3[, 9:109] <- lapply(df3[, 9:109], as.numeric)

# Check the structure of the first few columns to confirm the change
str(df3[, 9:109])
```

```{r}
# Subset df3 to include only columns up to column 119
df3 <- df3[, 1:119]

# Check the structure of the modified dataframe to confirm the change
str(df3)

# Exclude columns 2, 4, 5, 6, 7, 8 and keep only up to column 119
df3 <- df3[, -c(2, 4, 5, 6, 7, 8)]

# Exclude the ATCs
df3 <- df3[, !(names(df3) %in% c("ATC1", "ATC2"))]

```

# checking for the Pattern of Missings
Although it may not be necessary, because the missings are planned, random missings, one could check for the pattern of the missing data. 
```{r}

```

## NAs FIMLn
```{r}
df_dl <- df3[, 3:71]
# Load the lavaan package
library(lavaan)

# Model specification
model <- '
# Measurement model
y1 =~ F1F1 + F1F2 + F1F3 + F1F4 + F1F5 + F1F6 + F1F7 + F1F8 + F1F9 + F1F10 + F1F11 + F1F12 + F1F13 + F1F14
y2 =~ F2F1 + F2F2 + F2F3 + F2F4 + F2F5 + F2F6 + F2F7 + F2F8 + F2F9 + F2F10 + F2F11 + F2F12 + F2F13 + F2F14 + F2F15 + F2F16 + F2F17 + F2F18 + F2F19 + F2F20
y3 =~ F3F1 + F3F2 + F3F3 + F3F4 + F3F5 + F3F6 + F3F7 + F3F8 + F3F9 
y4 =~ F4F1 + F4F2 + F4F3 + F4F4 + F4F5 + F4F6 + F4F7 + F4F8 
y5 =~ F5F1 + F5F2 + F5F3 + F5F4 + F5F5 + F5F6 + F5F7 + F5F8 + F5F9 + F5F10 + F5F11 + F5F12 + F5F13 + F5F14 + F5F15 + F5F16 + F5F17 + F5F18 

# Factor variances
y1 ~~ y1
y2 ~~ y2
y3 ~~ y3
y4 ~~ y4
y5 ~~ y5

# Factor correlations
y1 ~~ y2
y1 ~~ y3
y1 ~~ y4
y1 ~~ y5
y2 ~~ y3
y2 ~~ y4
y2 ~~ y5
y3 ~~ y4
y3 ~~ y5
y4 ~~ y5
'

# Fit the model using FIML for missing data
fit <- sem(model, data=df_dl, missing="fiml", em.h1.iter.max=10000)
# fixed.x = F. FIML works by estimating the relationships of the variables with each other and requires estimating the means and variances of the variables. If fixed.x = T (the default), then the variances and covariances are fixed and are based on the existing sample values and are not estimated.
# Summary of the fit
summary(fit)
```

## or using mice?
```{r, echo=FALSE}
imputedData <- mice(df_dl, m=5, method='pmm', maxit=50)
completedData <- complete(imputedData, 1)
```

# Poweranalyse
```{r}
install.packages('semPower')
devtools::install_github('martscht/stuart/stuart', ref = 'develop')
library(semPower)
library(stuart)
library(lavaan)
```
 
# Poweranalyse - SemPower mit Model
```{r}
powerMI <- semPower.powerMI(
  type = 'a-priori',
  comparison = 'configural',
  nullEffect = 'metric',
  nIndicator = c(4, 4, 4, 4, 4), # Corrected to a single vector
  loadM = list(.5, .6), # Assuming baseline loadings for two groups
  tau = list(rep(0.0, 20), # Assuming intercepts for 20 indicators in the first group
             rep(seq(.1, .5, .1), each = 4)), # Assuming intercepts for 20 indicators in the second group, with increments
  alpha = .05, 
  power = .80,
  N = list(.80, .20) # Assuming 80/20 sized groups for split
)
# Show summary
summary(powerMI)

```
## Results Printed
semPower: A priori power analysis
                                     
 F0                        0.033897  
 RMSEA                     0.058221  
 Mc                        0.983194  
                                     
 df                        20        
 Required Num Observations 622       
                           (498, 125)
                                     
 Critical Chi-Square       31.41043  
 NCP                       21.05165  
 Alpha                     0.050000  
 Beta                      0.197753  
 Power (1 - Beta)          0.802247  
 Implied Alpha/Beta Ratio  0.252841  


# Poweranalyse - SemPower ohne Model für Sample Split
```{r}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 155)
summary(ap)
```
semPower: A priori power analysis
                                   
 F0                        0.387500
 RMSEA                     0.050000
 Mc                        0.823864
                                   
 df                        155     
 Required Num Observations 128     
                                   
 Critical Chi-Square       185.0523
 NCP                       49.21250
 Alpha                     0.050000
 Beta                      0.199807
 Power (1 - Beta)          0.800193
 Implied Alpha/Beta Ratio  0.250242


## Descriptive Analyses
```{r}
# packages
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)

# Create a subset of df3 with columns 104 to 112
demographics <- df3[, 102:111]

```

## Sample
```{r}
# gender (2 = male, female = 1, 3 = divers)
round(prop.table(table(demographics$Gender))*100, 1)

# year of birth
range(demographics$Age, na.rm = TRUE)
round(mean(demographics$Age, na.rm = TRUE), 2)
round(sd(demographics$Age, na.rm = TRUE), 2)

```

```{r}
# prepare subsets for my variables
describe_age <- demographics %>%
  dplyr::select(starts_with("Age")) %>%
  mutate_all(as.numeric)

describe_gender <- demographics %>%
  select(starts_with("Gender")) %>%
  mutate_all(as.numeric)

describe_education <- demographics %>%
  select(starts_with("Education")) %>%
  mutate_all(as.numeric)

describe_degree <- demographics %>%
  select(starts_with("Degree")) %>%
  mutate_all(as.numeric)

describe_workfilter <- demographics %>%
  select(starts_with("Workfilter")) %>%
  mutate(
    Workfilter = case_when(
      Workfilter == "Y" ~ 1,
      Workfilter == "N" ~ 0,
      TRUE ~ NA_integer_  # handle unexpected values, if any
    )
  )

describe_study <- demographics %>%
  select(starts_with("study")) %>%
  mutate(
    study = case_when(
      study == "Y" ~ 1,
      study == "N" ~ 0,
      TRUE ~ NA_integer_  # handle unexpected values, if any
    )
  )

describe_worktime <- demographics %>%
  select(starts_with("Worktime")) %>%
  mutate_all(as.numeric)

describe_occupation <- demographics %>%
  select(starts_with("Occupation")) %>%
  mutate_all(as.numeric)

```

```{r}
# create table for age
table_age <- data.frame(colMeans(is.na(describe_age)))
colnames(table_age) <- "Age"

# create table for Gender
table_gender <- data.frame(colMeans(is.na(describe_gender)))
colnames(table_gender) <- "Gender"

# create table for Education
table_education <- data.frame(colMeans(is.na(describe_education)))
colnames(table_education) <- "Education"

# create table for Studying
table_study <- data.frame(colMeans(is.na(describe_study)))
colnames(table_age) <- "Study"

# create table for Degree
table_degree <- data.frame(colMeans(is.na(describe_degree)))
colnames(table_degree) <- "Degree"

# create table for Working Y/N
table_wokfilter <- data.frame(colMeans(is.na(describe_workfilter)))
colnames(table_workfilter) <- "Workfilter"

# create table for Worktime
table_worktime <- data.frame(colMeans(is.na(describe_worktime)))
colnames(table_worktime) <- "Worktime"

# create table for Occupation
table_occupation <- data.frame(colMeans(is.na(describe_occupation)))
colnames(table_occupation) <- "Occupation"

# combine tables
table_missings <- data.frame(table_age, table_gender, table_education, table_study,
                             table_degree, table_wokfilter, table_worktime, table_occupation, check.names = FALSE)
# round
table_missings <- table_missings %>% mutate_if(is.numeric, round, digits = 2) * 100

# add % symbol
table_missings <- table_missings %>% 
  mutate(across(everything(), ~paste0(., "%")))
table_missings # print for HTML

```

```{r}
library(dplyr)
library(psych)
library(rempsyc)

# Example: Define and round descriptive statistics for each variable
table_descriptive_age <- round(psych::describe(describe_age), 2)
table_descriptive_gender <- round(psych::describe(describe_gender), 2)
table_descriptive_education <- round(psych::describe(describe_education), 2)
table_descriptive_study <- round(psych::describe(describe_study), 2)
table_descriptive_degree <- round(psych::describe(describe_degree), 2)
table_descriptive_workfilter <- round(psych::describe(describe_workfilter), 2)
table_descriptive_worktime <- round(psych::describe(describe_worktime), 2)
table_descriptive_occupation <- round(psych::describe(describe_occupation), 2)

# Combine all tables into a single data frame row-wise
table_descriptives <- bind_rows(
  table_descriptive_age,
  table_descriptive_gender,
  table_descriptive_education,
  table_descriptive_study,
  table_descriptive_degree,
  table_descriptive_workfilter,
  table_descriptive_worktime,
  table_descriptive_occupation
)

print(table_descriptives)

# Convert row names to a column in your data frame
table_descriptives <- table_descriptives %>%
  rownames_to_column(var = "Variables")

# Combine columns 3 to 5 and column 23 for selection
selected_columns <- c(1,4:6, 14)

# Use the combined columns in the nice_table function
nice_table(table_descriptives[selected_columns], stars = TRUE, title = "Sample Descriptives", note = "Descriptive Statistics on the demographic variables. sd = Standard deviation; se = Standard error")

```

## Heatmap
```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Example: Assuming your data frame is demographics

# Ensure variables are numeric
demographics <- demographics %>%
  mutate(
    study = ifelse(study == "Y", 1, 0),  # Convert Y/N to 1/0
    Workfilter = ifelse(Workfilter == "Y", 1, 0)  # Convert Y/N to 1/0
    # Add more conversions as needed for other variables
  ) %>%
  mutate(
    Age = as.numeric(Age),
    Gender = as.numeric(Gender),  # Assuming Gender is already binary
    Education = as.numeric(Education),
    Degree = as.numeric(Degree),
    Worktime = as.numeric(Worktime),
    Occupation = as.numeric(Occupation)
  )

# Select variables for correlation analysis
selected_vars <- demographics %>%
  select(Gender, Age, Education, study, Degree, Workfilter, Worktime, Occupation)

# Rename variables for clarity in heatmap
colnames(selected_vars) <- c(
  "Gender", "Age", "Education", "study", "Degree", "Workfilter", "Worktime", "Occupation"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final <- heatmap + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final)




```

```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Assuming your data frame is df_dl
# Ensure all variables are numeric
df_dl <- df_dl %>%
  mutate(across(everything(), as.numeric))

# Select variables for correlation analysis
selected_vars <- df_dl %>%
  select(everything())

colnames(selected_vars) <- c(
  "F1F1",  "F1F2",  "F1F3",  "F1F4",  "F1F5",  "F1F6",  "F1F7",  "F1F8",  "F1F9",  "F1F10",
  "F1F11", "F1F12", "F1F13", "F1F14", "F2F1",  "F2F2",  "F2F3",  "F2F4",  "F2F5",  "F2F6",
  "F2F7",  "F2F8",  "F2F9",  "F2F10", "F2F11", "F2F12", "F2F13", "F2F14", "F2F15", "F2F16",
  "F2F17", "F2F18", "F2F19", "F2F20", "F3F1",  "F3F2",  "F3F3",  "F3F4",  "F3F5",  "F3F6",
  "F3F7",  "F3F8",  "F3F9",  "F4F1",  "F4F2",  "F4F3",  "F4F4",  "F4F5",  "F4F6",  "F4F7",
  "F4F8",  "F5F1",  "F5F2",  "F5F3",  "F5F4",  "F5F5",  "F5F6",  "F5F7",  "F5F8",  "F5F9",
  "F5F10", "F5F11", "F5F12", "F5F13", "F5F14", "F5F15", "F5F16", "F5F17", "F5F18"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap2 <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final2 <- heatmap2 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final2)
print(heatmap2)
```

```{r}

```

## Filter for the partial completes
```{r}
# Splitting df3 into two datasets based on the "lastpage" variable
df4 <- subset(df3, lastpage == 13)
df_partial <- subset(df3, lastpage != 13)

```

# Stuart - Item selection
## Sample split
```{r}
library(stuart)
#, 0.4525
split <- holdout(df4, prop = 0.5475, grouping = NULL, seed = 2024, determined = NULL)
lapply(split, nrow) #check size of sample

# Accessing the split datasets
df_train <- split$calibrate
df_test <- split$validate

# Remove the 'determined' column by setting it to NULL
df_train$determined <- NULL
df_test$determined <- NULL

# adding the partially completed ones to the df train
df_train <- rbind(df_train, df_partial)

# exclude everything exept the dl items
df_train <- df_train[,3:71]

df_test_dl <- df_test[,3:71]
```

## using mice 
```{r, echo=FALSE}
imputed_training_data <- mice(df_train, m=5, method='pmm', maxit=50)
imputed_training_data_c <- complete(imputed_training_data, 1)

imputed_test_data <- mice(df_test_dl, m=5, method='pmm', maxit=50)
imputed_test_data_c <- complete(imputed_test_data, 1)
```

```{r}
write.csv2(imputed_training_data_c, "imputed_training_data_c.csv")
write.csv2(df_train, "df_train.csv")
```

```{r}

fs<-list(Comprehension=c('F1F1','F1F2','F1F3','F1F4','F1F5','F1F6','F1F7','F1F8',
                           'F1F9','F1F10','F1F11','F1F12','F1F13','F1F14'),
           Evaluation=c('F2F1','F2F2','F2F3','F2F4','F2F5','F2F6','F2F7','F2F8',
                        'F2F9','F2F10','F2F11','F2F12','F2F13','F2F14','F2F15',
                        'F2F16','F2F17','F2F18','F2F19','F2F20'),
           Integration=c('F3F1','F3F2','F3F3','F3F4','F3F5','F3F6','F3F7','F3F8','F3F9'),
           Communication=c('F4F1','F4F2','F4F3','F4F4','F4F5','F4F6','F4F7','F4F8'),
           Statistics=c('F5F1','F5F2','F5F3','F5F4','F5F5','F5F6','F5F7','F5F8',
                        'F5F9','F5F10','F5F11','F5F12','F5F13','F5F14','F5F15',
                        'F5F16','F5F17','F5F18')
                           )
```

## ordinal data transformation?
```{r}
ords <- imputed_training_data_c[, names(imputed_training_data_c)%in%unlist(fs)]
ords <- lapply(ords, as.ordered)
ords <- do.call(data.frame, ords)
```

## Objective-Function
```{r}
library(stuart)

# Define the theoretical nu matrix 
theoretical_nu <- matrix(c(5, 4, 2, 1,  # Factor 1 items
                      5, 4, 2, 1,  # Factor 2 items
                      5, 4, 2, 1,  # Factor 3 items
                      5, 4, 2, 1,  # Factor 4 items
                      5, 4, 2, 1), # Factor 5 items
                    nrow = 4, byrow = TRUE,
                    dimnames = list(c("Item1", "Item2", "Item3", "Item4"),
                                    c("Factor1", "Factor2", "Factor3", "Factor4", "Factor5")))

# Convert the matrix to a vector
nu_vector <- c(theoretical_nu)

# Calculate the standard deviation of the vector
ideal_sd <- sd(nu_vector)

# Print the ideal standard deviation
print(ideal_sd) #1.62

# Use 'ideal_sd' in my objective function
#ordinal data
obj <- function(chisq, df, pvalue,rmsea.scaled, srmr, cfi.scaled, crel, nu) {
  out1 = 0.5-(0.5/(1 + exp(- 100 * (rmsea.scaled-.05))))
  out2 = 0.5-(0.5/(1 + exp(- 100 * (srmr-.05))))
  out3 = (1/(1 + exp(- 100 * (cfi.scaled-.95))))
  #out4 = -(sd(nu) - 1.62)^2 + 1.62^2 # Quadratic function peaking at 'ideal_sd'
  out = (out1 + out2 + out3 )/3 #+ out4)/4
  return(out)                                
}

#stuart:::objective.preset
obj_default <- function(chisq, df, pvalue, rmsea, srmr, crel) {
                      1/(1 + exp(-10 * (crel - 0.6))) + 0.5 * (1 - (1/(1 + exp(-100 * 
                     (rmsea - 0.05))))) + 0.5 * (1 - (1/(1 + exp(-100 * (srmr - 
                      0.06)))))
                       }
#stuart:::objective.preset2
obj_default2 <- function(chisq, df, pvalue, rmsea, srmr, crel, nu) {
                      1/(1 + exp(-10 * (crel - 0.6))) + 0.5 * (1 - (1/(1 + exp(-100 * 
                     (rmsea - 0.05))))) + 0.5 * (1 - (1/(1 + exp(-100 * (srmr - 
                      0.06))))) - (sd(nu) - 1.62)^2 + 1.62^2 + 1e-6
                       }

```


```{r}
combinations(imputed_training_data_c, fs,4)
```
## Genetic Algorithm
### Kfold-Crossvalidation
```{r}

############ k-Folding #########################

# erster der holdout wird an kfold weitergegeben

# Call the kfold function with factor.structure as an argument
kfold_sel <- kfold(
  'gene', 
  k = 3, 
  data = imputed_training_data_c, 
  factor.structure = fs,  # Pass the variable directly
  max.invariance = "strict", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2024,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 300,
  individuals = 64,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
#########################################################
summary(kfold_sel)

inspect(kfold_sel$final, 'est')
inspect(kfold_sel$final, 'fit')

lavaan::summary(kfold_sel$final, standardized = TRUE) 
#lavaan::inspect(selection$final, 'est')$nu
```
- First Time it worked:
SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 61760 
Time Required: 1175.532 seconds

Crossvalidation Results with STRICT Invariance:

Average Jaccard Similarity: Comprehension: 0.422, Evaluation: 0.295, Integration: 0.295, Communication: 0.600, Statistics: 0.359

Constructed Subtests: (k = 1)
Comprehension: F1F2 F1F6 F1F8 F1F10
Evaluation: F2F2 F2F4 F2F14 F2F15
Integration: F3F3 F3F4 F3F5 F3F9
Communication: F4F1 F4F3 F4F6 F4F7
Statistics: F5F3 F5F4 F5F15 F5F18

```{r}
# Call the kfold function with factor.structure as an argument
kfold_sel2 <- kfold(
  'gene', 
  k = 3, 
  data = imputed_training_data_c, 
  factor.structure = fs,  # Pass the variable directly
  max.invariance = "strict", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2024,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default2,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals =100,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
#########################################################
summary(kfold_sel2)

inspect(kfold_sel2$final, 'est')
inspect(kfold_sel2$final, 'fit')

lavaan::summary(kfold_sel2$final, standardized = TRUE) 
lavaan::inspect(kfold_sel2$final, 'est')$nu
```


```{r}
library(stuart)
# kfold & genetic loop paar mal laufen lassen, für stabile ergebnisse

gene <- gene(
  data = ords,                 # Your data frame containing the observed variables
  factor.structure <- fs, # A list defining the factor structure of your model
  capacity = 4,      # Optional, capacity constraint for factors
  auxiliary = NULL,                # Optional, specify auxiliary variables
  software = "lavaan",             # Specify the SEM software to be used
  cores = 1,                    # Number of CPU cores to be used for parallel processing
  objective = obj,                # Optional, specify a custom objective function
  ignore.errors = TRUE,           # Whether to ignore errors during fitness evaluation
  burnin = 5,                      # Number of generations for burn-in phase
  generations = 256,               # Total number of generations
  individuals = 64,                # Number of individuals in each generation
  selection = "tournament",        # Selection method for genetic algorithm
  selection.pressure = NULL,       # Pressure for tournament selection
  elitism = NULL,                  # Rate of elitism
  reproduction = 0.5,               # Proportion of individuals reproduced in each generation
  mutation = 0.05,                  # Mutation rate
  mating.index = 0,                 # Index of mating type
  mating.size = 0.25,               # Proportion of mating pool size
  mating.criterion = "similarity",  # Criterion for selecting mates
  immigration = 0,                  # Proportion of immigrants in each generation
  convergence.criterion = "geno.between",  # Convergence criterion
  tolerance = NULL,                 # Tolerance for convergence
  reinit.n = 1,                     # Number of reinitializations
  reinit.criterion = convergence.criterion,  # Criterion for reinitialization
  reinit.tolerance = NULL,          # Tolerance for reinitialization
  reinit.prop = 0.75,               # Proportion of population reinitialized
  schedule = "run",                 # Schedule for genetic algorithm
  analysis.options = NULL,          # Additional options for analysis
  suppress.model = FALSE,           # Whether to suppress model output
  seed = 2024,                      # Seed for random number generation
  filename = "itemselection"                   # Optional, filename for saving results
)


```

## Objective-Function ideas
```{r}

# matritzen mit objectivematrices function
# theoretisch festlegen, oder kombinatorisch/technisch festlegen? Random draws für fit indices? 
```

## Crossvalidation
```{r}
crossvalidate(imputed_training_data_c, imputed_test_data_c)

```

## Check correlated residuals
```{r}
# Fit the initial SEM model
model <- "
# Measurement model
y1 =~ x1 + x2 + x3 + x4
y2 =~ x5 + x6 + x7 + x8
y3 =~ x9 + x10 + x11 + x12
y4 =~ x13 + x14 + x15 + x16
y5 =~ x17 + x18 + x19 + x20

# Factor variances
y1 ~~ y1
y2 ~~ y2
y3 ~~ y3
y4 ~~ y4
y5 ~~ y5

# Factor correlations
y1 ~~ y2
y1 ~~ y3
y1 ~~ y4
y1 ~~ y5
y2 ~~ y3
y2 ~~ y4
y2 ~~ y5
y3 ~~ y4
y3 ~~ y5
y4 ~~ y5
"
fit <- sem(model, data = data)

# Check modification indices
modIndices(fit, sort_ = TRUE)

# If justified, re-specify your model to include correlated residuals
model_revised <- '
  # Your original model specification
  residual1 ~~ residual2 # Add this line to specify correlated residuals
'
fit_revised <- sem(model_revised, data = your_data)

# Compare the original and revised models
anova(fit, fit_revised)
```

## Convergent Measures
```{r}
SWE <- df3 %>%
  dplyr::select(starts_with("SWE")) %>%
  mutate_all(as.numeric)

NFC <- df3 %>%
  select(starts_with("NFC")) %>%
  mutate_all(as.numeric)

ICT <- df3 %>%
  select(starts_with("ICT")) %>%
  mutate_all(as.numeric)

B5_openness <- df3 %>%
  select(starts_with("B5.o")) %>%
  mutate_all(as.numeric)

B5_con <- df3 %>%
  select(starts_with("B5.G")) %>%
  mutate_all(as.numeric)
```

## Calculating meanscores
```{r}
# Calculate row means for each row and add as a new column
B5_con <- B5_con %>%
  rowwise() %>%
  mutate(B5con_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
B5_openness <- B5_openness %>%
  rowwise() %>%
  mutate(B5o_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
NFC <- NFC %>%
  rowwise() %>%
  mutate(Nfc_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
ICT <- ICT %>%
  rowwise() %>%
  mutate(Ict_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
SWE <- SWE %>%
  rowwise() %>%
  mutate(Swe_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

```

```{r}
# Combine the row means into a single data frame
all_means <- data.frame(
  B5con_Row_Means = B5_con$B5con_Row_Means,
  B5o_Row_Means = B5_openness$B5o_Row_Means,
  Nfc_Row_Means = NFC$Nfc_Row_Means,
  Ict_Row_Means = ICT$Ict_Row_Means,
  Swe_Row_Means = SWE$Swe_Row_Means
)
```

## Correlations 
```{r}
library(apaTables)
cortable<-cor(all_means, use = "pairwise" ,method = "pearson")
apa.cor.table(cortable)
```

```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Select newly created row means for correlation analysis
selected_vars <- all_means %>%
  select(B5con_Row_Means, B5o_Row_Means, Nfc_Row_Means, Ict_Row_Means, Swe_Row_Means)

# Rename variables for clarity in heatmap
colnames(selected_vars) <- c(
  "B5con", "B5o", "Nfc", "Ict", "Swe"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap3 <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Row Means of Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final3 <- heatmap3 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final3)

```

## Reliability
```{r}
omega(B5_con[1:2],1)
omega(B5_openness[1:2],1)
omega(NFC[1:4],1)
omega(ICT[1:5],1)
omega(SWE[1:16],1)
#omega(DataLiteracy[],5)
```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

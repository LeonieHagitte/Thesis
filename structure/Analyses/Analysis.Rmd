---
title: "Analysis"
author: "Leonie"
date: "2024-04-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# to clear environment while working
rm(list=ls()) 

usethis::use_git_config(
  user.name = "LeonieHagitte",
  user.email = "leonie.hagitte@student.hu-berlin.de",
  init.defaultBranch = "devl")

```

# Reproducibility 
```{r}
if(!requireNamespace("remotes"))
install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
repro::automate() #creating a Dockerfile.    ####################### BUG ######
repro::use_docker()
repro::use_gha_docker() #GitHub action (GHA) is a cloud service that runs software when certain events trigger it.Add a GitHub Action to build the required Docker image with

# Install or update 'renv' package from CRAN if needed
# Load 'renv' package
library(renv)

# Initialize 'renv' in your project
renv::init()

# Activate the project-specific 'renv' environment
renv::activate()

# Install required packages for your project
#renv::install()

# Update renv.lock file
renv::snapshot()

```

# Model specification
```{r}
model <- "
# Measurement model
y1 =~ x1 + x2 + x3 + x4
y2 =~ x5 + x6 + x7 + x8
y3 =~ x9 + x10 + x11 + x12
y4 =~ x13 + x14 + x15 + x16
y5 =~ x17 + x18 + x19 + x20

# Factor variances
y1 ~~ y1
y2 ~~ y2
y3 ~~ y3
y4 ~~ y4
y5 ~~ y5

# Factor correlations
y1 ~~ y2
y1 ~~ y3
y1 ~~ y4
y1 ~~ y5
y2 ~~ y3
y2 ~~ y4
y2 ~~ y5
y3 ~~ y4
y3 ~~ y5
y4 ~~ y5
"

```

# Data Preparation
## Data enreading
```{r}
library(dplyr)
library(mice)
library(readr)

dataMA <- read_csv("/Users/leo/Desktop/Programming stuff/Thesis/structure/Analyses/DataMA.csv")
View(dataMA)

# Convert all columns to character to ensure consistent replacement
df <- data.frame(lapply(dataMA, as.character))

# Replace "AO01" etc with respective numeric values across all columns
df[] <- lapply(df, function(x) gsub("AO01", "1", x))
df[] <- lapply(df, function(x) gsub("AO02", "2", x))
df[] <- lapply(df, function(x) gsub("AO03", "3", x))
df[] <- lapply(df, function(x) gsub("AO04", "4", x))
df[] <- lapply(df, function(x) gsub("AO05", "5", x))
df[] <- lapply(df, function(x) gsub("AO06", "6", x))
df[] <- lapply(df, function(x) gsub("AO07", "7", x))
df[] <- lapply(df, function(x) gsub("AO08", "8", x))
df[] <- lapply(df, function(x) gsub("AO09", "9", x))
df[] <- lapply(df, function(x) gsub("AO10", "10", x))

# Convert columns back to numeric if all values in a column can be converted
df <- data.frame(lapply(df, function(x) {
  converted <- suppressWarnings(as.numeric(x))
  if(all(!is.na(converted))) converted else x
}))

# Rename column "ICT.ATC3." to "ATC3"
names(df)[names(df) == "ICT.ATC3."] <- "ATC3"

```

## Attention checks
```{r}
# Calculate the number of wrong answers for each person

df$wrong_answers <- 0
df$wrong_answers <- df$wrong_answers + (df$ATC1 != 1)
df$wrong_answers <- df$wrong_answers + (df$ATC2 != 0)
df$wrong_answers <- df$wrong_answers + (df$ATC3 != 5)
# df$wrong_answers <- df$wrong_answers + (df$ATC4 != 2)

# Filter IDs of people who failed more than two items
failed_ids <- df$id[df$wrong_answers > 2]

# Display the IDs
failed_ids

# 46, 92, 275, 283, 419, 512, 588, 599, 613, 663, 668, 713

# Count the number of people who failed more than two items, ignoring NA values
num_failed_more_than_two <- sum(df$wrong_answers > 2, na.rm = TRUE)

# Display the count
num_failed_more_than_two
```

## Filter for stubborn people 
```{r}
# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Bei der ersten Kontrollfrage (für die Qualitätssicherheit) dachte ich, es wäre eine Fangfrage & habe NICHT das gedrückt, was ich sollte… ansonsten habe ich alles nach bestem Wissen und Gewissen ausgefüllt ????????????"]

# Display the ID
person_id #id=23

# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Die Fragen ohne Inhalt mit Klickanweisung waren bizarr. Hintergrund unverständlich und nicht erläutert. Deshalb mit „weiß nicht“ beantwortet"]

# Display the ID
person_id #id=71

## Very long Comment - I couldnt find with the original code, i had used up to this point.##
# Defining a pattern that captures the essence of the comment while allowing for some variation
# For simplicity, this example uses a shorter, distinctive part of the comment
# Adjust the pattern as needed to be specific enough to uniquely identify the comment
pattern <- "Die Umfrage war sehr erhellend.*menschlichen Mitteln.*Doktor Faust"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id=275

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Fragen irritiert. Ich habe sie nicht wie gefordert beantwortet, da mir nicht transparent war, wozu dies wirklich dient. Es fühlte sich mehr wie ein Experiment an: Machen die Leute, was man ihnen sagt?"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids # id=419

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Habe die Frage.*nicht verstanden und immer irgendetwas gedrückt.*"

# Use grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, ignore.case = TRUE, perl = TRUE)

# Use the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id= 760

# Excluding stubborn people from former failed id list

# 46, 92, 283, 512, 588, 599, 613, 663, 668, 713
# leaves 10 people to exclude

```
```{r}
# Define the IDs to exclude
ids_to_exclude <- c(46, 92, 283, 512, 588, 599, 613, 663, 668, 713,796)

# id 796 is to be excluded, because the person indicated an age of 17
# Filter out the rows with the specified IDs and save the result to a new dataframe
df2 <- df[!df$id %in% ids_to_exclude, ]

# Display the first few rows of the new dataframe to verify
head(df2)
```

```{r}
# Convert 'lastpage' to numeric
df2$lastpage <- as.numeric(df2$lastpage)

# Inspect unique values of 'lastpage' after conversion
unique(df2$lastpage)

# Exclude rows where 'lastpage' is smaller than 6
df3 <- df2[df2$lastpage >= 6, ]

# This code updates df3 to keep only the rows where lastpage doesnt contain NA values.

# Check if 'lastpage' column exists in df3
if("lastpage" %in% names(df3)) {
  # If it exists, filter out NA values from 'lastpage'
  df3 <- df3[!is.na(df3$lastpage), ]
} else {
  cat("Column 'lastpage' does not exist in df3\n")
}

```

```{r, echo=FALSE}
# Recode columns 9 to 109 as numeric
df3[, 9:109] <- lapply(df3[, 9:109], as.numeric)

# Check the structure of the first few columns to confirm the change
str(df3[, 9:109])
```

```{r}
# Subset df3 to include only columns up to column 119
df3 <- df3[, 1:119]

# Check the structure of the modified dataframe to confirm the change
str(df3)

# Exclude columns 2, 4, 5, 6, 7, 8 and keep only up to column 119
df3 <- df3[, -c(2, 4, 5, 6, 7, 8)]

```

```{r}

```

```{r}

```

```{r}

```

# checking for the Pattern of Missings
Although it may not be necessary, because the missings are planned, random missings, one could check for the pattern of the 
missing data.
```{r}

```

## NAs FIMLn
```{r}
library(lavaan)
model_fiml <- sem('
                # Measurement model
                y1 =~ x1 + x2 + x3 + x4
                y2 =~ x5 + x6 + x7 + x8
                y3 =~ x9 + x10 + x11 + x12
                y4 =~ x13 + x14 + x15 + x16
                y5 =~ x17 + x18 + x19 + x20
                
                # Factor variances
                y1 ~~ y1
                y2 ~~ y2
                y3 ~~ y3
                y4 ~~ y4
                y5 ~~ y5
                
                # Factor correlations
                y1 ~~ y2
                y1 ~~ y3
                y1 ~~ y4
                y1 ~~ y5
                y2 ~~ y3
                y2 ~~ y4
                y2 ~~ y5
                y3 ~~ y4
                y3 ~~ y5
                y4 ~~ y5
              ', data = df_subset, missing = 'fiml', fixed.x = F) 
# fixed.x = F. FIML works by estimating the relationships of the variables with each other and requires estimating the means and variances of the variables. If fixed.x = T (the default), then the variances and covariances are fixed and are based on the existing sample values and are not estimated.
summary(model_fiml)
parameterestimates(model_fiml)

```

```{r}

```

# Poweranalyse
```{r}
install.packages('semPower')
devtools::install_github('martscht/stuart/stuart', ref = 'develop')
library(semPower)
library(stuart)
library(lavaan)
```
 
# Poweranalyse - SemPower mit Model
```{r}
powerMI <- semPower.powerMI(
  type = 'a-priori',
  comparison = 'configural',
  nullEffect = 'metric',
  nIndicator = c(4, 4, 4, 4, 4), # Corrected to a single vector
  loadM = list(.5, .6), # Assuming baseline loadings for two groups
  tau = list(rep(0.0, 20), # Assuming intercepts for 20 indicators in the first group
             rep(seq(.1, .5, .1), each = 4)), # Assuming intercepts for 20 indicators in the second group, with increments
  alpha = .05, 
  power = .80,
  N = list(.80, .20) # Assuming 80/20 sized groups for split
)
# Show summary
summary(powerMI)

```
## Results Printed
semPower: A priori power analysis
                                     
 F0                        0.033897  
 RMSEA                     0.058221  
 Mc                        0.983194  
                                     
 df                        20        
 Required Num Observations 622       
                           (498, 125)
                                     
 Critical Chi-Square       31.41043  
 NCP                       21.05165  
 Alpha                     0.050000  
 Beta                      0.197753  
 Power (1 - Beta)          0.802247  
 Implied Alpha/Beta Ratio  0.252841  


# Poweranalyse - SemPower ohne Model für Sample Split
```{r}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 155)
summary(ap)
```
semPower: A priori power analysis
                                   
 F0                        0.387500
 RMSEA                     0.050000
 Mc                        0.823864
                                   
 df                        155     
 Required Num Observations 128     
                                   
 Critical Chi-Square       185.0523
 NCP                       49.21250
 Alpha                     0.050000
 Beta                      0.199807
 Power (1 - Beta)          0.800193
 Implied Alpha/Beta Ratio  0.250242


## Descriptive Analyses
```{r}
# packages
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)

# import data
# Create a subset of df3 with columns 104 to 112
demographics <- df3[, 104:112]

```

## Sample
```{r}
# gender (2 = male, female = 1, 3 = divers)
round(prop.table(table(demographics$Gender))*100, 1)

# year of birth
range(demographics$Age, na.rm = TRUE)
round(mean(demographics$Age, na.rm = TRUE), 2)
round(sd(demographics$Age, na.rm = TRUE), 2)

```

```{r}
# prepare subsets for my variables
describe_age <- data_wide %>% dplyr::select(starts_with("age"))
describe_eurod <- data_wide %>% dplyr::select(starts_with("eurod"))
describe_finances <- data_wide %>% dplyr::select(starts_with("finances"))

```

```{r}
# create table for age
table_age <- data.frame(colMeans(is.na(describe_age)))
colnames(table_age) <- "Age"
rownames(table_age) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")

# create table for EURO-D
table_eurod <- data.frame(colMeans(is.na(describe_eurod)))
colnames(table_eurod) <- "EURO-D"
rownames(table_eurod) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")

# create table for finances
table_finances <- data.frame(colMeans(is.na(describe_finances)))
colnames(table_finances) <- "Finances"
rownames(table_finances) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")

# combine tables
table_missings <- data.frame(table_age, table_eurod, table_finances, check.names = FALSE)
# round
table_missings <- table_missings %>% mutate_if(is.numeric, round, digits = 2) * 100
# add % symbol
table_missings <- table_missings %>% 
  mutate(across(everything(), ~paste0(., "%")))
table_missings # print for HTML

```

```{r}
# for Age
table_descriptive_age <- round(psych::describe(describe_age), 2)
rownames(table_descriptive_age) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")
table_descriptive_age # print for HTML

# for EURO-D
table_descriptive_eurod <- round(psych::describe(describe_eurod), 2)
rownames(table_descriptive_eurod) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")
table_descriptive_eurod # print for HTML

# for finances
table_descriptive_finances <- round(psych::describe(describe_finances), 2)
rownames(table_descriptive_finances) <- c("Wave 4", "Wave 5", "Wave 6", "Wave 7", "Wave 8")
table_descriptive_finances # print for HTML

```

## Heatmap
```{r}
# for EURO-D + finances over time
  # create correlation matrix
both_correlations <- data_wide %>% dplyr::select(starts_with("eurod"), starts_with("finances"))
  # rename variables for clarity
both_correlations <- both_correlations %>% rename(
  "EURO-D (wave 4)" = eurod.4,
  "EURO-D (wave 5)" = eurod.5,
  "EURO-D (wave 6)" = eurod.6,
  "EURO-D (wave 7)" = eurod.7,
  "EURO-D (wave 8)" = eurod.8,
  "Finances (wave 4)" = finances.4,
  "Finances (wave 5)" = finances.5,
  "Finances (wave 6)" = finances.6,
  "Finances (wave 7)" = finances.7,
  "Finances (wave 8)" = finances.8)
  # get correlations
both_matrix <- round(corr.test(both_correlations, use = "pairwise")$r, 2)
  # only use upper part of matrix
both_matrix_lower_part <- function(both_matrix){
  both_matrix[lower.tri(both_matrix)]<- NA
  return(both_matrix)}
both_matrix_up <- both_matrix_lower_part(both_matrix)
  # reduce size of correlation matrix/long format
both_matrix_melted <- melt(both_matrix_up, na.rm = TRUE)
  # plotting heatmap 
both_heatmap <- ggplot(data = both_matrix_melted, aes(Var2, Var1, fill = value)) +
 ggtitle("Correlations - EURO-D + Finances") +
 geom_tile(color = "white") +
 scale_fill_gradient2(low = "blue", high = "red", mid = "white",
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelations") +
  theme_minimal() + 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1), axis.text.y = element_text(size = 8), plot.title = element_text(size = 12), legend.position = "right", legend.title = element_text(size = 8)) +
 coord_fixed()
  # add correlation coefficients
heatmap_final <- both_heatmap + 
geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.6, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.3))
heatmap_final # print for HTML

# check p-values
both_matrix_p <- round(corr.test(both_correlations, use = "pairwise")$p, 5)
both_matrix_p # print for HTML

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```


# Stuart - Item selection
## Sample split
```{r}
library(stuart)
###########################
data()

split <- holdout(data, prop = c(0.8, 0.20), grouping = NULL, seed = 2024, determined = NULL)
lapply(split, nrow) #check size of sample

```

## Genetic Algorithm
```{r}
library(stuart)
# mmas freier in der spezifikation, variabler
# kfold & genetic loop paar mal laufen lassen, für stabile ergebnisse

gene(
  data,                 # Your data frame containing the observed variables
  factor.structure,     # A list defining the factor structure of your model
  capacity = NULL,      # Optional, capacity constraint for factors
  item.weights = NULL,  # Optional, initial weights for each indicator
  item.invariance = "congeneric",  # Specify the type of invariance assumption for items
  repeated.measures = NULL,        # Optional, specify repeated measures design
  grouping = NULL,                 # Optional, specify grouping variable for multigroup analysis
  group.invariance = "strict",     # Specify the type of invariance assumption for multigroup analysis
  comparisons = NULL,              # Optional, specify comparisons for multigroup analysis
  auxiliary = NULL,                # Optional, specify auxiliary variables
  use.order = FALSE,               # Whether to use order constraints
  software = "lavaan",             # Specify the SEM software to be used
  cores = NULL,                    # Number of CPU cores to be used for parallel processing
  objective = NULL,                # Optional, specify a custom objective function
  ignore.errors = FALSE,           # Whether to ignore errors during fitness evaluation
  burnin = 5,                      # Number of generations for burn-in phase
  generations = 256,               # Total number of generations
  individuals = 64,                # Number of individuals in each generation
  selection = "tournament",        # Selection method for genetic algorithm
  selection.pressure = NULL,       # Pressure for tournament selection
  elitism = NULL,                  # Rate of elitism
  reproduction = 0.5,               # Proportion of individuals reproduced in each generation
  mutation = 0.05,                  # Mutation rate
  mating.index = 0,                 # Index of mating type
  mating.size = 0.25,               # Proportion of mating pool size
  mating.criterion = "similarity",  # Criterion for selecting mates
  immigration = 0,                  # Proportion of immigrants in each generation
  convergence.criterion = "geno.between",  # Convergence criterion
  tolerance = NULL,                 # Tolerance for convergence
  reinit.n = 1,                     # Number of reinitializations
  reinit.criterion = convergence.criterion,  # Criterion for reinitialization
  reinit.tolerance = NULL,          # Tolerance for reinitialization
  reinit.prop = 0.75,               # Proportion of population reinitialized
  schedule = "run",                 # Schedule for genetic algorithm
  analysis.options = NULL,          # Additional options for analysis
  suppress.model = FALSE,           # Whether to suppress model output
  seed = NULL,                      # Seed for random number generation
  filename = NULL                   # Optional, filename for saving results
)

```

## Objective-Function
```{r}
# threshhold for reliability
# correlations?
# variance in item difficulty - via intercepts?? -> intercepts  matrix in lavaan 
# "meanstructure" ?

# Define a matrix for item intercepts per factor
# Assuming a structure of 4 factors with 4 items each for simplicity
# Mu?
# interceptsMatrix
# int.ov 
lvMu <- matrix(c(5, 4, 2, 1,  # Factor 1 items
                 5, 4, 2, 1,  # Factor 2 items
                 5, 4, 2, 1,  # Factor 3 items
                 5, 4, 2, 1,  # Factor 4 items
                 5, 4, 2, 1), # Factor 5 items
                 nrow = 4, byrow = TRUE,
                 dimnames = list(c("Item1", "Item2", "Item3", "Item4"),
                                 c("Factor1", "Factor2", "Factor3", "Factor4", "Factor5")))


fixedobjective(
  criteria = c("rmsea", "srmr", "crel"), #rel ohne"c" für einzelne faktoren 
  add = c("chisq", "df", "pvalue"),
  side = NULL, #can this be specified for certain values?
  scale = 1,
  matrices = NULL,
  fixed = NULL, #weitere function einfügen - if-then function etc. subreliabilitäten 
  comparisons = NULL
)

# matritzen mit objectivematrices function
# theoretisch festlegen, oder kombinatorisch/technisch festlegen? Random draws für fit indices? 



#Fixed STUART objective function with:

#1 * pnorm(rmsea, 0.05, 0.015, lower.tail = FALSE) + 1 * pnorm(srmr, 0.05, 0.015, lower.tail = FALSE) + 1 * pnorm(crel, 0.8, 0.075, lower.tail = TRUE)

####stuart:::objective.preset + interceptsMatrix
obj.preset <- function (chisq, df, pvalue, rmsea, srmr, crel,lvMu) 
{
    1/(1 + exp(-10 * (crel - 0.6))) + 0.5 * (1 - (1/(1 + exp(-100 * 
    (rmsea - 0.05))))) + 0.5 * (1 - (1/(1 + exp(-100 * (srmr - 0.06)))))
}
###################### 
obj <- function(chisq, df, pvalue, rmsea, srmr, cfi, crel,lvMu) {
  (1-rmsea) + (1-srmr) 
}

#######################
#ordinal data
objective.normal <- function(chisq, df, pvalue,rmsea.scaled, srmr, cfi.scaled, crel, lvMu) {
  out1 = 0.5-(0.5/(1 + exp(- 100 * (rmsea.scaled-.05))))
  out2 = 0.5-(0.5/(1 + exp(- 100 * (srmr-.05))))
  out3 = (1/(1 + exp(- 100 * (cfi.scaled-.95))))
  out = (out1 + out2 + out3)/3
  return(out)                                
}

```

## Crossvalidation or kfold-Crossvalidation
```{r}

# Using the 'holdout' function for data split
data(data)
# split was defined above!

########################################################
# Simple example from gene
fs <- listlist(faktor1 =c('item1','item2','item3',.....),
            faktor2 = c('item17', 'item18','item19',....),
           faktor3 = c('item27', 'item28','item29',....),
           faktor4 = c('item37', 'item38','item39',....)
           ) # adapt the names for the respective factors and items
sel <- gene(split, fs, 4, objective = obj, cores = 1, seed = 2024)  # number of cores set to 1 - anzahl über N pro gen unsinnig, default nimmt alle cores

############ k-Folding #########################

# erster der holdout wird an kfold weitergegeben

data(data)
fs <- list(ra = names(data)[53:57]) # adapt the columns for the respective factor columns

sel <- kfold('gene', k = 5, #gene/mmas #how to arrive at number of folds?
  data = data, factor.structure = fs,
  max.invariance = "scalar", ## randomly sampled aus derselben population - höher schon besser/ realistisch 
  capacity = 3, seed = 2024, #what does the "capacity" do?
  seeded.search = TRUE,
  remove.details = TRUE,
  cores = 1) #what does the "cores" do?
#########################################################
summary(sel)

inspect(sel$final, 'est')
inspect(sel$final, 'fit')

lavaan::summary(sel$final, standardized = TRUE) # adapt final as name ggf

# Crossvalidation
crossvalidate(sel, split)

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

---
title: "Analysis"
author: "Leonie"
date: "2024-04-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# to clear environment while working
# and setting my account in github
rm(list=ls()) 

usethis::use_git_config(
  user.name = "LeonieHagitte",
  user.email = "leonie.hagitte@student.hu-berlin.de",
  init.defaultBranch = "devl")
```

```{r, echo=FALSE}
library(renv)
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)
library(dplyr)
library(mice)
library(readr)
library(lavaan)
library(semPower)
library(stuart)
library(reproducibleRchunks)

```

# Reproducibility 
```{r}
if(!requireNamespace("remotes"))
install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
repro::automate() #creating a Dockerfile.    ####################### BUG ######
repro::use_docker()
repro::use_gha_docker() #GitHub action (GHA) is a cloud service that runs software when certain events trigger it.Add a GitHub Action to build the required Docker image with

library(renv)
setwd("/Users/leo/Desktop/Programming stuff/Thesis")
# Initialize 'renv' in your project
renv::init()

# Activate the project-specific 'renv' environment
renv::activate()

# Install required packages for your project
#renv::install()

# Update renv.lock file
renv::snapshot()

if(!requireNamespace("pak"))
install.packages("pak")
pak::pkg_install("brandmaier/reproducibleRchunks")

```

# Data Preparation
## Data enreading
```{r}
library(dplyr)
library(mice)
library(readr)

dataMA <- read_csv("/Users/leo/Desktop/Programming stuff/Thesis/structure/Analyses/DataMA.csv")

# Convert all columns to character to ensure consistent replacement
df <- data.frame(lapply(dataMA, as.character))

# Replace "AO01" etc with respective numeric values across all columns
df[] <- lapply(df, function(x) gsub("AO01", "1", x))
df[] <- lapply(df, function(x) gsub("AO02", "2", x))
df[] <- lapply(df, function(x) gsub("AO03", "3", x))
df[] <- lapply(df, function(x) gsub("AO04", "4", x))
df[] <- lapply(df, function(x) gsub("AO05", "5", x))
df[] <- lapply(df, function(x) gsub("AO06", "6", x))
df[] <- lapply(df, function(x) gsub("AO07", "7", x))
df[] <- lapply(df, function(x) gsub("AO08", "8", x))
df[] <- lapply(df, function(x) gsub("AO09", "9", x))
df[] <- lapply(df, function(x) gsub("AO10", "10", x))

# Convert columns back to numeric if all values in a column can be converted
df <- data.frame(lapply(df, function(x) {
  converted <- suppressWarnings(as.numeric(x))
  if(all(!is.na(converted))) converted else x
}))

# Rename column "ICT.ATC3." to "ATC3"
names(df)[names(df) == "ICT.ATC3."] <- "ATC3"

```

## Attention checks
```{reproducibleR}
# Calculate the number of wrong answers for each person

df$wrong_answers <- 0
df$wrong_answers <- df$wrong_answers + (df$ATC1 != 1)
df$wrong_answers <- df$wrong_answers + (df$ATC2 != 0)
df$wrong_answers <- df$wrong_answers + (df$ATC3 != 5)
# df$wrong_answers <- df$wrong_answers + (df$ATC4 != 2)

# Filter IDs of people who failed more than two items
failed_ids <- df$id[df$wrong_answers > 2]

# Display the IDs
failed_ids

# 46, 92, 275, 283, 419, 512, 588, 599, 613, 663, 668, 713

# Count the number of people who failed more than two items, ignoring NA values
num_failed_more_than_two <- sum(df$wrong_answers > 2, na.rm = TRUE)

# Display the count
num_failed_more_than_two
```

## Filter for non-compliant people 
```{reproducibleR}
# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Bei der ersten Kontrollfrage (für die Qualitätssicherheit) dachte ich, es wäre eine Fangfrage & habe NICHT das gedrückt, was ich sollte… ansonsten habe ich alles nach bestem Wissen und Gewissen ausgefüllt ????????????"]

# Display the ID
person_id #id=23

# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Die Fragen ohne Inhalt mit Klickanweisung waren bizarr. Hintergrund unverständlich und nicht erläutert. Deshalb mit „weiß nicht“ beantwortet"]

# Display the ID
person_id #id=71

## Very long Comment - I couldnt find with the original code, i had used up to this point.##
# Defining a pattern that captures the essence of the comment while allowing for some variation
# For simplicity, this example uses a shorter, distinctive part of the comment
# Adjust the pattern as needed to be specific enough to uniquely identify the comment
pattern <- "Die Umfrage war sehr erhellend.*menschlichen Mitteln.*Doktor Faust"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id=275

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Fragen irritiert. Ich habe sie nicht wie gefordert beantwortet, da mir nicht transparent war, wozu dies wirklich dient. Es fühlte sich mehr wie ein Experiment an: Machen die Leute, was man ihnen sagt?"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids # id=419

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Habe die Frage.*nicht verstanden und immer irgendetwas gedrückt.*"

# Use grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, ignore.case = TRUE, perl = TRUE)

# Use the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id= 760

# Excluding stubborn people from former failed id list

# 46, 92, 283, 512, 588, 599, 613, 663, 668, 713
# leaves 10 people to exclude

```

```{reproducibleR}
# Define the IDs to exclude
ids_to_exclude <- c(46, 92, 283, 512, 588, 599, 613, 663, 668, 713,796)

# id 796 is to be excluded, because the person indicated an age of 17
# Filter out the rows with the specified IDs and save the result to a new dataframe
df2 <- df[!df$id %in% ids_to_exclude, ]

# Display the first few rows of the new dataframe to verify
head(df2)
```

```{reproducibleR}
# Convert 'lastpage' to numeric
df2$lastpage <- as.numeric(df2$lastpage)

# Inspect unique values of 'lastpage' after conversion
unique(df2$lastpage)

# Exclude rows where 'lastpage' is smaller than 6
df3 <- df2[df2$lastpage >= 6, ]

# This code updates df3 to keep only the rows where lastpage doesnt contain NA values.

# Check if 'lastpage' column exists in df3
if("lastpage" %in% names(df3)) {
  # If it exists, filter out NA values from 'lastpage'
  df3 <- df3[!is.na(df3$lastpage), ]
} else {
  cat("Column 'lastpage' does not exist in df3\n")
}

```

```{reproducibleR, echo=FALSE}
# Recode columns 9 to 109 as numeric
df3[, 9:109] <- lapply(df3[, 9:109], as.numeric)

# Check the structure of the first few columns to confirm the change
str(df3[, 9:109])
```

```{reproducibleR, echo=FALSE}
# Subset df3 to include only columns up to column 119
df3 <- df3[, 1:119]

# Check the structure of the modified dataframe to confirm the change
str(df3)

# Exclude columns 2, 4, 5, 6, 7, 8 and keep only up to column 119
df3 <- df3[, -c(2, 4, 5, 6, 7, 8)]

# Exclude the ATCs
df3 <- df3[, !(names(df3) %in% c("ATC1", "ATC2"))]

```

## kann raus? df99
```{reproducibleR}
df99 <- data.frame(df2)
# Convert 'lastpage' to numeric
df99$lastpage <- as.numeric(df2$lastpage)

# Inspect unique values of 'lastpage' after conversion
unique(df99$lastpage)

# Exclude rows where 'lastpage' is smaller than 6
df99 <- df99[df99$lastpage >= 13, ]

# This code updates df3 to keep only the rows where lastpage doesnt contain NA values.

# Check if 'lastpage' column exists in df3
if("lastpage" %in% names(df99)) {
  # If it exists, filter out NA values from 'lastpage'
  df99 <- df99[!is.na(df99$lastpage), ]
} else {
  cat("Column 'lastpage' does not exist in df99\n")
}

```

```{reproducibleR, echo=FALSE}
# Recode columns 9 to 109 as numeric
df99[, 9:109] <- lapply(df99[, 9:109], as.numeric)

# Check the structure of the first few columns to confirm the change
str(df99[, 9:109])
```

```{reproducibleR, echo=FALSE}
# Subset df3 to include only columns up to column 119
df99 <- df99[, 1:119]

# Check the structure of the modified dataframe to confirm the change
str(df99)

# Exclude columns 2, 4, 5, 6, 7, 8 and keep only up to column 119
df99 <- df99[, -c(2, 4, 5, 6, 7, 8)]

# Exclude the ATCs
df99 <- df99[, !(names(df99) %in% c("ATC1", "ATC2"))]

```

## NAs FIMLn
```{reproducibleR}
df_dl <- df3[, 3:71]
#df_dl99 <- df99[, 3:71]

```

```{reproducibleR}
library(dplyr)

# Calculate the number of columns
num_columns <- ncol(df_dl)

# Filter rows where most columns have a value of 0
filtered_df_dl <- df_dl %>%
  filter(rowSums(. == 0) >= num_columns * 0.8)  # Adjust the threshold as needed
```
## Outlier
```{r}
# Create boxplots for all variables in df_dl
boxplot(df_dl, main = "Boxplot for Outlier Detection", col = "lightblue", las = 2, outline = TRUE)

```
## Outliers, but keep ceiling effects in mind
```{r}
# Compute Z-scores for each variable in df_dl
z_scores <- scale(df_dl)

# Identify outliers based on Z-scores (absolute value greater than 3)
outliers_z_scores <- which(abs(z_scores) > 3, arr.ind = TRUE)
print(outliers_z_scores)

```

## or using mice?
```{r, echo=FALSE}
imputedData <- mice(df_dl, m=5, method='pmm', maxit=50)
completedData <- complete(imputedData, 1)
```

# Poweranalyse
```{r}
install.packages('semPower')
devtools::install_github('martscht/stuart/stuart', ref = 'develop')
library(semPower)
library(stuart)
library(lavaan)
```
 
# Poweranalyse - SemPower mit Model
```{r}
powerMI <- semPower.powerMI(
  type = 'a-priori',
  comparison = 'configural',
  nullEffect = 'metric',
  nIndicator = c(4, 4, 4, 4, 4), # Corrected to a single vector
  loadM = list(.5, .6), # Assuming baseline loadings for two groups
  tau = list(rep(0.0, 20), # Assuming intercepts for 20 indicators in the first group
             rep(seq(.1, .5, .1), each = 4)), # Assuming intercepts for 20 indicators in the second group, with increments
  alpha = .05, 
  power = .80,
  N = list(.80, .20) # Assuming 80/20 sized groups for split
)
# Show summary
summary(powerMI)

```
## Results Printed
semPower: A priori power analysis
                                     
 F0                        0.033897  
 RMSEA                     0.058221  
 Mc                        0.983194  
                                     
 df                        20        
 Required Num Observations 622       
                           (498, 125)
                                     
 Critical Chi-Square       31.41043  
 NCP                       21.05165  
 Alpha                     0.050000  
 Beta                      0.197753  
 Power (1 - Beta)          0.802247  
 Implied Alpha/Beta Ratio  0.252841  


# Poweranalyse - SemPower ohne Model für Sample Split
```{r}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 155)
summary(ap)
```
semPower: A priori power analysis
                                   
 F0                        0.387500
 RMSEA                     0.050000
 Mc                        0.823864
                                   
 df                        155     
 Required Num Observations 128     
                                   
 Critical Chi-Square       185.0523
 NCP                       49.21250
 Alpha                     0.050000
 Beta                      0.199807
 Power (1 - Beta)          0.800193
 Implied Alpha/Beta Ratio  0.250242


## Descriptive Analyses
```{r}
# packages
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)

# Create a subset of df3 with columns 104 to 112
demographics <- df3[, 102:111]

```

## Sample
```{reproducibleR}
# gender (2 = male, female = 1, 3 = divers)
round(prop.table(table(demographics$Gender))*100, 1)

# year of birth
range(demographics$Age, na.rm = TRUE)
round(mean(demographics$Age, na.rm = TRUE), 2)
round(sd(demographics$Age, na.rm = TRUE), 2)

```

```{r}
# prepare subsets for my variables
describe_age <- demographics %>%
  dplyr::select(starts_with("Age")) %>%
  mutate_all(as.numeric)

describe_gender <- demographics %>%
  select(starts_with("Gender")) %>%
  mutate_all(as.numeric)

describe_education <- demographics %>%
  select(starts_with("Education")) %>%
  mutate_all(as.numeric)

describe_degree <- demographics %>%
  select(starts_with("Degree")) %>%
  mutate_all(as.numeric)

describe_workfilter <- demographics %>%
  select(starts_with("Workfilter")) %>%
  mutate(
    Workfilter = case_when(
      Workfilter == "Y" ~ 1,
      Workfilter == "N" ~ 0,
      TRUE ~ NA_integer_  # handle unexpected values, if any
    )
  )

describe_study <- demographics %>%
  select(starts_with("study")) %>%
  mutate(
    study = case_when(
      study == "Y" ~ 1,
      study == "N" ~ 0,
      TRUE ~ NA_integer_  # handle unexpected values, if any
    )
  )

describe_worktime <- demographics %>%
  select(starts_with("Worktime")) %>%
  mutate_all(as.numeric)

describe_occupation <- demographics %>%
  select(starts_with("Occupation")) %>%
  mutate_all(as.numeric)

```

```{r}
# create table for age
table_age <- data.frame(colMeans(is.na(describe_age)))
colnames(table_age) <- "Age"

# create table for Gender
table_gender <- data.frame(colMeans(is.na(describe_gender)))
colnames(table_gender) <- "Gender"

# create table for Education
table_education <- data.frame(colMeans(is.na(describe_education)))
colnames(table_education) <- "Education"

# create table for Studying
table_study <- data.frame(colMeans(is.na(describe_study)))
colnames(table_age) <- "Study"

# create table for Degree
table_degree <- data.frame(colMeans(is.na(describe_degree)))
colnames(table_degree) <- "Degree"

# create table for Working Y/N
table_workfilter <- data.frame(colMeans(is.na(describe_workfilter)))
colnames(table_workfilter) <- "Workfilter"

# create table for Worktime
table_worktime <- data.frame(colMeans(is.na(describe_worktime)))
colnames(table_worktime) <- "Worktime"

# create table for Occupation
table_occupation <- data.frame(colMeans(is.na(describe_occupation)))
colnames(table_occupation) <- "Occupation"

# combine tables
table_missings <- data.frame(table_age, table_gender, table_education, table_study,
                             table_degree, table_workfilter, table_worktime, table_occupation, check.names = FALSE)
# round
table_missings <- table_missings %>% mutate_if(is.numeric, round, digits = 2) * 100

# add % symbol
table_missings <- table_missings %>% 
  mutate(across(everything(), ~paste0(., "%")))
table_missings # print for HTML

```

```{r}
library(dplyr)
library(psych)
library(rempsyc)
library(flextable)

# Example: Define and round descriptive statistics for each variable
table_descriptive_age <- round(psych::describe(describe_age), 2)
table_descriptive_gender <- round(psych::describe(describe_gender), 2)
table_descriptive_education <- round(psych::describe(describe_education), 2)
table_descriptive_study <- round(psych::describe(describe_study), 2)
table_descriptive_degree <- round(psych::describe(describe_degree), 2)
table_descriptive_workfilter <- round(psych::describe(describe_workfilter), 2)
table_descriptive_worktime <- round(psych::describe(describe_worktime), 2)
table_descriptive_occupation <- round(psych::describe(describe_occupation), 2)

# Combine all tables into a single data frame row-wise
table_descriptives <- bind_rows(
  table_descriptive_age,
  table_descriptive_gender,
  table_descriptive_education,
  table_descriptive_study,
  table_descriptive_degree,
  table_descriptive_workfilter,
  table_descriptive_worktime,
  table_descriptive_occupation
)

print(table_descriptives)

# Convert row names to a column in your data frame
table_descriptives <- table_descriptives %>%
  rownames_to_column(var = "Variables")

# Combine columns 3 to 5 and column 23 for selection
selected_columns <- c(1,4:6, 14)

# Use the combined columns in the nice_table function
nice_table(table_descriptives[selected_columns], stars = TRUE, title = "Sample Descriptives", note = "Descriptive Statistics on the demographic variables. sd = Standard deviation; se = Standard error")

```

```{r}
library(dplyr)

# Calculate the mean, SD, and frequencies for age by gender
mean_sd_age_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_age = mean(Age, na.rm = TRUE),
    sd_age = sd(Age, na.rm = TRUE),
    count_age = n()
  )

# Calculate the mean, SD, and frequencies for education by gender
mean_sd_education_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_education = mean(Education, na.rm = TRUE),
    sd_education = sd(Education, na.rm = TRUE),
    count_education = n()
  )

# Calculate the mean, SD, and frequencies for degree by gender
mean_sd_degree_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_degree = mean(Degree, na.rm = TRUE),
    sd_degree = sd(Degree, na.rm = TRUE),
    count_degree = n()
  )

# Calculate the mean, SD, and frequencies for worktime by gender
mean_sd_worktime_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_worktime = mean(Worktime, na.rm = TRUE),
    sd_worktime = sd(Worktime, na.rm = TRUE),
    count_worktime = n()
  )

# Calculate the mean, SD, and frequencies for occupation by gender
mean_sd_occupation_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_occupation = mean(Occupation, na.rm = TRUE),
    sd_occupation = sd(Occupation, na.rm = TRUE),
    count_occupation = n()
  )


# Print the resulting data frames
print(mean_sd_age_by_gender)
print(mean_sd_education_by_gender)
print(mean_sd_degree_by_gender)
print(mean_sd_worktime_by_gender)
print(mean_sd_occupation_by_gender)
```

```{r}
# Load the necessary library
library(dplyr)

# Assuming demographics is your data frame and it has the relevant columns

# Calculate the mean, SD, and frequencies for age by gender
table2_descriptive_age <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_age = mean(Age, na.rm = TRUE),
    sd_age = sd(Age, na.rm = TRUE),
    count_age = n()
  )

# Calculate the mean, SD, and frequencies for education by gender
table2_descriptive_education <- demographics %>%
  group_by(Gender, Education) %>%
  summarize(
    mean_education = mean(Education, na.rm = TRUE),
    sd_education = sd(Education, na.rm = TRUE),
    count_education = n()
  ) %>%
  ungroup()

# Calculate the mean, SD, and frequencies for degree by gender
table2_descriptive_degree <- demographics %>%
  group_by(Gender, Degree) %>%
  summarize(
    mean_degree = mean(Degree, na.rm = TRUE),
    sd_degree = sd(Degree, na.rm = TRUE),
    count_degree = n()
  ) %>%
  ungroup()

# Calculate the mean, SD, and frequencies for worktime by gender
table2_descriptive_worktime <- demographics %>%
  group_by(Gender, Worktime) %>%
  summarize(
    mean_worktime = mean(Worktime, na.rm = TRUE),
    sd_worktime = sd(Worktime, na.rm = TRUE),
    count_worktime = n()
  ) %>%
  ungroup()

# Calculate the mean, SD, and frequencies for occupation by gender
table2_descriptive_occupation <- demographics %>%
  group_by(Gender, Occupation) %>%
  summarize(
    mean_occupation = mean(Occupation, na.rm = TRUE),
    sd_occupation = sd(Occupation, na.rm = TRUE),
    count_occupation = n()
  ) %>%
  ungroup()

# Print the resulting data frames
print(table2_descriptive_age)
print(table2_descriptive_education)
print(table2_descriptive_degree)
print(table2_descriptive_worktime)
print(table2_descriptive_occupation)
```

```{r}
# Load the necessary libraries
library(dplyr)
library(tidyr)

# Function to calculate frequencies and percentages
calculate_frequencies <- function(data, variable) {
  data %>%
    mutate(Gender = as.factor(Gender), !!sym(variable) := as.factor(!!sym(variable))) %>%
    group_by(Gender, !!sym(variable)) %>%
    summarize(count = n(), .groups = 'drop') %>%
    ungroup() %>%
    group_by(Gender) %>%
    mutate(percentage = count / sum(count) * 100) %>%
    ungroup() %>%
    complete(Gender, !!sym(variable), fill = list(count = 0, percentage = 0)) %>%
    mutate(Gender = as.character(Gender)) %>%
    bind_rows(
      data %>%
        mutate(!!sym(variable) := as.factor(!!sym(variable))) %>%
        group_by(!!sym(variable)) %>%
        summarize(count = n(), .groups = 'drop') %>%
        mutate(Gender = "All") %>%
        mutate(percentage = count / sum(count) * 100)
    ) %>%
    arrange(Gender, !!sym(variable))
}

# Calculate frequencies and percentages for study
table_descriptive_study <- calculate_frequencies(demographics, "study")

# Calculate frequencies and percentages for workfilter
table_descriptive_workfilter <- calculate_frequencies(demographics, "Workfilter")

# Calculate frequencies and percentages for worktime
table_descriptive_worktime <- calculate_frequencies(demographics, "Worktime")

# Calculate frequencies and percentages for occupation
table_descriptive_occupation <- calculate_frequencies(demographics, "Occupation")

# Calculate frequencies and percentages for education
table_descriptive_education <- calculate_frequencies(demographics, "Education")

# Print the resulting data frames N= 540
print(table_descriptive_study) #136 25,19% yes; 74,81 no
print(table_descriptive_workfilter) #13,52% no; 86,48 yes +33women no 
print(table_descriptive_worktime) #N=467;64,88% VZ, 35,12TZ; of 228 women, 51,75% vollzeit , 48,25% teilzeit ; of men 238, 77,73%VZ, 22,27%tz
print(table_descriptive_occupation)
print(table_descriptive_education)
```

## Heatmap
```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Ensure variables are numeric
demographics <- demographics %>%
  mutate(
    study = ifelse(study == "Y", 1, 0),  # Convert Y/N to 1/0
    Workfilter = ifelse(Workfilter == "Y", 1, 0)  # Convert Y/N to 1/0
    # Add more conversions as needed for other variables
  ) %>%
  mutate(
    Age = as.numeric(Age),
    Gender = as.numeric(Gender),  # Assuming Gender is already binary
    Education = as.numeric(Education),
    Degree = as.numeric(Degree),
    Worktime = as.numeric(Worktime),
    Occupation = as.numeric(Occupation)
  )

# Select variables for correlation analysis
selected_vars <- demographics %>%
  select(Gender, Age, Education, study, Degree, Workfilter, Worktime, Occupation)

# Rename variables for clarity in heatmap
colnames(selected_vars) <- c(
  "Gender", "Age", "Education", "study", "Degree", "Workfilter", "Worktime", "Occupation"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final <- heatmap + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final)
```

```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Assuming your data frame is df_dl
# Ensure all variables are numeric
df_dl <- df_dl %>%
  mutate(across(everything(), as.numeric))

# Select variables for correlation analysis
selected_vars <- df_dl %>%
  select(everything())

colnames(selected_vars) <- c(
  "F1F1",  "F1F2",  "F1F3",  "F1F4",  "F1F5",  "F1F6",  "F1F7",  "F1F8",  "F1F9",  "F1F10",
  "F1F11", "F1F12", "F1F13", "F1F14", "F2F1",  "F2F2",  "F2F3",  "F2F4",  "F2F5",  "F2F6",
  "F2F7",  "F2F8",  "F2F9",  "F2F10", "F2F11", "F2F12", "F2F13", "F2F14", "F2F15", "F2F16",
  "F2F17", "F2F18", "F2F19", "F2F20", "F3F1",  "F3F2",  "F3F3",  "F3F4",  "F3F5",  "F3F6",
  "F3F7",  "F3F8",  "F3F9",  "F4F1",  "F4F2",  "F4F3",  "F4F4",  "F4F5",  "F4F6",  "F4F7",
  "F4F8",  "F5F1",  "F5F2",  "F5F3",  "F5F4",  "F5F5",  "F5F6",  "F5F7",  "F5F8",  "F5F9",
  "F5F10", "F5F11", "F5F12", "F5F13", "F5F14", "F5F15", "F5F16", "F5F17", "F5F18"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap2 <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final2 <- heatmap2 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final2)
print(heatmap2)
```

```{r}
# Load required libraries
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Ensure all variables are numeric
df_dl <- df_dl %>%
  mutate(across(everything(), as.numeric))

# Select the specified variables for correlation analysis
selected_vars2 <- df_dl %>%
  select(F1F6, F1F8, F1F11, F1F14,
         F2F2, F2F6, F2F15, F2F20,
         F3F2, F3F4, F3F6, F3F9,
         F4F1, F4F3, F4F6, F4F8,
         F5F3, F5F4, F5F8, F5F18)

# Calculate correlations
correlation_matrix2 <- round(cor(selected_vars2, use = "pairwise.complete.obs"), 2)

# Function to keep only the upper triangle of the matrix
upper_triangle2 <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

# Keep only the upper triangle
correlation_upper2 <- upper_triangle2(correlation_matrix2)

# Reshape the data for heatmap plotting
melted_data2 <- melt(correlation_upper2, na.rm = TRUE)

# Plot the heatmap
heatmap3 <- ggplot(data = melted_data2, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final3 <- heatmap3 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final3)
print(heatmap3)

```
```{r}
library(apaTables)
correlation_matrix2 <- round(cor(selected_vars2, use = "pairwise.complete.obs"), 2)
apa.cor.table(correlation_matrix2,filename = "itemscortable.rtf")

```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(scales) # For pretty_breaks

# Adjust the ggplot call to exclude NAs only from the figure
ggplot(describe_occupation %>% filter(!is.na(Occupation)), aes(x = factor(Occupation))) +
  geom_bar(fill = "blue", color = "black", alpha = 0.7) +
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5) + # Add count labels
  labs(title = "Histogram of Occupations",
       x = "Occupation",
       y = "Frequency") +
  scale_x_discrete(labels = function(x) {
    labels <- c(
      "1" = "Land-, Forst- und Tierwirtschaft",
      "2" = "Rohstoffgewinnung, Produktion und Fertigung",
      "3" = "Bau, Architektur, Gebäudetechnik",
      "4" = "Naturwissenschaft, Geografie und Informatik",
      "5" = "Verkehr, Logistik, Schutz und Sicherheit",
      "6" = "Kaufmännische Dienstleistungen, Vertrieb, Tourismus",
      "7" = "Buchhaltung, Recht und Verwaltung",
      "8" = "Gesundheit, Soziales, Lehre und Erziehung",
      "9" = "Geistes-, Gesellschafts- und Wirtschaftswissenschaften, Medien, Kunst, Kultur und Gestaltung",
      "10" = "Militär"
    )
    wrapped_labels <- lapply(labels[x], function(label) paste(strwrap(label, width = 20), collapse = "\n"))
    unlist(wrapped_labels) # Ensure the output is a vector, not a list
  }) +
  scale_y_continuous(breaks = pretty_breaks(n = 10)) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 9, angle = 55, hjust = 1, vjust = 1),
    axis.text.y = element_text(size = 9),
    plot.title = element_text(size = 10),
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)
  )
```

```{r}
library(dplyr)
library(tibble) # For creating a tibble if needed

# First, define the labels as a named vector
occupation_labels <- c(
  "1" = "Land-, Forst- und Tierwirtschaft",
  "2" = "Rohstoffgewinnung, Produktion und Fertigung",
  "3" = "Bau, Architektur, Gebäudetechnik",
  "4" = "Naturwissenschaft, Geografie und Informatik",
  "5" = "Verkehr, Logistik, Schutz und Sicherheit",
  "6" = "Kaufmännische Dienstleistungen, Vertrieb, Tourismus",
  "7" = "Buchhaltung, Recht und Verwaltung",
  "8" = "Gesundheit, Soziales, Lehre und Erziehung",
  "9" = "Geistes-, Gesellschafts- und Wirtschaftswissenschaften, Medien, Kunst, Kultur und Gestaltung",
  "10" = "Militär"
)

# Convert Occupation to factor with labels and count
frequency_table_occ <- describe_occupation %>%
  filter(!is.na(Occupation)) %>%
  mutate(Occupation = factor(Occupation, labels = occupation_labels[names(occupation_labels)])) %>%
  count(Occupation) %>%
  arrange(desc(n)) # Arrange by frequency, can be removed if not needed

# Print the frequency table
print(frequency_table_occ)
```
```{r}
library(dplyr)
library(tibble) # For creating a tibble if needed

# First, define the labels as a named vector
#"1" = "Kein Abschluss",
education_labels <- c(
  "2" = "Hauptschulabschluss",
  "3" = "Mittlere Reife",
  "4" = "Fachschulabschluss",
  "5" = "Allg. Hochschulreife",
  "6" = "Abgeschlossene Ausbildung",
  "7" = "Hochschulabschluss (FH)",
  "8" = "Hochschulabschluss (Uni)",
  "9" = "Promotion",
  "10" = "Anderer Abschluss"
)

# Convert Education to factor with labels and count
frequency_table_edu <- describe_education %>%
  filter(!is.na(Education)) %>%
  mutate(Education = factor(Education, labels = education_labels[names(education_labels)])) %>%
  count(Education) %>%
  arrange(desc(n)) 

# Print the frequency table
print(frequency_table_edu)
```
## Filter for the partial completes
```{r}
# Splitting df3 into two datasets based on the "lastpage" variable
df4 <- subset(df3, lastpage == 13)
df_partial <- subset(df3, lastpage != 13)

```

# Stuart - Item selection
## Sample split
```{r}
library(stuart)
#, 0.4525
split <- holdout(df4, prop = 0.5475, grouping = NULL, seed = 2024, determined = NULL)
lapply(split, nrow) #check size of sample

# Accessing the split datasets
df_train <- split$calibrate
df_test <- split$validate

# Remove the 'determined' column by setting it to NULL
df_train$determined <- NULL
df_test$determined <- NULL

# adding the partially completed ones to the df train
df_train <- rbind(df_train, df_partial)

# exclude everything exept the dl items
df_train <- df_train[,3:71]

df_test_dl <- df_test[,3:71]

```


```{r}
#F2F17, F2F4, F3F1  deleted
fs<-list(Comprehension=c("F1F1",  "F1F2",  "F1F3",  "F1F4",  "F1F5",  "F1F6",  "F1F7",  "F1F8",  "F1F9",  "F1F10", "F1F11",                              "F1F12", "F1F13", "F1F14"),
           Evaluation=c('F2F1',"F2F2",  "F2F3",  "F2F5",  "F2F6",  "F2F7",  "F2F8",  "F2F9",  "F2F10", "F2F11", "F2F12",                                "F2F13", "F2F14", "F2F15", "F2F16", "F2F18",'F2F19','F2F20'),
           Integration=c("F3F2",'F3F3' ,"F3F4",  "F3F5",  "F3F6",  "F3F7", "F3F8","F3F9"),
           Communication=c('F4F1','F4F2','F4F3','F4F4','F4F5','F4F6','F4F7','F4F8'),
           Statistics=c("F5F1",  "F5F2",  "F5F3",  "F5F4",  "F5F5",  "F5F6",  "F5F7", "F5F8",  "F5F9",  "F5F10",                                       "F5F11","F5F12", "F5F13", "F5F14", "F5F15", "F5F16", "F5F17", "F5F18")
                           )
```

## Preparing a metric data frame for the MG-CFA
```{r}
# Add the 'split' column to each data frame
df_train$split <- 1
df_test_dl$split <- 2

# Step 5: Combine the data frames
df_mi <- bind_rows(df_train, df_test_dl)
```
#####################
## Objective-Function
```{r}
library(stuart)
library(lavaan)

#stuart:::objective.preset
obj_default <- function (chisq, df, pvalue, rmsea, srmr, cfi) 
{
    1/(1 + exp(-10 * (cfi - 0.6))) + 0.5 * (1 - (1/(1 + exp(-100 * 
        (rmsea - 0.05))))) + 0.5 * (1 - (1/(1 + exp(-100 * (srmr - 
        0.06)))))
}


obj_default3 <- function(chisq, df, pvalue, rmsea.robust, srmr, cfi.robust, crel, nu)
                        {(1-rmsea.robust)+(1-srmr)+(1+cfi.robust)} # best jet


```

## Estimating the total number of item-combinations
```{r}
combinations(imputed_training_data_c, fs,4)
```
## Genetic Algorithm

```{r}
fit_single <- gene(
  data = df_dl,
  factor.structure = fs,  
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2028,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default3,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE
)
```

```{r}
summary(fit_single)

inspect(fit_single$final, 'est')
inspect(fit_single$final, 'fit')

lavaan::summary(fit_single$final, standardized = TRUE) 
lavaan::inspect(fit_single$final, 'est')$A$nu
lavaan::inspect(fit_single$final, 'est')$B$nu
```

### Kfold-Crossvalidation
```{r}
############ k-Folding #########################
set.seed(2028)

kfold_sel <- kfold(
  'gene', 
  k = 3, 
  data = df_train,
  factor.structure = fs,  
  max.invariance = "strict", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2028,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default3,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
#########################################################
```

### Summary Results No.1
```{r}
summary(kfold_sel)

inspect(kfold_sel$final, 'est')
inspect(kfold_sel$final, 'fit')

lavaan::summary(kfold_sel$final, standardized = TRUE) 
lavaan::inspect(kfold_sel$final, 'est')$A$nu
lavaan::inspect(kfold_sel$final, 'est')$B$nu
```
SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 61506 
Time Required: 32662.03 seconds

Crossvalidation Results with STRICT Invariance:

Average Jaccard Similarity: Comprehension: 0.206, Evaluation: 0.095, Integration: 0.270, Communication: 0.422, Statistics: 0.095

Constructed Subtests: (k = 1)
Comprehension: F1F4 F1F6 F1F8 F1F10
Evaluation: F2F9 F2F12 F2F14 F2F16
Integration: F3F3 F3F4 F3F6 F3F9
Communication: F4F3 F4F4 F4F7 F4F8
Statistics: F5F3 F5F6 F5F8 F5F18

### Kfold-Crossvalidation with multiple iterations
```{r}
set.seed(2028)
# Load the necessary libraries
library(stuart)

# Number of iterations for averaging
num_iterations <- 3

# Initialize a list to store the results of each iteration
results_list <- vector("list", num_iterations)

# Loop over the number of iterations
for (i in 1:num_iterations) {
  # Perform k-fold cross-validation and store the result
  results_list[[i]] <- kfold(
    'gene', 
    k = 3, 
    data = df_train,
    factor.structure = fs,
    max.invariance = "strict", 
    capacity = list(4, 4, 4, 4, 4), 
    seed = 2028 + i, # Change seed each time for variability
    seeded.search = TRUE,
    auxiliary = NULL,
    software = "lavaan",
    cores = 5,
    objective = obj_default3,
    ignore.errors = TRUE,
    burnin = 5,
    generations = 500,
    individuals = 34,
    selection = "tournament",
    selection.pressure = NULL,
    elitism = NULL,
    reproduction = 0.5,
    mutation = 0.05,
    mating.index = 0,
    mating.size = 0.25,
    mating.criterion = "similarity",
    immigration = 0,
    convergence.criterion = "geno.between",
    tolerance = NULL,
    reinit.n = 1,
    reinit.criterion = "geno.between",
    reinit.tolerance = NULL,
    reinit.prop = 0.75,
    schedule = "run",
    analysis.options = NULL,
    suppress.model = FALSE,
    remove.details = TRUE
  )
}

# Initialize a list to store averaged fit indices
fit_indices <- list(
  SRMR = numeric(num_iterations),
  RMSEA = numeric(num_iterations),
  CFI = numeric(num_iterations)
)

# Extract fit indices from each result
for (i in 1:num_iterations) {
  # Ensure the results are not NULL
  if (!is.null(results_list[[i]])) {
    # Use the inspect function to get the fit measures
    fit_measures <- lavaan::inspect(results_list[[i]]$final, "fit")
    
    # Store the fit indices if they exist
    if (!is.null(fit_measures)) {
      fit_indices$SRMR[i] <- fit_measures["srmr"]
      fit_indices$RMSEA[i] <- fit_measures["rmsea"]
      fit_indices$CFI[i] <- fit_measures["cfi"]
    }
  }
}

# Compute the average of each fit index, ensuring NA values are handled
average_fit_indices <- sapply(fit_indices, function(x) mean(x, na.rm = TRUE))

# Print the average fit indices
print("Average Fit Indices from Results:")
print(average_fit_indices)

# Inspect the complete output for available information
inspect(results_list[[1]]$final, 'fit')
inspect(results_list[[1]]$final, 'est')

lavaan::standardizedSolution(results_list[[i]]$final)

```
Comprehension: F1F6 F1F8 F1F11 F1F14
Evaluation: F2F2 F2F6 F2F15 F2F20
Integration: F3F2 F3F4 F3F6 F3F9
Communication: F4F1 F4F3 F4F6 F4F8
Statistics: F5F3 F5F4 F5F8 F5F18


### Validation via MG-CFA for MI
```{r}
set.seed(2028)
library(lavaan)

cfa_model_m <- '
# Measurement model
Comprehension =~ F1F6 + F1F8 + F1F11 + F1F14
Evaluation =~ F2F2 + F2F6 + F2F15 + F2F20
Integration =~ F3F2 + F3F4 + F3F6 + F3F9
Communication =~ F4F1 + F4F3 + F4F6 + F4F8
Statistics =~ F5F3 + F5F4 + F5F8 + F5F18

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'

fit_cfa_model_m <- cfa(cfa_model_m, df_mi, 
                       missing = "fiml", 
                       estimator = "MLR", 
                       group = "split", 
                       em.h1.iter.max = 1000)

summary(fit_cfa_model_m, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_m, 'fit')
```

## Multicollinearity
```{r}
cov_matrix <- lavInspect(fit_cfa_model_m, "cov.lv")
print(cov_matrix)

# Convert covariance matrices to correlation matrices
corr_matrix_group1 <- cov2cor(cov_matrix[[1]])
corr_matrix_group2 <- cov2cor(cov_matrix[[2]])

# Print correlation matrices
print(corr_matrix_group1)
print(corr_matrix_group2)

```

#### configural to metric
```{r}
set.seed(2028)
fit_cfa_model_metric <- cfa(cfa_model_m, df_mi, missing = "fiml" ,estimator = "MLR",em.h1.iter.max = 1000, group = "split", group.equal= "loadings")

comp_fit_metric <- compareFit(fit_cfa_model_m, fit_cfa_model_metric)
summary(comp_fit_metric)

inspect(fit_cfa_model_metric, 'fit')

```
#### configural to metric
Cfi delta: .002        #check
rmsea delta: .002 #check
srmr delta: .001 #check

#### metric to scalar
```{r}
set.seed(2028)
fit_cfa_model_scalar <- cfa(cfa_model_m, df_mi,missing = "fiml" , estimator = "MLR",em.h1.iter.max = 1000, group = "split",
                            group.equal = c('loadings','intercepts'))

comp_fit_scalar <- compareFit(fit_cfa_model_metric,fit_cfa_model_scalar)
summary(comp_fit_scalar)

inspect(fit_cfa_model_scalar, 'fit')
```
#### metric to scalar
Cfi delta: .001 #check
rmsea delta: .001 #check
srmr delta: .000 #check

#### scalar to strict
```{r}
set.seed(2028)
fit_cfa_model_strict <- cfa(cfa_model_m, df_mi,missing = "fiml" ,em.h1.iter.max = 1000, estimator = "MLR", group = "split",
                            group.equal = c('loadings','intercepts','residuals'))

comp_fit_strict <- compareFit(fit_cfa_model_scalar, fit_cfa_model_strict)
summary(comp_fit_strict)
inspect(fit_cfa_model_strict, 'fit')

```
#### scalar to strict
Cfi delta: .001 #fail
rmsea delta: .001 #check
srmr delta: .005 #check

#####################

## ezfit
```{r}
set.seed(2029)
library(ezCutoffs)
  
## function call
out <- ezCutoffs(model = cfa_model_m, n_obs = 1000, n_rep = 10, n_cores = 1)

out <- ezCutoffs(model = cfa_model_m, n_obs = c(300, 400), n_rep = 50, fit_indices = c("cfi.scaled","rmsea.scaled","srmr"), alpha_level = 0.05, estimator = "MLR", group = "group", group.equal = c("loadings", "intercepts"), n_cores = 1 )

## retrieve output
summary(out)
plot(out)

```

## Check correlated residuals
```{r}
# Check modification indices
modificationIndices(fit_cfa_model_m, sort = TRUE)

```

## Haywood cases
```{reproducibleR}
# Get predicted values
predicted_values <- predict(fit_cfa_model_m)

# Check for extreme values (Haywood cases)
# Flatten and combine the matrices into a single numeric vector
predicted_values_combined <- unlist(lapply(predicted_values, as.numeric))

# Check for extreme values
extreme_values <- predicted_values_combined[which(
  predicted_values_combined > quantile(predicted_values_combined, 0.95) | 
  predicted_values_combined < quantile(predicted_values_combined, 0.05)
)]

print(extreme_values)

```

```{reproducibleR}
# Get standardized residuals
residuals_standardized <- residuals(fit_cfa_model_m, type = "standardized")

# Extract covariance residuals and flatten them into a vector
residuals_matrix <- residuals_standardized$cov
residuals_vector <- as.numeric(residuals_matrix)

# Remove NA values
residuals_vector <- na.omit(residuals_vector)

# Calculate absolute values and quantiles
absolute_residuals <- abs(residuals_vector)
quantile_95 <- quantile(absolute_residuals, 0.95)

# Find extreme residuals
extreme_residuals <- residuals_vector[absolute_residuals > quantile_95]
print(extreme_residuals)

# Optional: Lower the threshold or examine additional quantiles
quantile_90 <- quantile(absolute_residuals, 0.90)
extreme_residuals_90 <- residuals_vector[absolute_residuals > quantile_90]
print(extreme_residuals_90)


```

## Factor Scores / Bartlett Scores
```{reproducibleR}
set.seed(2028)

# Calculate factor scores using the fitted model
factor_scores <- lavPredict(fit_cfa_model_m, method = "Bartlett")

# Add the 'split' column to the factor scores
factor_scores_combined <- data.frame(factor_scores)
factor_scores_combined$split <- df_mi$split

# View the factor scores
print(factor_scores_combined)

factor_scores_split2 <- subset(factor_scores_combined, split == 2)
# Remove the 'split' column
factor_scores_split2 <- factor_scores_split2[, !names(factor_scores_split2) %in% "split"]


# Calculate the mean (general) factor scores for each factor
general_factor_scores <- colMeans(factor_scores, na.rm = TRUE)

# Print the general factor scores (aggregated across individuals)
print("General Factor Scores (Mean Across Individuals):")
print(general_factor_scores)

```

### Perform t-tests to compare factor scores between groups
```{reproducibleR}
# Compute means of factor scores by group
mean_factor_scores <- aggregate(. ~ split, data = factor_scores_combined, FUN = mean)

# View the means
print(mean_factor_scores)

# Exclude the 'split' column from the comparison
factor_names <- names(factor_scores_combined)[names(factor_scores_combined) != "split"]

# Compute t-tests for each factor
t_test_results <- lapply(factor_names, function(factor) {
  t.test(factor_scores_combined[factor_scores_combined$split == 1, factor],
         factor_scores_combined[factor_scores_combined$split == 2, factor])
})

# Print the results of the t-tests
print(t_test_results)
```


```{reproducibleR}

df_result<- data.frame(
  F1F6 = df_test$F1F6,
  F1F8 = df_test$F1F8,
  F1F11 = df_test$F1F11,
  F1F14 = df_test$F1F14,
  F2F2 = df_test$F2F2,
  F2F6 = df_test$F2F6,
  F2F15 = df_test$F2F15,
  F2F20 = df_test$F2F20,
  F3F2 = df_test$F3F2,
  F3F4 = df_test$F3F4,
  F3F6 = df_test$F3F6,
  F3F9 = df_test$F3F9,
  F4F1 = df_test$F4F1,
  F4F3 = df_test$F4F3,
  F4F6 = df_test$F4F6,
  F4F8 = df_test$F4F8,
  F5F3 = df_test$F5F3,
  F5F4 = df_test$F5F4,
  F5F8 = df_test$F5F8,
  F5F18 = df_test$F5F18
)
```
## Convergent Measures
```{reproducibleR}
SWE <- df_test %>%
  dplyr::select(starts_with("SWE")) %>%
  mutate_all(as.numeric)

NFC <- df_test %>%
  select(starts_with("NFC")) %>%
  mutate_all(as.numeric)

ICT <- df_test %>%
  select(starts_with("ICT")) %>%
  mutate_all(as.numeric)

B5_openness <- df_test %>%
  select(starts_with("B5.o")) %>%
  mutate_all(as.numeric)

B5_con <- df_test %>%
  select(starts_with("B5.G")) %>%
  mutate_all(as.numeric)
```

## Inverting negatively keyed items
```{reproducibleR}
# Recoding for B5_con and B5_openness
B5_con$B5.G1. <- 6 - B5_con$B5.G1.
B5_openness$B5.O2. <- 6 - B5_openness$B5.O2.

# Recoding for NFC
NFC$NFC.NFC1. <- 8 - NFC$NFC.NFC1.
NFC$NFC.NFC4. <- 8 - NFC$NFC.NFC4.

# Recoding for SWE
SWE$SWE16.SWE4. <- 6 - SWE$SWE16.SWE4.
SWE$SWE16s2.SWE14. <- 6 - SWE$SWE16s2.SWE14.
```

## Calculating meanscores
```{reproducibleR}
# Calculate row means for each row and add as a new column
B5_con <- B5_con %>%
  rowwise() %>%
  mutate(B5con_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
B5_openness <- B5_openness %>%
  rowwise() %>%
  mutate(B5o_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
NFC <- NFC %>%
  rowwise() %>%
  mutate(Nfc_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
ICT <- ICT %>%
  rowwise() %>%
  mutate(Ict_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
SWE <- SWE %>%
  rowwise() %>%
  mutate(Swe_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

```

```{reproducibleR}
# Combine the row means into a single data frame
all_means <- data.frame(
  B5con_Row_Means = B5_con$B5con_Row_Means,
  B5o_Row_Means = B5_openness$B5o_Row_Means,
  Nfc_Row_Means = NFC$Nfc_Row_Means,
  Ict_Row_Means = ICT$Ict_Row_Means,
  Swe_Row_Means = SWE$Swe_Row_Means
)

# Combine the data frames
all_cor <- bind_cols(all_means_reset, factor_scores_split2)
```

## Normality test - B5 Conscientiousness
```{reproducibleR}
# Exclude the third column (Mean) for normality checks
raw_data <- B5_con[, -3]  # This excludes the third column

# Function to check normality for each column in the data frame
check_normality <- function(data) {
  for (col_name in colnames(data)) {
    cat("Checking normality for", col_name, "\n")
    
    variable <- data[[col_name]]
    
    # Visual checks
    hist(variable, main=paste("Histogram of", col_name), xlab=col_name, ylab="Frequency")
    qqnorm(variable)
    qqline(variable, col = "red")
    
    # Statistical tests
    shapiro_test <- shapiro.test(variable)
    cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "\n")
    
    # Skewness and Kurtosis
    skew <- skewness(variable)
    kurt <- kurtosis(variable)
    cat("Skewness:", skew, "\n")
    cat("Kurtosis:", kurt, "\n\n")
  }
}

# Check normality for the relevant columns
check_normality(raw_data) #nonnormal
```

## Normality openness
```{reproducibleR}
library(e1071)
# Exclude the third column (Mean) for normality checks
raw_data <- B5_openness[, -3]  # This excludes the third column

# Function to check normality for each column in the data frame
check_normality <- function(data) {
  for (col_name in colnames(data)) {
    cat("Checking normality for", col_name, "\n")
    
    variable <- data[[col_name]]
    
    # Visual checks
    hist(variable, main=paste("Histogram of", col_name), xlab=col_name, ylab="Frequency")
    qqnorm(variable)
    qqline(variable, col = "red")
    
    # Statistical tests
    shapiro_test <- shapiro.test(variable)
    cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "\n")
    
    # Skewness and Kurtosis
    skew <- skewness(variable)
    kurt <- kurtosis(variable)
    cat("Skewness:", skew, "\n")
    cat("Kurtosis:", kurt, "\n\n")
  }
}

# Check normality for the relevant columns
check_normality(raw_data) # nonnormal
```

```{reproducibleR}
# Exclude the third column (Mean) for normality checks
raw_data <- SWE[, -17]  # This excludes the third column

# Function to check normality for each column in the data frame
check_normality <- function(data) {
  for (col_name in colnames(data)) {
    cat("Checking normality for", col_name, "\n")
    
    variable <- data[[col_name]]
    
    # Visual checks
    hist(variable, main=paste("Histogram of", col_name), xlab=col_name, ylab="Frequency")
    qqnorm(variable)
    qqline(variable, col = "red")
    
    # Statistical tests
    shapiro_test <- shapiro.test(variable)
    cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "\n")
    
    # Skewness and Kurtosis
    skew <- skewness(variable)
    kurt <- kurtosis(variable)
    cat("Skewness:", skew, "\n")
    cat("Kurtosis:", kurt, "\n\n")
  }
}

# Check normality for the relevant columns
check_normality(raw_data) #nonnormal
```

```{reproducibleR}
# Exclude the third column (Mean) for normality checks
raw_data <- ICT[, -6]  # This excludes the third column

# Function to check normality for each column in the data frame
check_normality <- function(data) {
  for (col_name in colnames(data)) {
    cat("Checking normality for", col_name, "\n")
    
    variable <- data[[col_name]]
    
    # Visual checks
    hist(variable, main=paste("Histogram of", col_name), xlab=col_name, ylab="Frequency")
    qqnorm(variable)
    qqline(variable, col = "red")
    
    # Statistical tests
    shapiro_test <- shapiro.test(variable)
    cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "\n")
    
    # Skewness and Kurtosis
    skew <- skewness(variable)
    kurt <- kurtosis(variable)
    cat("Skewness:", skew, "\n")
    cat("Kurtosis:", kurt, "\n\n")
  }
}

# Check normality for the relevant columns
check_normality(raw_data) #nonnormal
```

```{reproducibleR}
# Exclude the third column (Mean) for normality checks
raw_data <- NFC[, -5]  # This excludes the third column

# Function to check normality for each column in the data frame
check_normality <- function(data) {
  for (col_name in colnames(data)) {
    cat("Checking normality for", col_name, "\n")
    
    variable <- data[[col_name]]
    
    # Visual checks
    hist(variable, main=paste("Histogram of", col_name), xlab=col_name, ylab="Frequency")
    qqnorm(variable)
    qqline(variable, col = "red")
    
    # Statistical tests
    shapiro_test <- shapiro.test(variable)
    cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "\n")
    
    # Skewness and Kurtosis
    skew <- skewness(variable)
    kurt <- kurtosis(variable)
    cat("Skewness:", skew, "\n")
    cat("Kurtosis:", kurt, "\n\n")
  }
}

# Check normality for the relevant columns
check_normality(raw_data)
```

```{reproducibleR}
# Exclude the third column (Mean) for normality checks
raw_data <- df_result  # This excludes the third column

# Function to check normality for each column in the data frame
check_normality <- function(data) {
  for (col_name in colnames(data)) {
    cat("Checking normality for", col_name, "\n")
    
    variable <- data[[col_name]]
    
    # Visual checks
    hist(variable, main=paste("Histogram of", col_name), xlab=col_name, ylab="Frequency")
    qqnorm(variable)
    qqline(variable, col = "red")
    
    # Statistical tests
    shapiro_test <- shapiro.test(variable)
    cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "\n")
    
    # Skewness and Kurtosis
    skew <- skewness(variable)
    kurt <- kurtosis(variable)
    cat("Skewness:", skew, "\n")
    cat("Kurtosis:", kurt, "\n\n")
  }
}

# Check normality for the relevant columns
check_normality(raw_data)
```

## Correlations - nonnormal
```{reproducibleR}
library(apaTables)
cortable<-cor(all_cor, use = "pairwise" ,method = "kendall")

apa.cor.table(all_cor, filename = "cortable.rtf")
```

```{reproducibleR}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Select newly created row means for correlation analysis
selected_vars <- all_cor %>%
  select(B5con_Row_Means, B5o_Row_Means, Nfc_Row_Means, Ict_Row_Means, Swe_Row_Means, Comprehension, Evaluation, Integration, Communication, Statistics)

# Rename variables for clarity in heatmap
colnames(selected_vars) <- c(
  "B5con", "B5o", "Nfc", "Ict", "Swe", "Comprehension", "Evaluation", "Integration", "Communication", "Statistics"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise",method = "kendall"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap3 <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Row Means of Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final3 <- heatmap3 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final3)

```

## Reliability
```{reproducibleR}
omega(B5_con[1:2],1)
omega(B5_openness[1:2],1)
omega(NFC[1:4],1)
omega(ICT[1:5],1)
omega(SWE[1:16],1)

omega(df_result,5)
```


## Item Difficulty
```{reproducibleR}
library(car)

# Set up graphics to have 1 row and 4 columns for histograms
par(mfrow=c(1,4))
# Plot histograms for each item
#Comprehension
hist(df_mi$F1F6, ylab='Häufigkeit', xlab='Item 1', main='Histogramm')
hist(df_mi$F1F8, ylab='Häufigkeit', xlab='Item 2', main='Histogramm')
hist(df_mi$F1F11, ylab='Häufigkeit', xlab='Item 3', main='Histogramm')
hist(df_mi$F1F14, ylab='Häufigkeit', xlab='Item 4', main='Histogramm')
```


```{reproducibleR}
par(mfrow=c(1,4))
  
#Evaluation
hist(df_mi$F2F2, ylab='Häufigkeit', xlab='Item 5', main='Histogramm')
hist(df_mi$F2F6, ylab='Häufigkeit', xlab='Item 6', main='Histogramm')
hist(df_mi$F2F15, ylab='Häufigkeit', xlab='Item 7', main='Histogramm')
hist(df_mi$F2F20, ylab='Häufigkeit', xlab='Item 8', main='Histogramm')
```

```{reproducibleR}
par(mfrow=c(1,4))
#Integration 
hist(df_mi$F3F2, ylab='Häufigkeit', xlab='Item 9', main='Histogramm')
hist(df_mi$F3F4, ylab='Häufigkeit', xlab='Item 10', main='Histogramm')
hist(df_mi$F3F6, ylab='Häufigkeit', xlab='Item 11', main='Histogramm')
hist(df_mi$F3F9, ylab='Häufigkeit', xlab='Item 12', main='Histogramm')
```

```{reproducibleR}
par(mfrow=c(1,4))
#Communication
  
hist(df_mi$F4F1, ylab='Häufigkeit', xlab='Item 13', main='Histogramm')
hist(df_mi$F4F3, ylab='Häufigkeit', xlab='Item 14', main='Histogramm')
hist(df_mi$F4F6, ylab='Häufigkeit', xlab='Item 15', main='Histogramm')
hist(df_mi$F4F8, ylab='Häufigkeit', xlab='Item 16', main='Histogramm')
```

```{reproducibleR}
par(mfrow=c(1,4))
#Statistics
hist(df_mi$F5F3, ylab='Häufigkeit', xlab='Item 17', main='Histogramm')
hist(df_mi$F5F4, ylab='Häufigkeit', xlab='Item 18', main='Histogramm')
hist(df_mi$F5F8, ylab='Häufigkeit', xlab='Item 19', main='Histogramm')
hist(df_mi$F5F18, ylab='Häufigkeit', xlab='Item 20', main='Histogramm')
```

```{reproducibleR}
library(car)

# Create a boxplot for the columns in df_mi
car::Boxplot(df_mi, ylab='Frequency', xlab='Items', main='Boxplot', col='darkgreen')

```

```{reproducibleR}
# Means
mean_F1F6 <- mean(df_mi$F1F6, na.rm = TRUE)
mean_F1F8 <- mean(df_mi$F1F8, na.rm = TRUE)
mean_F1F11 <- mean(df_mi$F1F11, na.rm = TRUE)
mean_F1F14 <- mean(df_mi$F1F14, na.rm = TRUE)
mean_F2F2 <- mean(df_mi$F2F2, na.rm = TRUE)
mean_F2F6 <- mean(df_mi$F2F6, na.rm = TRUE)
mean_F2F15 <- mean(df_mi$F2F15, na.rm = TRUE)
mean_F2F20 <- mean(df_mi$F2F20, na.rm = TRUE)
mean_F3F2 <- mean(df_mi$F3F2, na.rm = TRUE)
mean_F3F4 <- mean(df_mi$F3F4, na.rm = TRUE)
mean_F3F6 <- mean(df_mi$F3F6, na.rm = TRUE)
mean_F3F9 <- mean(df_mi$F3F9, na.rm = TRUE)
mean_F4F1 <- mean(df_mi$F4F1, na.rm = TRUE)
mean_F4F3 <- mean(df_mi$F4F3, na.rm = TRUE)
mean_F4F6 <- mean(df_mi$F4F6, na.rm = TRUE)
mean_F4F8 <- mean(df_mi$F4F8, na.rm = TRUE)
mean_F5F3 <- mean(df_mi$F5F3, na.rm = TRUE)
mean_F5F4 <- mean(df_mi$F5F4, na.rm = TRUE)
mean_F5F8 <- mean(df_mi$F5F8, na.rm = TRUE)
mean_F5F18 <- mean(df_mi$F5F18, na.rm = TRUE)

# Standard Deviations
sd_F1F6 <- sd(df_mi$F1F6, na.rm = TRUE)
sd_F1F8 <- sd(df_mi$F1F8, na.rm = TRUE)
sd_F1F11 <- sd(df_mi$F1F11, na.rm = TRUE)
sd_F1F14 <- sd(df_mi$F1F14, na.rm = TRUE)
sd_F2F2 <- sd(df_mi$F2F2, na.rm = TRUE)
sd_F2F6 <- sd(df_mi$F2F6, na.rm = TRUE)
sd_F2F15 <- sd(df_mi$F2F15, na.rm = TRUE)
sd_F2F20 <- sd(df_mi$F2F20, na.rm = TRUE)
sd_F3F2 <- sd(df_mi$F3F2, na.rm = TRUE)
sd_F3F4 <- sd(df_mi$F3F4, na.rm = TRUE)
sd_F3F6 <- sd(df_mi$F3F6, na.rm = TRUE)
sd_F3F9 <- sd(df_mi$F3F9, na.rm = TRUE)
sd_F4F1 <- sd(df_mi$F4F1, na.rm = TRUE)
sd_F4F3 <- sd(df_mi$F4F3, na.rm = TRUE)
sd_F4F6 <- sd(df_mi$F4F6, na.rm = TRUE)
sd_F4F8 <- sd(df_mi$F4F8, na.rm = TRUE)
sd_F5F3 <- sd(df_mi$F5F3, na.rm = TRUE)
sd_F5F4 <- sd(df_mi$F5F4, na.rm = TRUE)
sd_F5F8 <- sd(df_mi$F5F8, na.rm = TRUE)
sd_F5F18 <- sd(df_mi$F5F18, na.rm = TRUE)

```

```{reproducibleR}
# Print means
cat("Means of items:\n")
cat("Mean of F1F6: ", mean_F1F6, "\n")
cat("Mean of F1F8: ", mean_F1F8, "\n")
cat("Mean of F1F11: ", mean_F1F11, "\n")
cat("Mean of F1F14: ", mean_F1F14, "\n")
cat("Mean of F2F2: ", mean_F2F2, "\n")
cat("Mean of F2F6: ", mean_F2F6, "\n")
cat("Mean of F2F15: ", mean_F2F15, "\n")
cat("Mean of F2F20: ", mean_F2F20, "\n")
cat("Mean of F3F2: ", mean_F3F2, "\n")
cat("Mean of F3F4: ", mean_F3F4, "\n")
cat("Mean of F3F6: ", mean_F3F6, "\n")
cat("Mean of F3F9: ", mean_F3F9, "\n")
cat("Mean of F4F1: ", mean_F4F1, "\n")
cat("Mean of F4F3: ", mean_F4F3, "\n")
cat("Mean of F4F6: ", mean_F4F6, "\n")
cat("Mean of F4F8: ", mean_F4F8, "\n")
cat("Mean of F5F3: ", mean_F5F3, "\n")
cat("Mean of F5F4: ", mean_F5F4, "\n")
cat("Mean of F5F8: ", mean_F5F8, "\n")
cat("Mean of F5F18: ", mean_F5F18, "\n")

# Print standard deviations
cat("\nStandard Deviations of items:\n")
cat("SD of F1F6: ", sd_F1F6, "\n")
cat("SD of F1F8: ", sd_F1F8, "\n")
cat("SD of F1F11: ", sd_F1F11, "\n")
cat("SD of F1F14: ", sd_F1F14, "\n")
cat("SD of F2F2: ", sd_F2F2, "\n")
cat("SD of F2F6: ", sd_F2F6, "\n")
cat("SD of F2F15: ", sd_F2F15, "\n")
cat("SD of F2F20: ", sd_F2F20, "\n")
cat("SD of F3F2: ", sd_F3F2, "\n")
cat("SD of F3F4: ", sd_F3F4, "\n")
cat("SD of F3F6: ", sd_F3F6, "\n")
cat("SD of F3F9: ", sd_F3F9, "\n")
cat("SD of F4F1: ", sd_F4F1, "\n")
cat("SD of F4F3: ", sd_F4F3, "\n")
cat("SD of F4F6: ", sd_F4F6, "\n")
cat("SD of F4F8: ", sd_F4F8, "\n")
cat("SD of F5F3: ", sd_F5F3, "\n")
cat("SD of F5F4: ", sd_F5F4, "\n")
cat("SD of F5F8: ", sd_F5F8, "\n")
cat("SD of F5F18: ", sd_F5F18, "\n")


```

## correlation of residuals
```{reproducibleR}
# Inspect residuals
resid_cor <- residuals(fit_cfa_model_m, type = "cor")  # Correlation residuals
resid_cov <- residuals(fit_cfa_model_m, type = "raw")  # Covariance residuals (raw)
print(resid_cov)#needs to be printed in console
print(resid_cor)#needs to be printed in console

```

## Controlling for Age potential Confounders
```{reproducibleR}
# Define the column names you want to select
selected_columns <- c("id", "F1F6", "F1F8", "F1F11", "F1F14", "F2F2", "F2F6", "F2F15", "F2F20", 
                      "F3F2", "F3F4", "F3F6", "F3F9", "F4F1", "F4F3", "F4F6", "F4F8", 
                      "F5F3", "F5F4", "F5F8", "F5F18", "Gender", "Age", "Education", "study", 
                      "Degree", "Occupation")

# Select the specific columns and columns starting with certain prefixes
control_df <- df_test %>%
  select(all_of(selected_columns), 
         starts_with("SWE"),
         starts_with("NFC"),
         starts_with("ICT"),
         starts_with("B5.o"),
         starts_with("B5.G"))%>%
  mutate_all(as.numeric)

# View the resulting control_df
print(control_df)
 

```

```{reproducibleR}
# Define the columns you want to select
dl_control <- c("id", "F1F6", "F1F8", "F1F11", "F1F14", "F2F2", "F2F6", "F2F15", "F2F20", 
                "F3F2", "F3F4", "F3F6", "F3F9", "F4F1", "F4F3", "F4F6", "F4F8", 
                "F5F3", "F5F4", "F5F8", "F5F18", "Gender", "Age", "Education", "study", 
                "Degree", "Occupation")

# Select the desired columns from df_test and store in dl_control
dl_control_df <- df_test %>%
  select(all_of(dl_control))

# View the resulting dl_control_df
print(dl_control_df)


```

```{reproducibleR}
# List of items
likert_items <- c("F1F6", "F1F8", "F1F11", "F1F14", "F2F2", "F2F6", "F2F15", "F2F20", 
                  "F3F2", "F3F4", "F3F6", "F3F9", "F4F1", "F4F3", "F4F6", "F4F8", 
                  "F5F3", "F5F4", "F5F8", "F5F18")

# Define demographic variables
demographics <- c("Age", "Gender", "Education", "Degree", "Occupation")

# For each Likert item, a regression controlling for demographics
for (item in likert_items) {
  # Dynamically create the formula for the regression model
  formula <- as.formula(paste(item, "~ Age + Gender + Education + Degree + Occupation"))
  
  # Run the linear regression model
  model <- lm(formula, data = dl_control_df)
  
  # Print summary of the model for each Likert item
  print(paste("Results for", item))
  print(summary(model))
}

```

```{r}
library(ggplot2)

ggplot(demographics, aes(x = Education)) +
  geom_bar(aes(y = ..count..), fill = "skyblue", color = "black") +
  geom_text(aes(y = ..count.., label = scales::percent(..count../sum(..count..))),
            stat = "count", vjust = -0.5) +
  labs(title = "Distribution of Education Levels",
       x = "Education Levels",
       y = "Count") +
  theme_minimal()


```

```{r}
# Extract standardized residuals
residuals_standardized <- residuals(fit_cfa_model_m, type = "standardized")

# Extract the first residual covariance matrix (standardized residuals)
residuals_matrix <- residuals_standardized[[1]]$cov

# Convert the residuals matrix to a data frame
residuals_df <- as.data.frame(residuals_matrix)

# Assuming the column names are now correctly aligned
selected_variables <- c("F1F6", "F1F8", "F1F11", "F1F14", "F2F2", "F2F6", "F2F15", "F2F20",
                        "F3F2", "F3F4", "F3F6", "F3F9", "F4F1", "F4F3", "F4F6", "F4F8",
                        "F5F3", "F5F4", "F5F8", "F5F18")

# Check if selected variables match the available names
available_variables <- colnames(residuals_df)

# Filter selected variables based on availability in the residuals matrix
selected_variables <- selected_variables[selected_variables %in% available_variables]

# Subset the residuals for the selected variables
selected_residuals <- residuals_df %>%
  select(all_of(selected_variables))

# Calculate the correlation matrix for the selected variables
residuals_corr_matrix_selected <- cor(selected_residuals, use = "pairwise.complete.obs")
residuals_corr_matrix_df <- as.data.frame(residuals_corr_matrix_selected)
apa.cor.table(residuals_corr_matrix_df, filename="cortableRes1")
apa.cor.table(residuals_matrix, filename="cortableRes1")
# Print the correlation matrix
print(residuals_corr_matrix_selected)

```

```{r}
# Extract standardized residuals
residuals_standardized <- residuals(fit_cfa_model_m, type = "standardized")

# Extract the first residual covariance matrix (standardized residuals)
residuals_matrix <- residuals_standardized[[2]]$cov

# Convert the residuals matrix to a data frame
residuals_df <- as.data.frame(residuals_matrix)

# Assuming the column names are now correctly aligned
selected_variables <- c("F1F6", "F1F8", "F1F11", "F1F14", "F2F2", "F2F6", "F2F15", "F2F20",
                        "F3F2", "F3F4", "F3F6", "F3F9", "F4F1", "F4F3", "F4F6", "F4F8",
                        "F5F3", "F5F4", "F5F8", "F5F18")

# Check if selected variables match the available names
available_variables <- colnames(residuals_df)

# Filter selected variables based on availability in the residuals matrix
selected_variables <- selected_variables[selected_variables %in% available_variables]

# Subset the residuals for the selected variables
selected_residuals <- residuals_df %>%
  select(all_of(selected_variables))

# Calculate the correlation matrix for the selected variables
residuals_corr_matrix_selected <- cor(selected_residuals, use = "pairwise.complete.obs")
apa.cor.table(residuals_corr_matrix_df, filename="cortableRes2")
apa.cor.table(residuals_matrix, filename="cortableRes2")
# Print the correlation matrix
print(residuals_corr_matrix_selected)

```

```{r}


```

####################################################################
STUFF THAT I TRIED THAT DIDNT WORK OR I DIDNT USE IN THE END
####################################################################

## using mice to impute the missing data
```{r, echo=FALSE}
set.seed(2025)
imputed_training_data <- mice(df_train, m=5, method='pmm', maxit=50)
imputed_training_data_c <- complete(imputed_training_data, 1)

imputed_test_data <- mice(df_test_dl, m=5, method='pmm', maxit=50)
imputed_test_data_c <- complete(imputed_test_data, 1)
```

The following merging of answer categries and threshhold seeking is based on a run of the mice imputation process, that wasnt seeded properly, as it is now. So the results will most likely not be reproduced, but different values will occur, resulting in the following code not working precisely. 

### making it seperate files
```{r}
write.csv2(imputed_training_data_c, "imputed_training_data_c.csv")
write.csv2(df_train, "df_train.csv")
```

# Finding Thresholds of Items to merge in order to ose ordered data in the item selection and crossvalidation
```{r}
# Count values for each variable in imputed_training_data_c
value_counts <- imputed_training_data_c %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts)

# Filter for cases with values smaller or equal to 2
filtered_value_counts <- value_counts %>%
  filter(rowSums(. <= 2) > 0)

print(filtered_value_counts)

# Get the list of items to be modified
items_to_modify <- filtered_value_counts$Variable

# Ensure items_to_modify is a character vector
items_to_modify <- as.character(items_to_modify)

# Modify the imputed_training_data_c to merge answers 0 and 1 into 1 for these items
imputed_training_data_merged <- imputed_training_data_c %>%
  mutate(across(all_of(items_to_modify), ~ ifelse(. %in% c(0, 1), 1, .)))

imputed_training_data_merged <- imputed_training_data_merged %>%
  select(-c(F2F17, F2F4))

#print(imputed_training_data_merged)


```

```{r}
value_counts_merged <- imputed_training_data_merged %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


#print(value_counts_merged)
```

```{r}
# Define the selected items
selected_items10 <- c("F1F12", "F2F10", "F3F1", "F3F4", "F4F8", "F5F1", "F5F16", "F5F9", "F5F5", "F5F13", "F4F6")

# Modify the imputed_training_data_c to merge answers 0 and 1 into 1 for these selected items
imputed_training_data_merged1 <- imputed_training_data_merged %>%
  mutate(across(all_of(selected_items10), ~ ifelse(. %in% c(0, 1), 1, .)))

#print(imputed_training_data_merged1)

```

```{r}
# Define the selected items
selected_items <- c("F1F2", "F1F4", "F1F5", "F1F6", "F1F7", "F1F11", "F1F3", "F2F1", "F2F9", "F2F8", "F2F12", "F2F13", "F2F7", "F3F7", "F3F8" ,"F3F9", "F4F2", "F4F5", "F5F17", "F5F15", "F1F14", "F1F13")

# Modify the imputed_training_data_c to merge answers 1 and 2 into 2 for these selected items
imputed_training_data_merged2 <- imputed_training_data_merged1 %>%
  mutate(across(all_of(selected_items), ~ ifelse(. %in% c(1, 2), 2, .)))

#print(imputed_training_data_merged2)

```

```{r}
value_counts_merged2 <- imputed_training_data_merged2 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


#print(value_counts_merged2)
```

```{r}
# Define the selected items
selected_items23 <- c("F2F7", "F2F13", "F2F12", "F2F1")

# Modify the imputed_training_data_c to merge answers 1 and 2 into 2 for these selected items
imputed_training_data_merged3 <- imputed_training_data_merged2 %>%
  mutate(across(all_of(selected_items23), ~ ifelse(. %in% c(2, 3), 3, .)))

#print(imputed_training_data_merged3)
```

```{r}
value_counts_merged3 <- imputed_training_data_merged3 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


#print(value_counts_merged3)
```

```{r}
# Function to count unique values for each variable in a dataset
count_unique_values <- function(data) {
  data %>%
    pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
    count(Variable, Value) %>%
    pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))
}

# Count unique values for each variable in imputed_test_data_c
value_counts_test <- count_unique_values(imputed_test_data_merged3) #imputed_test_data_c

# Count unique values for each variable in imputed_training_data_merged3
value_counts_train <- count_unique_values(imputed_training_data_merged3)

# Identify common variables in both datasets
common_vars <- intersect(names(value_counts_train), names(value_counts_test))

# Exclude specific variables if they are not present in the training dataset
excluded_vars <- c("F2F17", "F2F4")
common_vars_filtered <- setdiff(common_vars, excluded_vars)

# Filter out variables not present in both datasets
value_counts_train_filtered <- value_counts_train %>%
  select(all_of(common_vars_filtered))

value_counts_test_filtered <- value_counts_test %>%
  select(all_of(common_vars_filtered))

# Ensure both dataframes have the same structure for comparison
value_counts_train_full <- value_counts_train_filtered %>%
  replace_na(list(n = 0))

value_counts_test_full <- value_counts_test_filtered %>%
  replace_na(list(n = 0))

# Combine both datasets for comparison
comparison <- value_counts_train_full %>%
  full_join(value_counts_test_full, by = "Variable", suffix = c("_train", "_test"))

# Function to compare sets of categories
compare_category_sets <- function(comp) {
  discrepancies <- comp %>%
    rowwise() %>%
    mutate(
      # Extract and count non-zero categories for train and test
      Unique_categories_train = sum(c_across(contains("_train")) > 0),
      Unique_categories_test = sum(c_across(contains("_test")) > 0),
      # Check if the number of unique categories differs
      Different_categories = Unique_categories_train != Unique_categories_test
    ) %>%
    filter(Different_categories) %>%
    select(Variable, Unique_categories_train, Unique_categories_test)
  
  return(discrepancies)
}

# Get discrepancies where category counts differ
discrepancies <- compare_category_sets(comparison)

# Print summary of discrepancies
cat("Variables with Different Number of Dimensions/Categories:\n")
print(discrepancies)

```

# The same for test data
## Finding Thresholds of Items to merge in order to ose ordered data in the item selection and crossvalidation
```{r}
# Count values for each variable in imputed_test_data_c
value_counts_test <- imputed_test_data_c %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts_test)

# Filter for cases with values smaller or equal to 2
filtered_value_counts_test <- value_counts %>%
  filter(rowSums(. <= 2) > 0)

print(filtered_value_counts)

# Get the list of items to be modified
items_to_modify <- filtered_value_counts$Variable

# Ensure items_to_modify is a character vector
items_to_modify <- as.character(items_to_modify)

# Modify the imputed_test_data_c to merge answers 0 and 1 into 1 for these items
imputed_test_data_merged <- imputed_test_data_c %>%
  mutate(across(all_of(items_to_modify), ~ ifelse(. %in% c(0, 1), 1, .)))

imputed_test_data_merged <- imputed_test_data_merged %>%
  select(-c(F2F17, F2F4))

print(imputed_test_data_merged)

```

```{r}
value_counts_merged_test <- imputed_test_data_c %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

#print(value_counts_merged_test)
```

```{r}

# Define the selected items
selected_items10 <- c("F1F12", "F2F10", "F3F1", "F3F4", "F4F8", "F5F1", "F5F16", "F5F9", "F5F5", "F5F13", "F4F6")

# Modify the imputed_training_data_c to merge answers 0 and 1 into 1 for these selected items
imputed_test_data_merged1 <- imputed_test_data_merged %>%
  mutate(across(all_of(selected_items10), ~ ifelse(. %in% c(0, 1), 1, .)))

#print(imputed_training_data_merged1)

```

```{r}
# Define the selected items
selected_items12t <- c("F1F2", "F1F4", "F1F5", "F1F6", "F1F7", "F1F11", "F1F3", "F2F1", "F2F9", "F2F8", "F2F12", "F2F13", "F2F7", "F3F7", "F3F8" ,"F3F9", "F4F2", "F4F5", "F5F17", "F5F15", "F1F14", "F1F13")

# Modify the imputed_training_data_c to merge answers 1 and 2 into 2 for these selected items
imputed_test_data_merged2 <- imputed_test_data_merged1 %>%
  mutate(across(all_of(selected_items12t), ~ ifelse(. %in% c(1, 2), 2, .)))

#print(imputed_test_data_merged2)

```

```{r}
# Define the selected items
selected_items23 <- c("F2F7", "F2F13", "F2F12", "F2F1")

# Modify the imputed_test_data_c to merge answers 2 and 3 into 3 for these selected items
imputed_test_data_merged3 <- imputed_test_data_merged2 %>%
  mutate(across(all_of(selected_items23), ~ ifelse(. %in% c(2, 3), 3, .)))

#print(imputed_test_data_merged3)
```


```{r}
value_counts_merged3_test <- imputed_test_data_merged3 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


print(value_counts_merged3_test)
```


## Preparing a metric data frame for the MG-CFA
```{r}
# Add the 'split' column to each data frame
imputed_training_data_c <- imputed_training_data_c %>%
  mutate(split = 1)

imputed_test_data_c <- imputed_test_data_c %>%
  mutate(split = 2)

# Join the two data frames
joined_mgcfa_data <- bind_rows(imputed_training_data_c, imputed_test_data_c)

# Print the resulting data frame
print(joined_mgcfa_data)

```

## ordinal data transformation?
```{r}
ords <- imputed_training_data_merged3[, names(imputed_training_data_merged3)%in%unlist(fs)]
ords <- lapply(ords, as.ordered)
ords <- do.call(data.frame, ords)

# Print the resulting data frame
print(ords)
```

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Function to count unique values for each variable in a dataset
count_unique_values <- function(data) {
  data %>%
    pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
    count(Variable, Value) %>%
    pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))
}

# Count unique values for each variable in imputed_test_data_c
value_counts_test <- count_unique_values(imputed_test_data_merged3) #imputed_test_data_c

# Count unique values for each variable in imputed_training_data_merged3
value_counts_train <- count_unique_values(imputed_training_data_merged3)

# Identify common variables in both datasets
common_vars <- intersect(names(value_counts_train), names(value_counts_test))

# Exclude specific variables if they are not present in the training dataset
excluded_vars <- c("F2F17", "F2F4")
common_vars_filtered <- setdiff(common_vars, excluded_vars)

# Filter out variables not present in both datasets
value_counts_train_filtered <- value_counts_train %>%
  select(all_of(common_vars_filtered))

value_counts_test_filtered <- value_counts_test %>%
  select(all_of(common_vars_filtered))

# Ensure both dataframes have the same structure for comparison
value_counts_train_full <- value_counts_train_filtered %>%
  replace_na(list(n = 0))

value_counts_test_full <- value_counts_test_filtered %>%
  replace_na(list(n = 0))

# Combine both datasets for comparison
comparison <- value_counts_train_full %>%
  full_join(value_counts_test_full, by = "Variable", suffix = c("_train", "_test"))

# Function to compare sets of categories
compare_category_sets <- function(comp) {
  discrepancies <- comp %>%
    rowwise() %>%
    mutate(
      # Extract and count non-zero categories for train and test
      Unique_categories_train = sum(c_across(contains("_train")) > 0),
      Unique_categories_test = sum(c_across(contains("_test")) > 0),
      # Check if the number of unique categories differs
      Different_categories = Unique_categories_train != Unique_categories_test
    ) %>%
    filter(Different_categories) %>%
    select(Variable, Unique_categories_train, Unique_categories_test)
  
  return(discrepancies)
}

# Get discrepancies where category counts differ
discrepancies <- compare_category_sets(comparison)

# Print summary of discrepancies
cat("Variables with Different Number of Dimensions/Categories:\n")
print(discrepancies)

```

### Creating data frame with equal levels per item for MI testing with ordinal data format
```{r}
library(dplyr)
library(tidyr)

# Define the selected items
selected_items01 <- c("F1F8", "F2F14", "F2F20")

# Modify the imputed_test_data_c to merge answers 0 and 1 into 1 for these selected items in a new data frame
imputed_test_data_merged_MI <- imputed_test_data_merged3 %>%
  mutate(across(all_of(selected_items01), ~ ifelse(. %in% c(0, 1), 1, .)))

print(imputed_test_data_merged_MI)
##########

# Define the selected items
selected_items02 <- "F3F2"

# Modify the imputed_test_data_c to merge answers 0 ,1 and 2 into 2 for these selected items
imputed_test_data_merged_MI <- imputed_test_data_merged_MI %>%
  mutate(across(all_of(selected_items02), ~ ifelse(. %in% c(1, 2), 2, .)))

print(imputed_test_data_merged_MI)

###########
# Define the selected items
selected_items_tr01 <- c("F1F8","F2F14", "F2F20")

# Modify the imputed_training_data_merged3 to merge answers 0 and 1 into 1 for these selected items
imputed_train_data_merged_MI <- imputed_training_data_merged3 %>%
  mutate(across(all_of(selected_items_tr01), ~ ifelse(. %in% c(0, 1), 1, .)))

print(imputed_train_data_merged_MI)
############

# Define the selected items
selected_items_tr12 <- c("F1F1", "F1F5", "F1F12", "F2F15", "F3F3")

# Modify the imputed_training_data_merged3 to merge answers 0,1 and 2 into 2 for these selected items
imputed_train_data_merged_MI <- imputed_train_data_merged_MI %>%
  mutate(across(all_of(selected_items_tr12), ~ ifelse(. %in% c(0, 1, 2), 2, .)))

############

# Define the selected items
selected_items_tr23 <- "F3F9"

# Modify the imputed_training_data_merged3 to merge answers 2 and 3 into 3 for these selected items
imputed_train_data_merged_MI <- imputed_train_data_merged_MI %>%
  mutate(across(all_of(selected_items_tr23), ~ ifelse(. %in% c(2, 3), 3, .)))

print(imputed_train_data_merged_MI)

```

## ordinal MI data
```{r}
ords1 <- imputed_train_data_merged_MI[, names(imputed_train_data_merged_MI)%in%unlist(fs)]
ords1 <- lapply(ords1, as.ordered)
ords1 <- do.call(data.frame, ords1)

ords2 <- imputed_test_data_merged_MI[, names(imputed_test_data_merged_MI)%in%unlist(fs)]
ords2 <- lapply(ords2, as.ordered)
ords2 <- do.call(data.frame, ords2)

# Step 1: Get the list of common factor levels for each column
common_levels <- lapply(names(ords1), function(col) {
  unique(c(levels(ords1[[col]]), levels(ords2[[col]])))
})

# Step 2: Ensure each column in both data frames has these common levels
for (i in seq_along(ords1)) {
  ords1[[i]] <- factor(ords1[[i]], levels = common_levels[[i]], ordered = TRUE)
  ords2[[i]] <- factor(ords2[[i]], levels = common_levels[[i]], ordered = TRUE)
}

# Step 3: Convert lists back to data frames
ords1 <- as.data.frame(ords1)
ords2 <- as.data.frame(ords2)

# Step 4: Add the 'split' column to each data frame
ords1$split <- 1
ords2$split <- 2

# Step 5: Combine the data frames
ords_mi <- bind_rows(ords1, ords2)

```

# Up until here - regarding the reproducibility comment

## Objective-Function
```{r}
library(stuart)
library(lavaan)

#extracting the taus
taus <- lavInspect(fit_again, "est")$tau
# printing them
print(taus)

sd_tau <- sd(taus[grep('t1', rownames(taus))]) + 
                 sd(taus[grep('t2', rownames(taus))]) +
                 sd(taus[grep('t3', rownames(taus))]) + 
                 sd(taus[grep('t4', rownames(taus))]) +
                 sd(taus[grep('t5', rownames(taus))])

print(theothrsh_tau)#2.194

overall_sd_tau <- sd(taus)
print(overall_sd_tau)# 1.136692

#ordinal data
objective.normal <- function(rmsea.scaled, srmr, cfi.scaled) {
  out1 = 0.5-(0.5/(1 + exp(- 100 * (rmsea.scaled-.05))))
  out2 = 0.5-(0.5/(1 + exp(- 100 * (srmr-.05))))
  out3 = (1/(1 + exp(- 100 * (cfi.scaled-.95))))
  out = (out1 + out2 + out3)/3
  return(out)                                
}


```

## Sensitivity Analysis
```{r}
set.seed(2028)
library(lavaan)
library(SEMsens)

# Define your original model
cfa_model_m

# Define sensitivity model
sens_model <- '
  Comprehension =~ F1F6 + F1F8 + F1F11 + F1F14
  Evaluation =~ F2F2 + F2F6 + F2F15 + F2F20
  Integration =~ F3F2 + F3F4 + F3F6 + F3F9
  Communication =~ F4F1 + F4F3 + F4F6 + F4F8
  Statistics =~ F5F3 + F5F4 + F5F8 + F5F18

  Comprehension ~~ Comprehension
  Evaluation ~~ Evaluation
  Integration ~~ Integration
  Communication ~~ Communication
  Statistics ~~ Statistics

  Comprehension ~~ Evaluation
  Comprehension ~~ Integration
  Comprehension ~~ Communication
  Comprehension ~~ Statistics
  Evaluation ~~ Integration
  Evaluation ~~ Communication
  Evaluation ~~ Statistics
  Integration ~~ Communication
  Integration ~~ Statistics
  Communication ~~ Statistics

  Comprehension ~ phantom1*phantom
  Evaluation ~ phantom2*phantom
  Integration ~ phantom3*phantom
  Communication ~ phantom4*phantom
  Statistics ~ phantom5*phantom
  
  phantom =~ 0
  phantom ~~ 1*phantom
'

# Fit the original model
old.out <- sem(cfa_model_m,missing="fiml", data = df_mi)

# Fit the sensitivity analysis model
my.sa <- sa.aco(
  data = df_mi,
  model = cfa_model_m,
  sens.model = sens_model,
  sample.nobs = nrow(df_mi),
  k = 10,
  missing = "fiml",
  max.value = 200,
  max.iter = 100,
  opt.fun = 1,
  estimator = "MLR",
  rate.of.conv = .1,
  paths = c(1:20),
  seed = 1,
  verbose = FALSE
)

# Get and review results
my.sa.table <- sens.tables(my.sa)
print(my.sa.table)

```

```{r}
lvcor <- corr_matrix_group1 

obj_default4 <- function(chisq, df, pvalue, rmsea.robust, srmr, cfi.robust, crel, lvcor)
{(1-rmsea.robust)+(1-srmr)+(1+cfi.robust)+ sum(lvcor[lower.tri(lvcor)]^2)}

fit_coll <- gene(
  data = df_dl,
  factor.structure = fs,  
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2028,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default4,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE
)
```

```{r}
summary(fit_coll)

inspect(fit_coll$final, 'est')
inspect(fit_coll$final, 'fit')

lavaan::summary(fit_coll$final, standardized = TRUE) 
```

```{r}
set.seed(2028)
library(lavaan)

cfa_model_coll <- '
# Measurement model
Comprehension =~ F1F3 + F1F4 + F1F9 + F1F12
Evaluation =~ F2F3 + F2F8 + F2F10 + F2F12
Integration =~ F3F3 + F3F6 + F3F8 + F3F9
Communication =~ F4F2 + F4F5 + F4F6 + F4F8
Statistics =~ F5F5 + F5F7 + F5F10 + F5F15

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'

fit_cfa_model_coll <- cfa(cfa_model_coll, df_mi, 
                       missing = "fiml", 
                       estimator = "MLR", 
                       group = "split", 
                       em.h1.iter.max = 1000)

summary(fit_cfa_model_coll, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_coll, 'fit')
```

## Multicollinearity
```{r}
cov_matrix2 <- lavInspect(fit_cfa_model_coll, "cov.lv")
print(cov_matrix2)
# Check eigenvalues for group 1 covariance matrix
eigenvalues_group1 <- eigen(cov_matrix2[[1]])$values
print(eigenvalues_group1)

# Check eigenvalues for group 2 covariance matrix
eigenvalues_group2 <- eigen(cov_matrix2[[2]])$values
print(eigenvalues_group2)

# Convert covariance matrices to correlation matrices
corr_matrix2_group1 <- cov2cor(cov_matrix2[[1]])
corr_matrix2_group2 <- cov2cor(cov_matrix2[[2]])

# Print correlation matrices
print(corr_matrix2_group1)
print(corr_matrix2_group2)

```

##########################################################################
### Objective Function with NU (Metric and MLR)
```{r}
obj_default_nu <- function(chisq, df, pvalue, cfi.robust, rmsea.robust, srmr, crel, nu) {
  # Ensure nu is an atomic vector
  if (!is.atomic(nu)) {
    nu <- unlist(nu)
  }
  
  # Calculate the standard deviation of nu
  nu_sd <- sd(nu)
  
  # Compute the objective function
  1/(1 + exp(-10 * (crel - 0.6))) + 
  1/(1 + exp(-10 * (cfi.robust - 0.6))) +
  0.5 * (1 - (1/(1 + exp(-100 * (rmsea.robust - 0.05))))) + 
  0.5 * (1 - (1/(1 + exp(-100 * (srmr - 0.06))))) + 
  1 / (1 + exp(-10 * (nu_sd - 0.1))) # Adjust the threshold as needed
}
```

### Kfold-Crossvalidation with NU (Metric and MLR) - looped
```{r}
library(stuart)

# Number of iterations for averaging
num_iterations <- 3

# Initialize a list to store the results of each iteration
results_list_nu <- vector("list", num_iterations)

# Loop over the number of iterations
for (i in 1:num_iterations) {
  # Perform k-fold cross-validation and store the result
  results_list_nu[[i]] <- kfold(
  'gene', 
  k = 3, 
  data = imputed_training_data_c, 
  factor.structure = fs,  # Pass the variable directly
  max.invariance = "strict", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2024,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default_nu,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
}

# Initialize a list to store averaged fit indices
fit_indices_nu <- list(
  SRMR = numeric(num_iterations),
  RMSEA = numeric(num_iterations),
  CFI = numeric(num_iterations)
)

# Extract fit indices from each result
for (i in 1:num_iterations) {
  # Ensure the results are not NULL
  if (!is.null(results_list_nu[[i]])) {
    # Use the inspect function to get the fit measures
    fit_measures <- lavaan::inspect(results_list_nu[[i]]$final, "fit")
    
    # Store the fit indices if they exist
    if (!is.null(fit_measures)) {
      fit_indices_nu$SRMR[i] <- fit_measures["srmr"]
      fit_indices_nu$RMSEA[i] <- fit_measures["rmsea"]
      fit_indices_nu$CFI[i] <- fit_measures["cfi"]
    }
  }
}

# Compute the average of each fit index, ensuring NA values are handled
average_fit_indices <- sapply(fit_indices_nu, function(x) mean(x, na.rm = TRUE))

# Print the average fit indices
print("Average Fit Indices from Results:")
print(average_fit_indices)

# Inspect the complete output for available information
inspect(results_list_nu[[1]]$final, 'fit')
inspect(results_list_nu[[1]]$final, 'est')

lavaan::standardizedSolution(results_list_nu[[i]]$final)
#########################################################
```

### MG-CFA with NU and Metric data 
```{r}
cfa_model_m1 <- '
# Measurement model
Comprehension =~ F1F5 + F1F6 + F1F8 + F1F9 
Evaluation =~ F2F3 + F2F7 + F2F8 + F2F20 
Integration =~ F3F1 + F3F2 + F3F7 + F3F9 
Communication =~ F4F3 + F4F4 + F4F6 + F4F7 
Statistics =~ F5F3 + F5F5 + F5F7 + F5F8 

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'
fit_cfa_model_m1 <- cfa(cfa_model_m1, joined_mgcfa_data, estimator = "MLR", group = "split" )
summary(fit_cfa_model_m1, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_m1, 'fit')
```

#### configural to metric
```{r}
fit_cfa_model_metric1 <- cfa(cfa_model_m1, joined_mgcfa_data, estimator = "MLR", group = "split", group.equal= "loadings")

comp_fit_metric1 <- compareFit(fit_cfa_model_m1, fit_cfa_model_metric1)
summary(comp_fit_metric1)

```
#### metric to scalar
```{r}
fit_cfa_model_scalar1 <- cfa(cfa_model_m1, joined_mgcfa_data, estimator = "MLR", group = "split",
                            group.equal = c('loadings','intercepts'))

comp_fit_scalar1 <- compareFit(fit_cfa_model_metric1,fit_cfa_model_scalar1)
summary(comp_fit_scalar1)

```

#### scalar to strict
```{r}
fit_cfa_model_strict1 <- cfa(cfa_model_m1, joined_mgcfa_data, estimator = "MLR", group = "split",
                            group.equal = c('loadings','intercepts','residuals'))

comp_fit_strict1 <- compareFit(fit_cfa_model_scalar1, fit_cfa_model_strict1)
summary(comp_fit_strict1)

```
##########
```{r}
library(lavaan)
# Extract the tau matrix from the fitted model
tau <- lavInspect(fit_again, "est")$tau

# Extract the item names by splitting the row names at the "|" character
item_names <- sapply(strsplit(rownames(tau), "\\|"), `[`, 1)

# Get unique item names
unique_items <- unique(item_names)

# Initialize an empty matrix to store the results
taus <- matrix(NA, nrow = length(unique_items), ncol = 2)
colnames(taus) <- c("Item", "Avg_SD")

# Loop through each unique item
for (i in seq_along(unique_items)) {
  item <- unique_items[i]
  
  # Extract the thresholds for the item
  thresholds <- tau[item_names == item, , drop = FALSE]
  
  # Calculate the standard deviation for the thresholds
  sd_values <- sd(thresholds, na.rm = TRUE)
  
  # Store the item name and average standard deviation in the matrix
  taus[i, ] <- c(item, sd_values)
}

# Convert the matrix to a data frame for better readability
taus <- as.data.frame(taus)
taus$Avg_SD <- as.numeric(as.character(taus$Avg_SD))

# Print the resulting matrix
print(taus)

# Create a new matrix with Avg_SD values and row names as Item names
taus_matrix <- matrix(taus$Avg_SD, nrow = length(unique_items), ncol = 1)
rownames(taus_matrix) <- taus$Item
colnames(taus_matrix) <- "Avg_SD"

# Print the resulting matrix
print(taus_matrix)
```

################################################

## K-fold Crossvalidation with Ordinal data Structure (WLSMV)
```{r, eval=FALSE}
library(stuart)
set.seed(125)
# Number of iterations for averaging
num_iterations <- 3

# Initialize a list to store the results of each iteration
results_list_ord <- vector("list", num_iterations)

# Loop over the number of iterations
for (i in 1:num_iterations) {
  # Perform k-fold cross-validation and store the result
  results_list_ord[[i]] <- kfold(
  'gene', 
  k = 3, 
  data = ords1,
  factor.structure = fs,  
  max.invariance = "strong", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2026,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = objective.normal,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
}

# Initialize a list to store averaged fit indices
fit_indices_ord <- list(
  SRMR = numeric(num_iterations),
  RMSEA = numeric(num_iterations),
  CFI = numeric(num_iterations)
)

# Extract fit indices from each result
for (i in 1:num_iterations) {
  # Ensure the results are not NULL
  if (!is.null(results_list_ord[[i]])) {
    # Use the inspect function to get the fit measures
    fit_measures <- lavaan::inspect(results_list_ord[[i]]$final, "fit")
    
    # Store the fit indices if they exist
    if (!is.null(fit_measures)) {
      fit_indices_ord$SRMR[i] <- fit_measures["srmr"]
      fit_indices_ord$RMSEA[i] <- fit_measures["rmsea"]
      fit_indices_ord$CFI[i] <- fit_measures["cfi"]
    }
  }
}

# Compute the average of each fit index, ensuring NA values are handled
average_fit_indices_ord <- sapply(fit_indices_ord, function(x) mean(x, na.rm = TRUE))

# Print the average fit indices
print("Average Fit Indices from Results:")
print(average_fit_indices_ord)

# Inspect the complete output for available information
inspect(results_list_ord[[1]]$final, 'fit')
inspect(results_list_ord[[1]]$final, 'est')

lavaan::standardizedSolution(results_list_ord[[i]]$final)

```

```{r, eval=FALSE}
library(stuart)
set.seed(127)
# Number of iterations for averaging
num_iterations <- 5

# Initialize a list to store the results of each iteration
results_list_ord1 <- vector("list", num_iterations)

# Loop over the number of iterations
for (i in 1:num_iterations) {
  # Perform k-fold cross-validation and store the result
  results_list_ord1[[i]] <- kfold(
  'gene', 
  k = 3, 
  data = ords1,
  factor.structure = fs,  
  max.invariance = "strong", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2026,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 6,
  objective = objective.normal,
  ignore.errors = TRUE,
  burnin = 10,
  generations = 1000,
  individuals = 50,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = 1e-6, # stricter tolerance
  reinit.n = 2, # adjusted reinit
  reinit.criterion = "geno.between",
  reinit.tolerance = 1e-6, # stricter reinit tolerance
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
}

# Initialize a list to store averaged fit indices
fit_indices_ord1 <- list(
  SRMR = numeric(num_iterations),
  RMSEA = numeric(num_iterations),
  CFI = numeric(num_iterations)
)

# Extract fit indices from each result
for (i in 1:num_iterations) {
  # Ensure the results are not NULL
  if (!is.null(results_list_ord1[[i]])) {
    # Use the inspect function to get the fit measures
    fit_measures_ord1 <- lavaan::inspect(results_list_ord1[[i]]$final, "fit")
    
    # Store the fit indices if they exist
    if (!is.null(fit_measures)) {
      fit_indices_ord1$SRMR[i] <- fit_measures["srmr"]
      fit_indices_ord1$RMSEA[i] <- fit_measures["rmsea"]
      fit_indices_ord1$CFI[i] <- fit_measures["cfi"]
    }
  }
}

# Compute the average of each fit index, ensuring NA values are handled
average_fit_indices_ord1 <- sapply(fit_indices_ord1, function(x) mean(x, na.rm = TRUE))

# Print the average fit indices
print("Average Fit Indices from Results:")
print(average_fit_indices_ord1)

# Inspect the complete output for available information
inspect(results_list_ord1[[1]]$final, 'fit')
inspect(results_list_ord1[[1]]$final, 'est')

lavaan::standardizedSolution(results_list_ord1[[i]]$final)

```

```{r}
# Measurement model before fuck up
#Comprehension =~ F1F4 + F1F6 + F1F10 + F1F11 
#Evaluation =~ F2F1 + F2F6 + F2F13 + F2F16 
#Integration =~ F3F5 + F3F6 + F3F7 + F3F8 
#Communication =~ F4F3 + F4F6 + F4F7 + F4F8 
#Statistics =~ F5F6 + F5F8 + F5F9 + F5F16 

cfa_model_ord <- '
# Measurement model
Comprehension =~ F1F5 + F1F6 + F1F8 + F1F9 
Evaluation =~ F2F10 + F2F14 + F2F15 + F2F20 
Integration =~ F3F2 + F3F7 + F3F8 + F3F8 
Communication =~ F4F3 + F4F4 + F4F6 + F4F7 
Statistics =~ F5F1 + F5F5 + F5F8 + F5F15 

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'
fit_cfa_model_ords <- cfa(cfa_model_ord, ords, ordered = TRUE, estimator = "WLSMV")
summary(fit_cfa_model_ords, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_ords,'fit')
```

### MG-CFA with Ordinal data Structure (WLSMV)
```{r}
fit_cfa_model_ords <- cfa(cfa_model_ord, ords_mi, ordered = TRUE, estimator = "WLSMV", group = "split")
summary(fit_cfa_model_ords, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_ords,'fit')
```

#### configural to metric
```{r}
library(lavaan)
library(semTools)
fit_cfa_model_ords_m <- cfa(cfa_model_ord, ords_mi, estimator = "WLSMV", group = "split", group.equal= "loadings")

comp_fit_ord <- compareFit(fit_cfa_model_ords, fit_cfa_model_ords_m)
summary(comp_fit_ord)
inspect(fit_cfa_model_ords_m ,'fit')
```
#### configural to metric
Cfi scaled: 
rmsea scaled: 
srmr delta: 

#### metric to scalar
```{r}
fit_cfa_model_ords_scalar <- cfa(cfa_model_ord, ords_mi, estimator = "WLSMV", group = "split",
                            group.equal = c('loadings','intercepts'))

comp_fit_ords_scalar <- compareFit(fit_cfa_model_ords_m,fit_cfa_model_ords_scalar)
summary(comp_fit_ords_scalar)
inspect(fit_cfa_model_ords_scalar,'fit')
```

### Trying to exchange some items with especially low loadings
```{r}
cfa_model_ord_manu <- '
# Measurement model
Comprehension =~ F1F2 + F1F5 + F1F6 + F1F11 
Evaluation =~ F2F1 + F2F6 + F2F13 + F2F16 
Integration =~ F3F5 + F3F6 + F3F7 + F3F9 
Communication =~ F4F3 + F4F6 + F4F7 + F4F8 
Statistics =~ F5F1 + F5F6 + F5F8 + F5F9

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'
fit_cfa_model_ords_manu <- cfa(cfa_model_ord_manu, ords, ordered = TRUE, estimator = "WLSMV")
summary(fit_cfa_model_ords_manu, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_ords_manu,'fit')
```

```{r}
fit_cfa_model_ords_manu <- cfa(cfa_model_ord_manu, ords_mi, ordered = TRUE, estimator = "WLSMV", group = "split")
summary(fit_cfa_model_ords_manu, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_ords_manu,'fit')
```

#### configural to metric
```{r}
library(lavaan)
library(semTools)
fit_model_ord_manu_m <- cfa(cfa_model_ord_manu, ords_mi, ordered = TRUE,estimator = "WLSMV", group = "split", group.equal= "loadings")

comp_fit_ord_manu <- compareFit(fit_cfa_model_ords_manu, fit_model_ord_manu_m)
summary(comp_fit_ord_manu)
inspect(fit_model_ord_manu_m ,'fit')
```


#### metric to scalar
```{r}
fit_model_ord_manu_scalar <- cfa(cfa_model_ord_manu, ords_mi, estimator = "WLSMV", group = "split",
                            group.equal = c('loadings','intercepts'))

comp_fit_ord_manu_scalar <- compareFit(fit_model_ord_manu_m,fit_model_ord_manu_scalar)
summary(comp_fit_ord_manu_scalar)
inspect(fit_model_ord_manu_scalar,'fit')
```


##########################################################################

## Trying to incorporate tau 
### K-fold Crossvalidation with Ordinal data Structure (WLSMV) and Tau
```{r, eval=FALSE}
library(stuart)
# objective
# Define the objective function
#
objective.normal2 <- function(rmsea.scaled, srmr, cfi.scaled, tau) {

  if (!is.numeric(tau)) {
    tau <- as.numeric(tau)
  }
  # Compute the components of the objective function
  out1 <- 0.5 - (0.5 / (1 + exp(-100 * (rmsea.scaled - 0.05))))
  out2 <- 0.5 - (0.5 / (1 + exp(-100 * (srmr - 0.05))))
  out3 <- 1 / (1 + exp(-100 * (cfi.scaled - 0.95)))
  out4 <- sd(tau)  
  
  # Combine the components into the final objective function
  out <- (out1 + out2 + out3 + out4) / 4
  return(out)
}

kfold_sel3 <- kfold(
  'gene', 
  k = 3, 
  data = ords, 
  factor.structure = fs,  
  max.invariance = "strong", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2026,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = objective.normal2,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 300,
  individuals = 30,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
# Inspect the structure of kfold_sel2$final
tau_values3a <- lavaan::inspect(kfold_sel3$final, 'est')$A$tau
tau_values3b <- lavaan::inspect(kfold_sel3$final, 'est')$B$tau
print(tau_values3a)
print(tau_values3b)

#########################################################
```

### Summary and Results of K-fold Crossvalidation with Ordinal data Structure (WLSMV) and Tau
```{r}
summary(kfold_sel3)

inspect(kfold_sel3$final, 'est')
inspect(kfold_sel3$final, 'fit')

lavaan::summary(kfold_sel3$final, standardized = TRUE) 
lavaan::inspect(kfold_sel3$final, 'est')$A$tau
lavaan::inspect(kfold_sel3$final, 'est')$B$tau
```

---
title: "Analysis"
author: "Leonie"
date: "2024-04-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# to clear environment while working
rm(list=ls()) 

usethis::use_git_config(
  user.name = "LeonieHagitte",
  user.email = "leonie.hagitte@student.hu-berlin.de",
  init.defaultBranch = "devl")

```

```{r, echo=FALSE}
library(renv)
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)
library(dplyr)
library(mice)
library(readr)
library(lavaan)
library(semPower)
library(stuart)
library(reproducibleRchunks)

```

# Reproducibility 
```{r}
if(!requireNamespace("remotes"))
install.packages("remotes")
remotes::install_github("aaronpeikert/repro")
repro::automate() #creating a Dockerfile.    ####################### BUG ######
repro::use_docker()
repro::use_gha_docker() #GitHub action (GHA) is a cloud service that runs software when certain events trigger it.Add a GitHub Action to build the required Docker image with

library(renv)
setwd("/Users/leo/Desktop/Programming stuff/Thesis")
# Initialize 'renv' in your project
renv::init()

# Activate the project-specific 'renv' environment
renv::activate()

# Install required packages for your project
#renv::install()

# Update renv.lock file
renv::snapshot()

if(!requireNamespace("pak"))
install.packages("pak")
pak::pkg_install("brandmaier/reproducibleRchunks")

```

# Model specification
```{reproducibleR}
model <- "
# Measurement model
Comprehension =~ x1 + x2 + x3 + x4
Evaluation =~ x5 + x6 + x7 + x8
Integration =~ x9 + x10 + x11 + x12
Communication =~ x13 + x14 + x15 + x16
Statistics =~ x17 + x18 + x19 + x20

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
"

```

# Data Preparation
## Data enreading
```{r}
library(dplyr)
library(mice)
library(readr)

dataMA <- read_csv("/Users/leo/Desktop/Programming stuff/Thesis/structure/Analyses/DataMA.csv")

# Convert all columns to character to ensure consistent replacement
df <- data.frame(lapply(dataMA, as.character))

# Replace "AO01" etc with respective numeric values across all columns
df[] <- lapply(df, function(x) gsub("AO01", "1", x))
df[] <- lapply(df, function(x) gsub("AO02", "2", x))
df[] <- lapply(df, function(x) gsub("AO03", "3", x))
df[] <- lapply(df, function(x) gsub("AO04", "4", x))
df[] <- lapply(df, function(x) gsub("AO05", "5", x))
df[] <- lapply(df, function(x) gsub("AO06", "6", x))
df[] <- lapply(df, function(x) gsub("AO07", "7", x))
df[] <- lapply(df, function(x) gsub("AO08", "8", x))
df[] <- lapply(df, function(x) gsub("AO09", "9", x))
df[] <- lapply(df, function(x) gsub("AO10", "10", x))

# Convert columns back to numeric if all values in a column can be converted
df <- data.frame(lapply(df, function(x) {
  converted <- suppressWarnings(as.numeric(x))
  if(all(!is.na(converted))) converted else x
}))

# Rename column "ICT.ATC3." to "ATC3"
names(df)[names(df) == "ICT.ATC3."] <- "ATC3"

```

## Attention checks
```{r}
# Calculate the number of wrong answers for each person

df$wrong_answers <- 0
df$wrong_answers <- df$wrong_answers + (df$ATC1 != 1)
df$wrong_answers <- df$wrong_answers + (df$ATC2 != 0)
df$wrong_answers <- df$wrong_answers + (df$ATC3 != 5)
# df$wrong_answers <- df$wrong_answers + (df$ATC4 != 2)

# Filter IDs of people who failed more than two items
failed_ids <- df$id[df$wrong_answers > 2]

# Display the IDs
failed_ids

# 46, 92, 275, 283, 419, 512, 588, 599, 613, 663, 668, 713

# Count the number of people who failed more than two items, ignoring NA values
num_failed_more_than_two <- sum(df$wrong_answers > 2, na.rm = TRUE)

# Display the count
num_failed_more_than_two
```

## Filter for stubborn people 
```{r}
# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Bei der ersten Kontrollfrage (für die Qualitätssicherheit) dachte ich, es wäre eine Fangfrage & habe NICHT das gedrückt, was ich sollte… ansonsten habe ich alles nach bestem Wissen und Gewissen ausgefüllt ????????????"]

# Display the ID
person_id #id=23

# Find the ID of the person who wrote the specific comment in the "Comments" column
person_id <- df$id[df$Comments == "Die Fragen ohne Inhalt mit Klickanweisung waren bizarr. Hintergrund unverständlich und nicht erläutert. Deshalb mit „weiß nicht“ beantwortet"]

# Display the ID
person_id #id=71

## Very long Comment - I couldnt find with the original code, i had used up to this point.##
# Defining a pattern that captures the essence of the comment while allowing for some variation
# For simplicity, this example uses a shorter, distinctive part of the comment
# Adjust the pattern as needed to be specific enough to uniquely identify the comment
pattern <- "Die Umfrage war sehr erhellend.*menschlichen Mitteln.*Doktor Faust"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id=275

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Fragen irritiert. Ich habe sie nicht wie gefordert beantwortet, da mir nicht transparent war, wozu dies wirklich dient. Es fühlte sich mehr wie ein Experiment an: Machen die Leute, was man ihnen sagt?"

# Using grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, perl = TRUE)

# Using the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids # id=419

# Find the ID of the person who wrote the specific comment in the "Comments" column
pattern <- "Habe die Frage.*nicht verstanden und immer irgendetwas gedrückt.*"

# Use grep to search for the pattern in the "Comments" column, returning the row indices
matching_rows <- grep(pattern, df$Comments, ignore.case = TRUE, perl = TRUE)

# Use the indices to find the IDs of the matching rows
matching_ids <- df$id[matching_rows]

# Display the matching IDs
matching_ids #id= 760

# Excluding stubborn people from former failed id list

# 46, 92, 283, 512, 588, 599, 613, 663, 668, 713
# leaves 10 people to exclude

```
```{r}
# Define the IDs to exclude
ids_to_exclude <- c(46, 92, 283, 512, 588, 599, 613, 663, 668, 713,796)

# id 796 is to be excluded, because the person indicated an age of 17
# Filter out the rows with the specified IDs and save the result to a new dataframe
df2 <- df[!df$id %in% ids_to_exclude, ]

# Display the first few rows of the new dataframe to verify
head(df2)
```

```{r}
# Convert 'lastpage' to numeric
df2$lastpage <- as.numeric(df2$lastpage)

# Inspect unique values of 'lastpage' after conversion
unique(df2$lastpage)

# Exclude rows where 'lastpage' is smaller than 6
df3 <- df2[df2$lastpage >= 6, ]

# This code updates df3 to keep only the rows where lastpage doesnt contain NA values.

# Check if 'lastpage' column exists in df3
if("lastpage" %in% names(df3)) {
  # If it exists, filter out NA values from 'lastpage'
  df3 <- df3[!is.na(df3$lastpage), ]
} else {
  cat("Column 'lastpage' does not exist in df3\n")
}

```

```{r, echo=FALSE}
# Recode columns 9 to 109 as numeric
df3[, 9:109] <- lapply(df3[, 9:109], as.numeric)

# Check the structure of the first few columns to confirm the change
str(df3[, 9:109])
```

```{r, echo=FALSE}
# Subset df3 to include only columns up to column 119
df3 <- df3[, 1:119]

# Check the structure of the modified dataframe to confirm the change
str(df3)

# Exclude columns 2, 4, 5, 6, 7, 8 and keep only up to column 119
df3 <- df3[, -c(2, 4, 5, 6, 7, 8)]

# Exclude the ATCs
df3 <- df3[, !(names(df3) %in% c("ATC1", "ATC2"))]

```

# checking for the Pattern of Missings
Although it may not be necessary, because the missings are planned, random missings, one could check for the pattern of the missing data. 
```{r}

```

## NAs FIMLn
```{r}
df_dl <- df3[, 3:71]

```

```{r}
library(dplyr)

# Assuming df_fl is your dataframe
# Calculate the number of columns
num_columns <- ncol(df_dl)

# Filter rows where most columns have a value of 0
filtered_df_dl <- df_dl %>%
  filter(rowSums(. == 0) >= num_columns * 0.8)  # Adjust the threshold as needed
```


```{r}
# Load the lavaan package
library(lavaan)
# Model specification
model <- '
# Measurement model
Comprehension =~ F1F1 + F1F2 + F1F3 + F1F4 + F1F5 + F1F6 + F1F7 + F1F8 + F1F9 + F1F10 + F1F11 + F1F12 + F1F13 + F1F14
Evaluation =~ F2F1 + F2F2 + F2F3 + F2F4 + F2F5 + F2F6 + F2F7 + F2F8 + F2F9 + F2F10 + F2F11 + F2F12 + F2F13 + F2F14 + F2F15 + F2F16 + F2F17 + F2F18 + F2F19 + F2F20
Integration =~ F3F1 + F3F2 + F3F3 + F3F4 + F3F5 + F3F6 + F3F7 + F3F8 + F3F9 
Communication =~ F4F1 + F4F2 + F4F3 + F4F4 + F4F5 + F4F6 + F4F7 + F4F8 
Statistics =~ F5F1 + F5F2 + F5F3 + F5F4 + F5F5 + F5F6 + F5F7 + F5F8 + F5F9 + F5F10 + F5F11 + F5F12 + F5F13 + F5F14 + F5F15 + F5F16 + F5F17 + F5F18 

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'

# Fit the model using FIML for missing data
fit <- sem(model, data=df_dl, missing="fiml", em.h1.iter.max=10000)
# fixed.x = F. FIML works by estimating the relationships of the variables with each other and requires estimating the means and variances of the variables. If fixed.x = T (the default), then the variances and covariances are fixed and are based on the existing sample values and are not estimated.
# Summary of the fit
summary(fit)
```

## or using mice?
```{r, echo=FALSE}
imputedData <- mice(df_dl, m=5, method='pmm', maxit=50)
completedData <- complete(imputedData, 1)
```

# Poweranalyse
```{r}
install.packages('semPower')
devtools::install_github('martscht/stuart/stuart', ref = 'develop')
library(semPower)
library(stuart)
library(lavaan)
```
 
# Poweranalyse - SemPower mit Model
```{r}
powerMI <- semPower.powerMI(
  type = 'a-priori',
  comparison = 'configural',
  nullEffect = 'metric',
  nIndicator = c(4, 4, 4, 4, 4), # Corrected to a single vector
  loadM = list(.5, .6), # Assuming baseline loadings for two groups
  tau = list(rep(0.0, 20), # Assuming intercepts for 20 indicators in the first group
             rep(seq(.1, .5, .1), each = 4)), # Assuming intercepts for 20 indicators in the second group, with increments
  alpha = .05, 
  power = .80,
  N = list(.80, .20) # Assuming 80/20 sized groups for split
)
# Show summary
summary(powerMI)

```
## Results Printed
semPower: A priori power analysis
                                     
 F0                        0.033897  
 RMSEA                     0.058221  
 Mc                        0.983194  
                                     
 df                        20        
 Required Num Observations 622       
                           (498, 125)
                                     
 Critical Chi-Square       31.41043  
 NCP                       21.05165  
 Alpha                     0.050000  
 Beta                      0.197753  
 Power (1 - Beta)          0.802247  
 Implied Alpha/Beta Ratio  0.252841  


# Poweranalyse - SemPower ohne Model für Sample Split
```{r}
ap <- semPower.aPriori(effect = .05, effect.measure = 'RMSEA', 
                       alpha = .05, power = .80, df = 155)
summary(ap)
```
semPower: A priori power analysis
                                   
 F0                        0.387500
 RMSEA                     0.050000
 Mc                        0.823864
                                   
 df                        155     
 Required Num Observations 128     
                                   
 Critical Chi-Square       185.0523
 NCP                       49.21250
 Alpha                     0.050000
 Beta                      0.199807
 Power (1 - Beta)          0.800193
 Implied Alpha/Beta Ratio  0.250242


## Descriptive Analyses
```{r}
# packages
library(tidyverse)
library(psych)
library(reshape2)
library(lcsm)

# Create a subset of df3 with columns 104 to 112
demographics <- df3[, 102:111]

```

## Sample
```{r}
# gender (2 = male, female = 1, 3 = divers)
round(prop.table(table(demographics$Gender))*100, 1)

# year of birth
range(demographics$Age, na.rm = TRUE)
round(mean(demographics$Age, na.rm = TRUE), 2)
round(sd(demographics$Age, na.rm = TRUE), 2)

```

```{r}
# prepare subsets for my variables
describe_age <- demographics %>%
  dplyr::select(starts_with("Age")) %>%
  mutate_all(as.numeric)

describe_gender <- demographics %>%
  select(starts_with("Gender")) %>%
  mutate_all(as.numeric)

describe_education <- demographics %>%
  select(starts_with("Education")) %>%
  mutate_all(as.numeric)

describe_degree <- demographics %>%
  select(starts_with("Degree")) %>%
  mutate_all(as.numeric)

describe_workfilter <- demographics %>%
  select(starts_with("Workfilter")) %>%
  mutate(
    Workfilter = case_when(
      Workfilter == "Y" ~ 1,
      Workfilter == "N" ~ 0,
      TRUE ~ NA_integer_  # handle unexpected values, if any
    )
  )

describe_study <- demographics %>%
  select(starts_with("study")) %>%
  mutate(
    study = case_when(
      study == "Y" ~ 1,
      study == "N" ~ 0,
      TRUE ~ NA_integer_  # handle unexpected values, if any
    )
  )

describe_worktime <- demographics %>%
  select(starts_with("Worktime")) %>%
  mutate_all(as.numeric)

describe_occupation <- demographics %>%
  select(starts_with("Occupation")) %>%
  mutate_all(as.numeric)

```

```{r}
# create table for age
table_age <- data.frame(colMeans(is.na(describe_age)))
colnames(table_age) <- "Age"

# create table for Gender
table_gender <- data.frame(colMeans(is.na(describe_gender)))
colnames(table_gender) <- "Gender"

# create table for Education
table_education <- data.frame(colMeans(is.na(describe_education)))
colnames(table_education) <- "Education"

# create table for Studying
table_study <- data.frame(colMeans(is.na(describe_study)))
colnames(table_age) <- "Study"

# create table for Degree
table_degree <- data.frame(colMeans(is.na(describe_degree)))
colnames(table_degree) <- "Degree"

# create table for Working Y/N
table_workfilter <- data.frame(colMeans(is.na(describe_workfilter)))
colnames(table_workfilter) <- "Workfilter"

# create table for Worktime
table_worktime <- data.frame(colMeans(is.na(describe_worktime)))
colnames(table_worktime) <- "Worktime"

# create table for Occupation
table_occupation <- data.frame(colMeans(is.na(describe_occupation)))
colnames(table_occupation) <- "Occupation"

# combine tables
table_missings <- data.frame(table_age, table_gender, table_education, table_study,
                             table_degree, table_wokfilter, table_worktime, table_occupation, check.names = FALSE)
# round
table_missings <- table_missings %>% mutate_if(is.numeric, round, digits = 2) * 100

# add % symbol
table_missings <- table_missings %>% 
  mutate(across(everything(), ~paste0(., "%")))
table_missings # print for HTML

```

```{r}
library(dplyr)
library(psych)
library(rempsyc)
library(flextable)

# Example: Define and round descriptive statistics for each variable
table_descriptive_age <- round(psych::describe(describe_age), 2)
table_descriptive_gender <- round(psych::describe(describe_gender), 2)
table_descriptive_education <- round(psych::describe(describe_education), 2)
table_descriptive_study <- round(psych::describe(describe_study), 2)
table_descriptive_degree <- round(psych::describe(describe_degree), 2)
table_descriptive_workfilter <- round(psych::describe(describe_workfilter), 2)
table_descriptive_worktime <- round(psych::describe(describe_worktime), 2)
table_descriptive_occupation <- round(psych::describe(describe_occupation), 2)

# Combine all tables into a single data frame row-wise
table_descriptives <- bind_rows(
  table_descriptive_age,
  table_descriptive_gender,
  table_descriptive_education,
  table_descriptive_study,
  table_descriptive_degree,
  table_descriptive_workfilter,
  table_descriptive_worktime,
  table_descriptive_occupation
)

print(table_descriptives)

# Convert row names to a column in your data frame
table_descriptives <- table_descriptives %>%
  rownames_to_column(var = "Variables")

# Combine columns 3 to 5 and column 23 for selection
selected_columns <- c(1,4:6, 14)

# Use the combined columns in the nice_table function
nice_table(table_descriptives[selected_columns], stars = TRUE, title = "Sample Descriptives", note = "Descriptive Statistics on the demographic variables. sd = Standard deviation; se = Standard error")

```

```{r}
library(dplyr)

# Calculate the mean, SD, and frequencies for age by gender
mean_sd_age_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_age = mean(Age, na.rm = TRUE),
    sd_age = sd(Age, na.rm = TRUE),
    count_age = n()
  )

# Calculate the mean, SD, and frequencies for education by gender
mean_sd_education_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_education = mean(Education, na.rm = TRUE),
    sd_education = sd(Education, na.rm = TRUE),
    count_education = n()
  )

# Calculate the mean, SD, and frequencies for degree by gender
mean_sd_degree_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_degree = mean(Degree, na.rm = TRUE),
    sd_degree = sd(Degree, na.rm = TRUE),
    count_degree = n()
  )

# Calculate the mean, SD, and frequencies for worktime by gender
mean_sd_worktime_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_worktime = mean(Worktime, na.rm = TRUE),
    sd_worktime = sd(Worktime, na.rm = TRUE),
    count_worktime = n()
  )

# Calculate the mean, SD, and frequencies for occupation by gender
mean_sd_occupation_by_gender <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_occupation = mean(Occupation, na.rm = TRUE),
    sd_occupation = sd(Occupation, na.rm = TRUE),
    count_occupation = n()
  )


# Print the resulting data frames
print(mean_sd_age_by_gender)
print(mean_sd_education_by_gender)
print(mean_sd_degree_by_gender)
print(mean_sd_worktime_by_gender)
print(mean_sd_occupation_by_gender)
```

```{r}
# Load the necessary library
library(dplyr)

# Assuming demographics is your data frame and it has the relevant columns

# Calculate the mean, SD, and frequencies for age by gender
table2_descriptive_age <- demographics %>%
  group_by(Gender) %>%
  summarize(
    mean_age = mean(Age, na.rm = TRUE),
    sd_age = sd(Age, na.rm = TRUE),
    count_age = n()
  )

# Calculate the mean, SD, and frequencies for education by gender
table2_descriptive_education <- demographics %>%
  group_by(Gender, Education) %>%
  summarize(
    mean_education = mean(Education, na.rm = TRUE),
    sd_education = sd(Education, na.rm = TRUE),
    count_education = n()
  ) %>%
  ungroup()

# Calculate the mean, SD, and frequencies for degree by gender
table2_descriptive_degree <- demographics %>%
  group_by(Gender, Degree) %>%
  summarize(
    mean_degree = mean(Degree, na.rm = TRUE),
    sd_degree = sd(Degree, na.rm = TRUE),
    count_degree = n()
  ) %>%
  ungroup()

# Calculate the mean, SD, and frequencies for worktime by gender
table2_descriptive_worktime <- demographics %>%
  group_by(Gender, Worktime) %>%
  summarize(
    mean_worktime = mean(Worktime, na.rm = TRUE),
    sd_worktime = sd(Worktime, na.rm = TRUE),
    count_worktime = n()
  ) %>%
  ungroup()

# Calculate the mean, SD, and frequencies for occupation by gender
table2_descriptive_occupation <- demographics %>%
  group_by(Gender, Occupation) %>%
  summarize(
    mean_occupation = mean(Occupation, na.rm = TRUE),
    sd_occupation = sd(Occupation, na.rm = TRUE),
    count_occupation = n()
  ) %>%
  ungroup()

# Print the resulting data frames
print(table2_descriptive_age)
print(table2_descriptive_education)
print(table2_descriptive_degree)
print(table2_descriptive_worktime)
print(table2_descriptive_occupation)
```
```{r}
# Load the necessary libraries
library(dplyr)
library(tidyr)

# Assuming demographics is your data frame and it has the relevant columns

# Function to calculate frequencies and percentages
calculate_frequencies <- function(data, variable) {
  data %>%
    mutate(Gender = as.factor(Gender), !!sym(variable) := as.factor(!!sym(variable))) %>%
    group_by(Gender, !!sym(variable)) %>%
    summarize(count = n(), .groups = 'drop') %>%
    ungroup() %>%
    group_by(Gender) %>%
    mutate(percentage = count / sum(count) * 100) %>%
    ungroup() %>%
    complete(Gender, !!sym(variable), fill = list(count = 0, percentage = 0)) %>%
    mutate(Gender = as.character(Gender)) %>%
    bind_rows(
      data %>%
        mutate(!!sym(variable) := as.factor(!!sym(variable))) %>%
        group_by(!!sym(variable)) %>%
        summarize(count = n(), .groups = 'drop') %>%
        mutate(Gender = "All") %>%
        mutate(percentage = count / sum(count) * 100)
    ) %>%
    arrange(Gender, !!sym(variable))
}

# Calculate frequencies and percentages for study
table_descriptive_study <- calculate_frequencies(demographics, "study")

# Calculate frequencies and percentages for workfilter
table_descriptive_workfilter <- calculate_frequencies(demographics, "Workfilter")

# Calculate frequencies and percentages for worktime
table_descriptive_worktime <- calculate_frequencies(demographics, "Worktime")

# Calculate frequencies and percentages for occupation
table_descriptive_occupation <- calculate_frequencies(demographics, "Occupation")

# Calculate frequencies and percentages for education
table_descriptive_education <- calculate_frequencies(demographics, "Education")

# Print the resulting data frames N= 540
print(table_descriptive_study) #136 25,19% yes; 74,81 no
print(table_descriptive_workfilter) #13,52% no; 86,48 yes +33women no 
print(table_descriptive_worktime) #N=467;64,88% VZ, 35,12TZ; of 228 women, 51,75% vollzeit , 48,25% teilzeit ; of men 238, 77,73%VZ, 22,27%tz
print(table_descriptive_occupation)
print(table_descriptive_education)
```

## Heatmap
```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Example: Assuming your data frame is demographics

# Ensure variables are numeric
demographics <- demographics %>%
  mutate(
    study = ifelse(study == "Y", 1, 0),  # Convert Y/N to 1/0
    Workfilter = ifelse(Workfilter == "Y", 1, 0)  # Convert Y/N to 1/0
    # Add more conversions as needed for other variables
  ) %>%
  mutate(
    Age = as.numeric(Age),
    Gender = as.numeric(Gender),  # Assuming Gender is already binary
    Education = as.numeric(Education),
    Degree = as.numeric(Degree),
    Worktime = as.numeric(Worktime),
    Occupation = as.numeric(Occupation)
  )

# Select variables for correlation analysis
selected_vars <- demographics %>%
  select(Gender, Age, Education, study, Degree, Workfilter, Worktime, Occupation)

# Rename variables for clarity in heatmap
colnames(selected_vars) <- c(
  "Gender", "Age", "Education", "study", "Degree", "Workfilter", "Worktime", "Occupation"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final <- heatmap + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final)
```

```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Assuming your data frame is df_dl
# Ensure all variables are numeric
df_dl <- df_dl %>%
  mutate(across(everything(), as.numeric))

# Select variables for correlation analysis
selected_vars <- df_dl %>%
  select(everything())

colnames(selected_vars) <- c(
  "F1F1",  "F1F2",  "F1F3",  "F1F4",  "F1F5",  "F1F6",  "F1F7",  "F1F8",  "F1F9",  "F1F10",
  "F1F11", "F1F12", "F1F13", "F1F14", "F2F1",  "F2F2",  "F2F3",  "F2F4",  "F2F5",  "F2F6",
  "F2F7",  "F2F8",  "F2F9",  "F2F10", "F2F11", "F2F12", "F2F13", "F2F14", "F2F15", "F2F16",
  "F2F17", "F2F18", "F2F19", "F2F20", "F3F1",  "F3F2",  "F3F3",  "F3F4",  "F3F5",  "F3F6",
  "F3F7",  "F3F8",  "F3F9",  "F4F1",  "F4F2",  "F4F3",  "F4F4",  "F4F5",  "F4F6",  "F4F7",
  "F4F8",  "F5F1",  "F5F2",  "F5F3",  "F5F4",  "F5F5",  "F5F6",  "F5F7",  "F5F8",  "F5F9",
  "F5F10", "F5F11", "F5F12", "F5F13", "F5F14", "F5F15", "F5F16", "F5F17", "F5F18"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap2 <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final2 <- heatmap2 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final2)
print(heatmap2)
```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(scales) # For pretty_breaks

# Adjust the ggplot call to exclude NAs only from the figure
ggplot(describe_occupation %>% filter(!is.na(Occupation)), aes(x = factor(Occupation))) +
  geom_bar(fill = "blue", color = "black", alpha = 0.7) +
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5) + # Add count labels
  labs(title = "Histogram of Occupations",
       x = "Occupation",
       y = "Frequency") +
  scale_x_discrete(labels = function(x) {
    labels <- c(
      "1" = "Land-, Forst- und Tierwirtschaft",
      "2" = "Rohstoffgewinnung, Produktion und Fertigung",
      "3" = "Bau, Architektur, Gebäudetechnik",
      "4" = "Naturwissenschaft, Geografie und Informatik",
      "5" = "Verkehr, Logistik, Schutz und Sicherheit",
      "6" = "Kaufmännische Dienstleistungen, Vertrieb, Tourismus",
      "7" = "Buchhaltung, Recht und Verwaltung",
      "8" = "Gesundheit, Soziales, Lehre und Erziehung",
      "9" = "Geistes-, Gesellschafts- und Wirtschaftswissenschaften, Medien, Kunst, Kultur und Gestaltung",
      "10" = "Militär"
    )
    wrapped_labels <- lapply(labels[x], function(label) paste(strwrap(label, width = 20), collapse = "\n"))
    unlist(wrapped_labels) # Ensure the output is a vector, not a list
  }) +
  scale_y_continuous(breaks = pretty_breaks(n = 10)) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 9, angle = 55, hjust = 1, vjust = 1),
    axis.text.y = element_text(size = 9),
    plot.title = element_text(size = 10),
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)
  )
```
```{r}
library(dplyr)
library(tibble) # For creating a tibble if needed

# Assuming describe_occupation is your dataframe
# First, define the labels as a named vector
occupation_labels <- c(
  "1" = "Land-, Forst- und Tierwirtschaft",
  "2" = "Rohstoffgewinnung, Produktion und Fertigung",
  "3" = "Bau, Architektur, Gebäudetechnik",
  "4" = "Naturwissenschaft, Geografie und Informatik",
  "5" = "Verkehr, Logistik, Schutz und Sicherheit",
  "6" = "Kaufmännische Dienstleistungen, Vertrieb, Tourismus",
  "7" = "Buchhaltung, Recht und Verwaltung",
  "8" = "Gesundheit, Soziales, Lehre und Erziehung",
  "9" = "Geistes-, Gesellschafts- und Wirtschaftswissenschaften, Medien, Kunst, Kultur und Gestaltung",
  "10" = "Militär"
)

# Convert Occupation to factor with labels and count
frequency_table_occ <- describe_occupation %>%
  filter(!is.na(Occupation)) %>%
  mutate(Occupation = factor(Occupation, labels = occupation_labels[names(occupation_labels)])) %>%
  count(Occupation) %>%
  arrange(desc(n)) # Arrange by frequency, can be removed if not needed

# Print the frequency table
print(frequency_table)
```

## Filter for the partial completes
```{r}
# Splitting df3 into two datasets based on the "lastpage" variable
df4 <- subset(df3, lastpage == 13)
df_partial <- subset(df3, lastpage != 13)

```

# Stuart - Item selection
## Sample split
```{r}
library(stuart)
#, 0.4525
split <- holdout(df4, prop = 0.5475, grouping = NULL, seed = 2024, determined = NULL)
lapply(split, nrow) #check size of sample

# Accessing the split datasets
df_train <- split$calibrate
df_test <- split$validate

# Remove the 'determined' column by setting it to NULL
df_train$determined <- NULL
df_test$determined <- NULL

# adding the partially completed ones to the df train
df_train <- rbind(df_train, df_partial)

# exclude everything exept the dl items
df_train <- df_train[,3:71]

df_test_dl <- df_test[,3:71]
```

## using mice 
```{r, echo=FALSE}
seed(2025)
imputed_training_data <- mice(df_train, m=5, method='pmm', maxit=50)
imputed_training_data_c <- complete(imputed_training_data, 1)

imputed_test_data <- mice(df_test_dl, m=5, method='pmm', maxit=50)
imputed_test_data_c <- complete(imputed_test_data, 1)
```

```{r}
write.csv2(imputed_training_data_c, "imputed_training_data_c.csv")
write.csv2(df_train, "df_train.csv")
```

# Count values for each variable
```{r}
library(dplyr)
library(tidyr)

# Count values for each variable in imputed_training_data_c
value_counts <- imputed_training_data_c %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts)

# Filter for cases with values smaller or equal to 2
filtered_value_counts <- value_counts %>%
  filter(rowSums(. <= 2) > 0)

print(filtered_value_counts)

# Get the list of items to be modified
items_to_modify <- filtered_value_counts$Variable

# Ensure items_to_modify is a character vector
items_to_modify <- as.character(items_to_modify)

# Modify the imputed_training_data_c to merge answers 0 and 1 into 1 for these items
imputed_training_data_merged <- imputed_training_data_c %>%
  mutate(across(all_of(items_to_modify), ~ ifelse(. %in% c(0, 1), 1, .)))

imputed_training_data_merged <- imputed_training_data_merged %>%
  select(-c(F2F17, F2F4))

print(imputed_training_data_merged)


```

```{r}
value_counts_merged <- imputed_training_data_merged %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


print(value_counts_merged)
```
```{r}
library(dplyr)
library(tidyr)

# Define the selected items
selected_items10 <- c("F1F12", "F2F10", "F3F1", "F3F4", "F4F8", "F5F1", "F5F16", "F5F9", "F5F5", "F5F13", "F4F6")

# Modify the imputed_training_data_c to merge answers 1 and 2 into 2 for these selected items
imputed_training_data_merged1 <- imputed_training_data_merged %>%
  mutate(across(all_of(selected_items10), ~ ifelse(. %in% c(0, 1), 1, .)))

print(imputed_training_data_merged1)

```


```{r}
library(dplyr)
library(tidyr)

# Define the selected items
selected_items <- c("F1F2", "F1F4", "F1F5", "F1F6", "F1F7", "F1F11", "F1F3", "F2F1", "F2F9", "F2F8", "F2F12", "F2F13", "F2F7", "F3F7", "F3F8" ,"F3F9", "F4F2", "F4F5", "F5F17", "F5F15", "F1F14", "F1F13")

# Modify the imputed_training_data_c to merge answers 1 and 2 into 2 for these selected items
imputed_training_data_merged2 <- imputed_training_data_merged1 %>%
  mutate(across(all_of(selected_items), ~ ifelse(. %in% c(1, 2), 2, .)))

print(imputed_training_data_merged2)

```

```{r}
value_counts_merged2 <- imputed_training_data_merged2 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


print(value_counts_merged2)
```

```{r}
library(dplyr)
library(tidyr)

# Define the selected items
selected_items23 <- c("F2F7", "F2F13", "F2F12", "F2F1")

# Modify the imputed_training_data_c to merge answers 1 and 2 into 2 for these selected items
imputed_training_data_merged3 <- imputed_training_data_merged2 %>%
  mutate(across(all_of(selected_items23), ~ ifelse(. %in% c(2, 3), 3, .)))

print(imputed_training_data_merged3)
```

```{r}
value_counts_merged3 <- imputed_training_data_merged3 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))


print(value_counts_merged3)
```

## The same for test data
```{r}
library(dplyr)
library(tidyr)

# Count values for each variable in imputed_test_data_c
value_counts_test <- imputed_test_data_c %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts_test)
```

```{r}
# Filter for cases with values smaller or equal to 2
filtered_value_counts_test <- value_counts_test %>%
  filter(rowSums(. <= 2) > 0)

print(filtered_value_counts_test)

# Get the list of items to be modified
items_to_modify_test <- filtered_value_counts_test$Variable

# Ensure items_to_modify_test is a character vector
items_to_modify_test <- as.character(items_to_modify_test)

# Modify the imputed_test_data_c to merge answers 0 and 1 into 1 for these items
imputed_test_data_merged <- imputed_test_data_c %>%
  mutate(across(all_of(items_to_modify_test), ~ ifelse(. %in% c(0, 1), 1, .)))

imputed_test_data_merged <- imputed_test_data_merged %>%
  select(-c(F2F17, F2F4))

print(imputed_test_data_merged)
```

```{r}
# Count values for each variable in the merged test data
value_counts_merged_test <- imputed_test_data_merged %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts_merged_test)

# Define the selected items
selected_items10 <- c("F1F12", "F2F10", "F3F1", "F3F4", "F4F8", "F5F1", "F5F16", "F5F9", "F5F5", "F5F13", "F4F6")

# Modify the imputed_test_data_merged to merge answers 1 and 2 into 2 for these selected items
imputed_test_data_merged1 <- imputed_test_data_merged %>%
  mutate(across(all_of(selected_items10), ~ ifelse(. %in% c(0, 1), 1, .)))

print(imputed_test_data_merged1)
```

```{r}
# Define the selected items
selected_items <- c("F1F2", "F1F4", "F1F5", "F1F6", "F1F7", "F1F11", "F1F3", "F2F1", "F2F9", "F2F8", "F2F12", "F2F13", "F2F7", "F3F7", "F3F8" ,"F3F9", "F4F2", "F4F5", "F5F17", "F5F15", "F1F14", "F1F13")

# Modify the imputed_test_data_merged1 to merge answers 1 and 2 into 2 for these selected items
imputed_test_data_merged2 <- imputed_test_data_merged1 %>%
  mutate(across(all_of(selected_items), ~ ifelse(. %in% c(1, 2), 2, .)))

print(imputed_test_data_merged2)

# Count values for each variable in the merged test data
value_counts_merged2_test <- imputed_test_data_merged2 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts_merged2_test)
```

```{r}
# Define the selected items
selected_items23 <- c("F2F7", "F2F13", "F2F12", "F2F1")

# Modify the imputed_test_data_merged2 to merge answers 1 and 2 into 2 for these selected items
imputed_test_data_merged3 <- imputed_test_data_merged2 %>%
  mutate(across(all_of(selected_items23), ~ ifelse(. %in% c(2, 3), 3, .)))

print(imputed_test_data_merged3)
```

```{r}
# Count values for each variable in the merged test data
value_counts_merged3_test <- imputed_test_data_merged3 %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  count(Variable, Value) %>%
  pivot_wider(names_from = Value, values_from = n, values_fill = list(n = 0))

print(value_counts_merged3_test)
```

```{r}
names(imputed_training_data_merged3)

#F2F17 and F2F4 deleted
fs<-list(Comprehension=c("F1F1",  "F1F2",  "F1F3",  "F1F4",  "F1F5",  "F1F6",  "F1F7",  "F1F8",  "F1F9",  "F1F10", "F1F11",                              "F1F12", "F1F13", "F1F14"),
           Evaluation=c('F2F1',"F2F2",  "F2F3",  "F2F5",  "F2F6",  "F2F7",  "F2F8",  "F2F9",  "F2F10", "F2F11", "F2F12",                                "F2F13", "F2F14", "F2F15", "F2F16", "F2F18",'F2F19','F2F20'),
           Integration=c("F3F1",  "F3F2",  "F3F3",  "F3F4",  "F3F5",  "F3F6",  "F3F7", "F3F8",  "F3F9"),
           Communication=c('F4F1','F4F2','F4F3','F4F4','F4F5','F4F6','F4F7','F4F8'),
           Statistics=c("F5F1",  "F5F2",  "F5F3",  "F5F4",  "F5F5",  "F5F6",  "F5F7", "F5F8",  "F5F9",  "F5F10",                                       "F5F11","F5F12", "F5F13", "F5F14", "F5F15", "F5F16", "F5F17", "F5F18")
                           )
```


## ordinal data transformation?
```{r}
ords <- imputed_training_data_merged3[, names(imputed_training_data_merged3)%in%unlist(fs)]
ords <- lapply(ords, as.ordered)
ords <- do.call(data.frame, ords)

ords2 <- imputed_test_data_merged3[, names(imputed_test_data_merged3)%in%unlist(fs)]
ords2 <- lapply(ords2, as.ordered)
ords2 <- do.call(data.frame, ords2)

#for (col in names(ords)) {
#  if (is.factor(ords[[col]]) && is.factor(ords2[[col]])) {
#    levels_combined <- union(levels(ords[[col]]), levels(ords2[[col]]))
#    ords[[col]] <- factor(ords[[col]], levels = levels_combined, ordered = TRUE)
#    ords2[[col]] <- factor(ords2[[col]], levels = levels_combined, ordered = TRUE)
#  }
#}

# Add the 'split' column to each data frame
ords <- ords %>%
  mutate(split = 1)

ords2 <- ords2 %>%
  mutate(split = 2)

# Combine the two data frames
ords_c <- bind_rows(ords, ords2)

# Print the resulting data frame
print(ords_c)
```

```{r}
model <- '
# Measurement model
Comprehension =~ F1F1 + F1F2 + F1F3 + F1F4 + F1F5 + F1F6 + F1F7 + F1F8 + F1F9 + F1F10 + F1F11 + F1F12 + F1F13 + F1F14
Evaluation =~ F2F1 + F2F2 + F2F3 + F2F5 + F2F6 + F2F7 + F2F8 + F2F9 + F2F10 + F2F11 + F2F12 + F2F13 + F2F14 + F2F15 + F2F16 + F2F18 + F2F19 + F2F20
Integration =~ F3F1 + F3F2 + F3F3 + F3F4 + F3F5 + F3F6 + F3F7 + F3F8 + F3F9 
Communication =~ F4F1 + F4F2 + F4F3 + F4F4 + F4F5 + F4F6 + F4F7 + F4F8 
Statistics =~ F5F1 + F5F2 + F5F3 + F5F4 + F5F5 + F5F6 + F5F7 + F5F8 + F5F9 + F5F10 + F5F11 + F5F12 + F5F13 + F5F14 + F5F15 + F5F16 + F5F17 + F5F18 

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'
fit_again <- cfa(model, ords, ordered = TRUE, estimator = "WLSMV")
summary(fit_again)
lavInspect(fit_again, "est")$tau
```

## Objective-Function
```{r}
library(stuart)
library(lavaan)

#extracting the taus
taus <- lavInspect(fit_again, "est")$tau
# printing them
print(taus)

## taus?
sd_tau <- sd(taus[grep('t1', rownames(taus))]) + 
                 sd(taus[grep('t2', rownames(taus))]) +
                 sd(taus[grep('t3', rownames(taus))]) + 
                 sd(taus[grep('t4', rownames(taus))]) +
                 sd(taus[grep('t5', rownames(taus))])

print(theothrsh_tau)#2.194

# Assuming tau is a vector containing all tau values across items
overall_sd_tau <- sd(taus)
print(overall_sd_tau)# 1.136692

#ordinal data
objective.normal <- function(rmsea.scaled, srmr, cfi.scaled) {
  out1 = 0.5-(0.5/(1 + exp(- 100 * (rmsea.scaled-.05))))
  out2 = 0.5-(0.5/(1 + exp(- 100 * (srmr-.05))))
  out3 = (1/(1 + exp(- 100 * (cfi.scaled-.95))))
  out = (out1 + out2 + out3)/3
  return(out)                                
}

obj <- function(chisq, df, pvalue, rmsea.robust, srmr, cfi.robust, crel, tau) {
  out1 = 0.5 - (0.5 / (1 + exp(-100 * (rmsea.robust - .05))))
  out2 = 0.5 - (0.5 / (1 + exp(-100 * (srmr - .05))))
  out3 = (1 / (1 + exp(-100 * (cfi.robust - .95))))
  # Calculating the standard deviation of tau
  sd_tau = sd(tau)
  
  # Adjusting out4 to maximize the standard deviation of tau
  out4 = 1 / (1 + exp(-5 * (sd_tau - 0.5))) 
  
  out = (out1 + out2 + out3 + out4) / 4
  return(out)

}

sd(tau)

#stuart:::objective.preset
obj_default <- function(chisq, df, pvalue, rmsea.robust, srmr, crel, nu) {
                      1/(1 + exp(-10 * (crel - 0.6))) + 0.5 * (1 - (1/(1 + exp(-100 * 
                     (rmsea.robust - 0.05))))) + 0.5 * (1 - (1/(1 + exp(-100 * (srmr - 
                      0.06)))))
}
#1 / (1 + exp(-10 * (nu_sd - 0.1))) #0.1 is the threshhold and can be adjusted

obj_default3 <- function(chisq, df, pvalue, rmsea, srmr, cfi, crel, nu)
                        {(1-rmsea)+(1-srmr) + sd(nu)}

#(sd(nu) - 1.62)^2 + 1.62^2 + 1e-6

fixedobjective(
  criteria = c("rmsea.robust", "srmr", "crel"),
  add = c("chisq", "df", "pvalue"),
  side = NULL,
  scale = 1,
  matrices = 'nu',
  fixed = NULL,
  comparisons = NULL
)

```


```{r}
combinations(imputed_training_data_c, fs,4)
```
## Genetic Algorithm
### Kfold-Crossvalidation
```{r}

############ k-Folding #########################

# Call the kfold function with factor.structure as an argument
kfold_sel <- kfold(
  'gene', 
  k = 3, 
  data = imputed_training_data_c, 
  factor.structure = fs,  # Pass the variable directly
  max.invariance = "strict", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2024,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 200,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
#########################################################
```

```{r}

summary(kfold_sel)

inspect(kfold_sel$final, 'est')
inspect(kfold_sel$final, 'fit')

lavaan::summary(kfold_sel$final, standardized = TRUE) 
lavaan::inspect(kfold_sel$final, 'est')$A$nu
lavaan::inspect(kfold_sel$final, 'est')$B$nu
```
- First Time it worked:
SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 61760 
Time Required: 1175.532 seconds

Crossvalidation Results with STRICT Invariance:

Average Jaccard Similarity: Comprehension: 0.422, Evaluation: 0.295, Integration: 0.295, Communication: 0.600, Statistics: 0.359

Constructed Subtests: (k = 1)
Comprehension: F1F2 F1F6 F1F8 F1F10
Evaluation: F2F2 F2F4 F2F14 F2F15
Integration: F3F3 F3F4 F3F5 F3F9
Communication: F4F1 F4F3 F4F6 F4F7
Statistics: F5F3 F5F4 F5F15 F5F18


## First time with robust indices:
SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 85824 
Time Required: 1625.887 seconds

Crossvalidation Results with STRICT Invariance:

Average Jaccard Similarity: Comprehension: 0.270, Evaluation: 0.270, Integration: 0.422, Communication: 0.733, Statistics: 0.733

Constructed Subtests: (k = 3)
Comprehension: F1F5 F1F6 F1F11 F1F12
Evaluation: F2F6 F2F7 F2F12 F2F17
Integration: F3F2 F3F3 F3F4 F3F9
Communication: F4F3 F4F6 F4F7 F4F8
Statistics: F5F6 F5F7 F5F17 F5F18
```{r}
obj_default_nu <- function(chisq, df, pvalue, rmsea.robust, srmr, crel, nu) {
  # Ensure nu is an atomic vector
  if (!is.atomic(nu)) {
    nu <- unlist(nu)
  }
  
  # Calculate the standard deviation of nu
  nu_sd <- sd(nu)
  
  # Compute the objective function
  1/(1 + exp(-10 * (crel - 0.6))) + 
  0.5 * (1 - (1/(1 + exp(-100 * (rmsea.robust - 0.05))))) + 
  0.5 * (1 - (1/(1 + exp(-100 * (srmr - 0.06))))) + 
  1 / (1 + exp(-10 * (nu_sd - 0.1))) # Adjust the threshold as needed
}
```

```{r}
kfold_sel_nu <- kfold(
  'gene', 
  k = 3, 
  data = imputed_training_data_c, 
  factor.structure = fs,  # Pass the variable directly
  max.invariance = "strict", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2024,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = obj_default_nu,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 200,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
#########################################################
```

```{r}
summary(kfold_sel_nu)

inspect(kfold_sel_nu$final, 'est')
inspect(kfold_sel_nu$final, 'fit') # better fit than without nu

lavaan::summary(kfold_sel_nu$final, standardized = TRUE) 
lavaan::inspect(kfold_sel_nu$final, 'est')$A$nu
lavaan::inspect(kfold_sel_nu$final, 'est')$B$nu
```

SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 36788 
Time Required: 467.855 seconds

Crossvalidation Results with STRICT Invariance:

Average Jaccard Similarity: Comprehension: 0.206, Evaluation: 0.143, Integration: 0.422, Communication: 0.511, Statistics: 0.200

Constructed Subtests: (k = 3)
Comprehension: F1F1 F1F5 F1F6 F1F11
Evaluation: F2F2 F2F5 F2F6 F2F12
Integration: F3F2 F3F3 F3F4 F3F9
Communication: F4F3 F4F6 F4F7 F4F8
Statistics: F5F6 F5F7 F5F17 F5F18

## Crossvalidation
```{r}
# Load the necessary library
library(dplyr)

# Assuming imputed_training_data_c and imputed_test_data_c are your data frames

# Add the 'split' column to each data frame
imputed_training_data_c <- imputed_training_data_c %>%
  mutate(split = 1)

imputed_test_data_c <- imputed_test_data_c %>%
  mutate(split = 2)

# Join the two data frames
joined_mgcfa_data <- bind_rows(imputed_training_data_c, imputed_test_data_c)

# Print the resulting data frame
print(joined_mgcfa_data)
```


```{r}
library(lavaan)
#stuart::crossvalidate(kfold_sel_nu, imputed_training_data_c, imputed_test_data_c, filename = NULL )

cfa_model_m <- '
# Measurement model
Comprehension =~ F1F1 + F1F5 + F1F6 + F1F11 
Evaluation =~ F2F2 + F2F5 + F2F6 + F2F12 
Integration =~ F3F2 + F3F3 + F3F4 + F3F9 
Communication =~ F4F3 + F4F6 + F4F7 + F4F8 
Statistics =~ F5F6 + F5F7 + F5F17 + F5F18 

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'
fit_cfa_model_m <- cfa(cfa_model_m, joined_mgcfa_data, estimator = "MLR", group = "split" )
summary(fit_cfa_model_m, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_m, 'fit')
```

```{r}
library(lavaan)
library(semTools)
fit_cfa_model_metric <- cfa(cfa_model_m, joined_mgcfa_data, estimator = "MLR", group = "split", group.equal= "loadings")


# We can compare models using compareFit

comp_fit_metric <- compareFit(fit_cfa_model_m, fit_cfa_model_metric)
summary(comp_fit_metric)

```
## configural to metric
Cfi delta: .016 #fail
rmsea delta: .001 #check
srmr delta: .009 #check

```{r}
library(lavaan)
library(semTools)

fit_cfa_model_scalar <- cfa(cfa_model_m, joined_mgcfa_data, estimator = "MLR", group = "split",
                            group.equal = c('loadings','intercepts'))


# We can compare models using compareFit

comp_fit_scalar <- compareFit(fit_cfa_model_metric,fit_cfa_model_scalar)
summary(comp_fit_scalar)

```
## metric to scalar
Cfi delta: .003 #check
rmsea delta: .002 #check
srmr delta: .001 #check
```{r}
library(lavaan)
library(semTools)

fit_cfa_model_strict <- cfa(cfa_model_m, joined_mgcfa_data, estimator = "MLR", group = "split",
                            group.equal = c('loadings','intercepts','residuals'))


# We can compare models using compareFit

comp_fit_strict <- compareFit(fit_cfa_model_scalar, fit_cfa_model_strict)
summary(comp_fit_strict)

```
## scalar to strict
Cfi delta: .015 #fail
rmsea delta: 0 #check
srmr delta: .006 #check

################################################
################################################
## Ordinal data Structure
```{r, eval=FALSE}
# Call the kfold function with factor.structure as an argument
kfold_sel2 <- kfold(
  'gene', 
  k = 3, 
  data = ords, 
  factor.structure = fs,  
  max.invariance = "strong", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2026,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = objective.normal,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
#########################################################
```

```{r}
summary(kfold_sel2)

inspect(kfold_sel2$final, 'est')
inspect(kfold_sel2$final, 'fit')
lavaan::summary(kfold_sel2$final, standardized = TRUE) 
lavaan::inspect(kfold_sel2$final, 'est')$B$tau

fit_kfold_sel2 <- cfa(model, ords, ordered = TRUE, estimator = "WLSMV")
summary(fit_kfold_sel2)
```

## OMG first time ordinal ran (w.o. tau)
Warning: This is a beta-build of stuart. Please report any bugs you encounter.

SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 52088 
Time Required: 1096.398 seconds

Crossvalidation Results with STRONG Invariance:

Average Jaccard Similarity: Comprehension: 0.600, Evaluation: 0.333, Integration: 0.600, Communication: 0.600, Statistics: 0.333

Constructed Subtests: (k = 1)
Comprehension: F1F3 F1F6 F1F12 F1F14
Evaluation: F2F5 F2F12 F2F14 F2F16
Integration: F3F3 F3F4 F3F7 F3F8
Communication: F4F1 F4F2 F4F3 F4F8
Statistics: F5F3 F5F6 F5F15 F5F18

```{r}
library(lavaan)

cfa_model_ord <- '
# Measurement model
Comprehension =~ F1F3 + F1F6 + F1F12 + F1F14 
Evaluation =~ F2F5 + F2F12 + F2F14 + F2F16 
Integration =~ F3F3 + F3F4 + F3F7 + F3F8 
Communication =~ F4F1 + F4F2 + F4F3 + F4F8 
Statistics =~ F5F3 + F5F6 + F5F15 + F5F18 

# Factor variances
Comprehension ~~ Comprehension
Evaluation ~~ Evaluation
Integration ~~ Integration
Communication ~~ Communication
Statistics ~~ Statistics

# Factor correlations
Comprehension ~~ Evaluation
Comprehension ~~ Integration
Comprehension ~~ Communication
Comprehension ~~ Statistics
Evaluation ~~ Integration
Evaluation ~~ Communication
Evaluation ~~ Statistics
Integration ~~ Communication
Integration ~~ Statistics
Communication ~~ Statistics
'
fit_cfa_model_ords <- cfa(cfa_model_ord, ords_c, ordered = TRUE, estimator = "WLSMV", group = "split")
summary(fit_cfa_model_ords, fit.measures = TRUE, standardized = TRUE)

inspect(fit_cfa_model_ords, 'fit')
```

```{r}
library(lavaan)

# Assuming 'fit_again' is your fitted lavaan model object
# Example: fit_again <- cfa(model, data = your_data)

# Extract the tau matrix from the fitted model
tau <- lavInspect(fit_again, "est")$tau

# Extract the item names by splitting the row names at the "|" character
item_names <- sapply(strsplit(rownames(tau), "\\|"), `[`, 1)

# Get unique item names
unique_items <- unique(item_names)

# Initialize an empty matrix to store the results
taus <- matrix(NA, nrow = length(unique_items), ncol = 2)
colnames(taus) <- c("Item", "Avg_SD")

# Loop through each unique item
for (i in seq_along(unique_items)) {
  item <- unique_items[i]
  
  # Extract the thresholds for the item
  thresholds <- tau[item_names == item, , drop = FALSE]
  
  # Calculate the standard deviation for the thresholds
  sd_values <- sd(thresholds, na.rm = TRUE)
  
  # Store the item name and average standard deviation in the matrix
  taus[i, ] <- c(item, sd_values)
}

# Convert the matrix to a data frame for better readability
taus <- as.data.frame(taus)
taus$Avg_SD <- as.numeric(as.character(taus$Avg_SD))

# Print the resulting matrix
print(taus)

# Create a new matrix with Avg_SD values and row names as Item names
taus_matrix <- matrix(taus$Avg_SD, nrow = length(unique_items), ncol = 1)
rownames(taus_matrix) <- taus$Item
colnames(taus_matrix) <- "Avg_SD"

# Print the resulting matrix
print(taus_matrix)
```


```{r, eval=FALSE}
library(stuart)
# objective
# Define the objective function
#
objective.normal2 <- function(rmsea.scaled, srmr, cfi.scaled, tau) {

  if (!is.numeric(tau)) {
    tau <- as.numeric(tau)
  }
  # Compute the components of the objective function
  out1 <- 0.5 - (0.5 / (1 + exp(-100 * (rmsea.scaled - 0.05))))
  out2 <- 0.5 - (0.5 / (1 + exp(-100 * (srmr - 0.05))))
  out3 <- 1 / (1 + exp(-100 * (cfi.scaled - 0.95)))
  out4 <- sd(tau)  
  
  # Combine the components into the final objective function
  out <- (out1 + out2 + out3 + out4) / 4
  return(out)
}

kfold_sel3 <- kfold(
  'gene', 
  k = 3, 
  data = ords, 
  factor.structure = fs,  
  max.invariance = "strong", 
  capacity = list(4, 4, 4, 4, 4), 
  seed = 2026,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = objective.normal2,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 300,
  individuals = 30,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
# Inspect the structure of kfold_sel2$final
tau_values3a <- lavaan::inspect(kfold_sel3$final, 'est')$A$tau
tau_values3b <- lavaan::inspect(kfold_sel3$final, 'est')$B$tau
print(tau_values3a)
print(tau_values3b)

#########################################################
```

```{r, eval=FALSE}
library(stuart)

# Inspect the structure of kfold_sel2$final
tau_valuesa <- lavaan::inspect(kfold_sel2$final, 'est')$A$tau
tau_valuesb <- lavaan::inspect(kfold_sel2$final, 'est')$B$tau
print(tau_valuesa)
print(tau_valuesb)

# objective
# Define the objective function
objective.normal2 <- function(rmsea.scaled, srmr, cfi.scaled, tau) {
  # Ensure tau is an atomic vector
  if (!is.atomic(tau)) {
    tau <- as.numeric(unlist(tau))
  }
  # Calculate the standard deviation of tau
  sd_tau <- sd(tau)
  
  # Compute the components of the objective function
  out1 <- 0.5 - (0.5 / (1 + exp(-100 * (rmsea.scaled - 0.05))))
  out2 <- 0.5 - (0.5 / (1 + exp(-100 * (srmr - 0.05))))
  out3 <- 1 / (1 + exp(-100 * (cfi.scaled - 0.95)))
  
  # Adjust out4 to maximize the standard deviation of tau
  out4 <- 1 / (1 + exp(-5 * (sd_tau - 0.5)))
  
  # Combine the components into the final objective function
  out <- (out1 + out2 + out3 + out4) / 4
  return(out)
}

kfold_sel3 <- kfold(
  'gene',
  k = 3,
  data = ords,
  factor.structure = fs,
  max.invariance = "strong",
  capacity = list(4, 4, 4, 4, 4),
  seed = 2026,
  seeded.search = TRUE,
  auxiliary = NULL,
  software = "lavaan",
  cores = 4,
  objective = objective.normal2,
  ignore.errors = TRUE,
  burnin = 5,
  generations = 500,
  individuals = 34,
  selection = "tournament",
  selection.pressure = NULL,
  elitism = NULL,
  reproduction = 0.5,
  mutation = 0.05,
  mating.index = 0,
  mating.size = 0.25,
  mating.criterion = "similarity",
  immigration = 0,
  convergence.criterion = "geno.between",
  tolerance = NULL,
  reinit.n = 1,
  reinit.criterion = "geno.between",
  reinit.tolerance = NULL,
  reinit.prop = 0.75,
  schedule = "run",
  analysis.options = NULL,
  suppress.model = FALSE,
  remove.details = TRUE
)
```

Running cross-validation.

Error in dimnames(x) <- dn : 
  length of 'dimnames' [1] not equal to array extent
Warning: The crossvalidation produced an error in fold 1


```{r}
summary(kfold_sel3)

inspect(kfold_sel3$final, 'est')
inspect(kfold_sel3$final, 'fit')

lavaan::summary(kfold_sel3$final, standardized = TRUE) 
lavaan::inspect(kfold_sel3$final, 'est')$A$tau
lavaan::inspect(kfold_sel3$final, 'est')$B$tau
```
SUMMARY OF ANALYSIS:

Number of Folds: 3 
Analysis Type: gene 
Estimation Software: lavaan 
Models Estimated: 52088 
Time Required: 1064.206 seconds

Crossvalidation Results with STRONG Invariance:

Average Jaccard Similarity: Comprehension: 0.600, Evaluation: 0.333, Integration: 0.600, Communication: 0.600, Statistics: 0.333

Constructed Subtests: (k = 1)
Comprehension: F1F3 F1F6 F1F12 F1F14
Evaluation: F2F5 F2F12 F2F14 F2F16
Integration: F3F3 F3F4 F3F7 F3F8
Communication: F4F1 F4F2 F4F3 F4F8
Statistics: F5F3 F5F6 F5F15 F5F18

```{r, eval=FALSE}
library(stuart)
# kfold & genetic loop paar mal laufen lassen, für stabile ergebnisse
# theoretisch festlegen, oder kombinatorisch/technisch festlegen? Random draws für fit indices?
```

## Ordinal crossvalidation
```{r}


```
#####################################
#####################################
## Check correlated residuals
```{r}
# Fit the initial SEM model

fit <- sem(cfa_model_m, joined_mgcfa_data)

# Check modification indices
modificationIndices(fit, sort = TRUE)

# If justified, re-specify your model to include correlated residuals
model_revised <- '
  # Your original model specification
  residual1 ~~ residual2 # Add this line to specify correlated residuals
'
fit_revised <- sem(model_revised, data = your_data)

# Compare the original and revised models
anova(fit, fit_revised)
```

## Convergent Measures
```{r}
SWE <- df3 %>%
  dplyr::select(starts_with("SWE")) %>%
  mutate_all(as.numeric)

NFC <- df3 %>%
  select(starts_with("NFC")) %>%
  mutate_all(as.numeric)

ICT <- df3 %>%
  select(starts_with("ICT")) %>%
  mutate_all(as.numeric)

B5_openness <- df3 %>%
  select(starts_with("B5.o")) %>%
  mutate_all(as.numeric)

B5_con <- df3 %>%
  select(starts_with("B5.G")) %>%
  mutate_all(as.numeric)
```

## Inverting negatively keyed items
```{r}
# Recoding for B5_con and B5_openness
B5_con$B5.G1. <- 6 - B5_con$B5.G1.
B5_openness$B5.O2. <- 6 - B5_openness$B5.O2.

# Recoding for NFC
NFC$NFC.NFC1. <- 8 - NFC$NFC.NFC1.
NFC$NFC.NFC4. <- 8 - NFC$NFC.NFC4.

# Recoding for SWE
SWE$SWE16.SWE4. <- 6 - SWE$SWE16.SWE4.
SWE$SWE16s2.SWE14. <- 6 - SWE$SWE16s2.SWE14.
```

## Calculating meanscores
```{r}
# Calculate row means for each row and add as a new column
B5_con <- B5_con %>%
  rowwise() %>%
  mutate(B5con_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
B5_openness <- B5_openness %>%
  rowwise() %>%
  mutate(B5o_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
NFC <- NFC %>%
  rowwise() %>%
  mutate(Nfc_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
ICT <- ICT %>%
  rowwise() %>%
  mutate(Ict_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

# Calculate row means for each row and add as a new column
SWE <- SWE %>%
  rowwise() %>%
  mutate(Swe_Row_Means = mean(c_across(everything()), na.rm = TRUE)) %>%
  ungroup()

```

```{r}
# Combine the row means into a single data frame
all_means <- data.frame(
  B5con_Row_Means = B5_con$B5con_Row_Means,
  B5o_Row_Means = B5_openness$B5o_Row_Means,
  Nfc_Row_Means = NFC$Nfc_Row_Means,
  Ict_Row_Means = ICT$Ict_Row_Means,
  Swe_Row_Means = SWE$Swe_Row_Means
)
```

## Correlations 
```{r}
library(apaTables)
cortable<-cor(all_means, use = "pairwise" ,method = "pearson")
apa.cor.table(all_means)
```

```{r}
library(dplyr)
library(reshape2)  # for melt function
library(ggplot2)
library(psych)     # for corr.test function

# Select newly created row means for correlation analysis
selected_vars <- all_means %>%
  select(B5con_Row_Means, B5o_Row_Means, Nfc_Row_Means, Ict_Row_Means, Swe_Row_Means)

# Rename variables for clarity in heatmap
colnames(selected_vars) <- c(
  "B5con", "B5o", "Nfc", "Ict", "Swe"
)

# Calculate correlations
correlation_matrix <- round(cor(selected_vars, use = "pairwise"), 2)

# Function to keep only upper part of the matrix
upper_triangle <- function(mat) {
  mat[lower.tri(mat)] <- NA
  return(mat)
}

correlation_upper <- upper_triangle(correlation_matrix)

# Reshape data for plotting
melted_data <- melt(correlation_upper, na.rm = TRUE)

# Plot heatmap
heatmap3 <- ggplot(data = melted_data, aes(Var2, Var1, fill = value)) +
  ggtitle("Correlations - Row Means of Selected Variables") +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1), space = "Lab",
    name = "Pearson\nCorrelations"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1),
    axis.text.y = element_text(size = 8),
    plot.title = element_text(size = 12),
    legend.position = "right",
    legend.title = element_text(size = 8)
  ) +
  coord_fixed()

# Add correlation coefficients as text annotations
heatmap_final3 <- heatmap3 + 
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 2.5) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(0.6, 0.7),
    legend.direction = "horizontal"
  ) +
  guides(
    fill = guide_colorbar(
      barwidth = 7, barheight = 1,
      title.position = "top", title.hjust = 0.3
    )
  )

# Print or view the final heatmap
print(heatmap_final3)

```

## Reliability
```{r}
omega(B5_con[1:2],1)
omega(B5_openness[1:2],1)
omega(NFC[1:4],1)
omega(ICT[1:5],1)
omega(SWE[1:16],1)
#omega(DataLiteracy[],5)
```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

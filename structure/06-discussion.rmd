# Discussion

This study meant to examine whether 20 items out of the initial item pool, would reflect the suggested measurement model within the current sample. Furthermore, the aim was to find a solution utilizing the genetic algorithm as well as a 3-fold crossvalidation in 'stuart', that yields good model fit as well as reliability, and that shows measurement invariance with a random split-sample. Additionally, correlations with other scales and constructs were to be examined to locate the construct within the nomological net. 
The model demonstrating the best overall fit was selected, based on a set of 20 items representing five factors: Comprehension, Evaluation, Integration, Communication, and Statistics, each with four items and respective measurement errors. The model also included inter-factor correlations between all latent factors.
Construct validity was evaluated through CFA, using MLR as estimator, as well as correlation analyses with related constructs.

## Model fit Initial Item Selection
Although the RMSEA was a bit low, the initial model exhibited an acceptable to good fit, with indices suggesting good model fit (RMSEA = 0.05, SRMR = 0.08, CFI = 0.96, TLI = 0.96), according to @hu1999.
However, recent advancements, such as the "ezCutoffs" package [@schmalbach2019], propose simulated cutoffs that adapt to specific model characteristics, potentially offering a more rigorous and context-sensitive approach to assessing model fit.
According to this, the following cutoffs are suggested: scaled CFI: .986; scaled RMSEA: .022 and SRMR: .041. 
Respectively, the model exhibits non-acceptable fit.
It is argued, that fixed cutoffs are derived from specific confirmatory factor analysis models and may not perform well under varying conditions, such as sample size, model complexity, and response types [@mcneish_wolf_2023; @Goretzko2023]. 
This woult suggests that while traditional cutoffs provide a useful starting point, they may not adequately capture the nuances of different modeling scenarios.
The factor loadings ranged from 0.50 to 0.81 across the five factors (cf. Figure 1), and the models McDonaldâ€™s $\omega$\textsubscript{total} was 0.92, indicating reliable internal consistency.
Regarding the first hypothesis (H1) it can be concluded, that the algorithm was able to find a solution, exhibiting good model fit in the training sample, for the suggested measurement model.

## Measurement Invariance and Model Fit
For further validation, the model was tested for measurement invariance against the test-sample. 
The results suggests that invariance levels up to scalar invariance hold (cf. Table 3).
This conclusion is drawn based on the fit indices remaining within the acceptable range proposed by @chen2007. 
However, given that fit indices are regarded more as rough guidelines than as precise or universally applicable cut-off values, an additional perspective was considered. 
Specifically, the differences between the (robust) confirmatory fit indices were examined to ensure they remained below 0.01, as recommended by @cheung2002.  
In practical terms, when scalar invariance holds, the factor loadings, intercepts, and measurement scales can be considered equivalent across groups.
Thus, the data indicates that all factor loadings are consistent across groups [@cheung2002]. 
This suggests that the data literacy items and their underlying factors are associated with the same strengths in both samples. Additionally, scalar invariance was established, implying that beyond the factorial structure and factor loadings, the item intercepts are also invariant across groups.
This allows for meaningful comparisons of latent means between groups rather than being due to measurement bias or differences in how the construct is understood or measured across groups [@cheung2002; @riordan1994].
The lack of residual invariance suggests that the residuals (unexplained variance in the indicators) are not equivalent across groups. This implies that there are group-specific differences in how much of the variance in the observed variables remains unexplained by the latent factors. Meaning, this lack of residual invariance complicates the interpretation of differences between groups [@cheung2002]. While the factors themselves may be measured similarly (because scalar invariance holds), the amount of unexplained variability in the responses differs across groups, indicating potential unmodeled differences in how the groups respond to certain items.
As a result, any observed differences in the latent factors could be influenced by differing error variances across groups, making it challenging to draw definitive conclusions about true group differences. Thus, while latent means can be compared, the comparisons may be confounded by measurement error that varies across groups.  
Regarding the second hypothesis (H2) it can be concluded that the latent factor structure of the initial analysis was supported by the test-sample. 
It is also to be noted that the model fit on the configural level (RMSEA = 0.041, SRMR = 0.09, CFI = 0.92), according to @hu1999 and  the 'ezCutoffs' package [@schmalbach2019], is worse than the fit of the training sample, except for the RMSEA (cf. Table 3). 
The changing of the fit in the analysis with the test sample is also reflected in the measurement model (cf. Figure 2). 
There does not appear to be a consistent direction of change, as some factor loadings on the items increase while others decrease.
The same applies to the latent correlations between the factors, which could be interpreted as an indication of the solution's instability.

Across the versions of this model, the CFI decreases the most and the RMSEA suffers the least.
As the CFI is comparing the tested models against a base-line model, it could be that the strong decrease in this index reflects the divergence of the model respective to the differing sample and thus indicates overfitting.
Furthermore, some items correlated [VERWEIS ANHANG], as well as having high residual correlations [VERWEIS ANHANG] (shared variance not explained by the latent factor), therefore the model fit could deteriorates and reduces the CFI.
The SRMR showed mediocre fit, but got worse in the MGCFA and progressively with increasing the invariance levels. This could in part be due to the fact, that the SRMR makes no correction for parsimony, thus gets better the more paths a model contains. 
As invariance is increased and paths are constrained to equality, it could make sense for the SRMR to increase. 
[SCHULTZE EID QUELLE?]
The RMSEA in turn accounts for some complexity of the model, and also performs better in this case as the SRMR.

Those correlated residuals, also indicate potential specification errors, both in the training and test samples. Furthermore, the latent correlations among factors were overall strong, especially between Communication and Statistics (0.91 in the training sample and 0.84 in the test sample) (cf. Tables 4 & 5). 
This could indicate redundancy among the factors, that needs to be looked at in terms of items and factor specification.

## Construct Validity
The moderate to high correlations between the newly created data literacy scale and the SWE-IV-16, and the general items of the ICT-SC25 (cf. Table 6) suggest that the scale measures a concept that is related but distinct. 
This supports the expectations outlined in H3 and H4. 
The results consistent with H3 and H4, which concern the alignment of the measured latent factors with competency-based constructs, further support the interpretation that the scale captures perceived competency rather than a stable personal character trait.
Those correlations could indicate convergent validity [@gregory2004].  
The strong correlation between the data literacy scale and NFC-K are not in line with H5 (cf. Table 6). 
This might reflect conceptual proximity between the two measures, and could indicate that the data literacy questionnaire measures a construct more aligned with trait characteristics than planned. 
It is plausible that individuals who enjoy understanding concepts in detail (as measured by NFC-K) also display behaviors associated with critically examining data and information. This finding underscores the relevance of motivation as a contributing factor to the observed behaviors, suggesting conceptual overlapping elements between data literacy and need for cognition.  
The data literacy factors also showed small to moderate correlations with personality traits, including a negative correlation with openness and a positive correlation with conscientiousness. The positive association with conscientiousness aligns with H7, indicating that individuals who are organized and diligent may also exhibit behaviors related to data literacy.
This correlation could be an indicator for discriminant validity [@hubley1996; @cronbach1955].  
However, the negative correlation with openness contradicts expectations (H6).
Given the conceptualization of data literacy as a proficiency that incorporates motivation to engage in related behaviors, a slight positive correlation with openness was anticipated, as openness typically reflects curiosity and a willingness to explore new ideas, or a persons motivation to show certain behaviors.
The opposite was found.
This unexpected negative relationship could suggest that the items of the data literacy scale emphasize ability or skill over proficiency, thereby capturing behaviors less influenced by intrinsic motivation.  
Another plausible explanation for the findings would be a mismatch between the items and the study sample. Motivation can be linked to expertise [@earley1990; @paletz2013], and participants may perceive the assessed behaviors as routine rather than requiring deliberate effort or motivation. For these individuals, data literacy behaviors might represent habitual actions rather than aspirational or exploratory tendencies.

@paletz2013 highlight that individuals engaged in routine behaviors can identify problems and adapt their processes as they gain experience. This suggests that, as expertise develops, individuals may rely more on established routines rather than on motivation to initiate behaviors. Similarly, @fodor2017 suggest that with increased familiarity, task-related behaviors may become less influenced by external motivational factors and instead governed by ingrained, automated routines.
The development of expertise also frequently leads to cognitive simplification of complex tasks. As individuals gain familiarity, they tend to schematize their behaviors into automatic processes, reducing the reliance on deliberate motivational triggers [@earley1990]. This cognitive simplification allows individuals to focus less on initiating behaviors and more on refining and adapting their routines as needed.  
Taken together, these perspectives suggest that the role of motivation in data literacy behaviors may diminish with increasing expertise, as participants shift toward automated and habitual patterns of engagement. 

## Control Variables
The control variable analysis revealed statistically significant effects of educational level, degree pursuit, and age on specific items. These findings could help to explain how individual characteristics influence self-reported behaviors and competencies and offer insights for educational interventions.
The negative effect of Education5 (A-levels) on F2F15 ("Ich Ã¼berprÃ¼fe die Qualifikation von Autor*innen, bevor ich mich auf die Informationen verlasse.") suggests that individuals with A-levels as their highest educational attainment are less inclined to evaluate an authorâ€™s qualifications before trusting their information. This may reflect insufficient emphasis on critical evaluation skills at the A-level stage, which are typically developed more extensively in tertiary education. This observation underscores a potential gap in curricula, which could be addressed by integrating critical appraisal training earlier in the educational trajectory. Notably, some German schools have already reformed their curricula to emphasize media competency [@richter_scheiter_2023; @mediaeducationlab2020], potentially serving as a model for broader implementation.

For F3F2 ("Ich beschÃ¤ftige mich mit Informationen, die meine Ansichten in Frage stellen."), the analysis was inconclusive. Singularities in the data, likely caused by collinearity or insufficient variability in the predictor variables, limit interpretability. This finding highlights a methodological limitation, suggesting that the item F3F2 may lack robustness for statistical modeling in its current form. Future studies should consider refining or reformulating this item to enhance its validity.

Education5 (A-levels) exhibited a positive effect on F4F3 ("Ich kann Daten in Grafiken so prÃ¤sentieren, dass sie fÃ¼r verschiedene Zielgruppen verstÃ¤ndlich sind."), indicating that A-level graduates report confidence in presenting data graphically to diverse audiences. This may reflect the inclusion of basic communication and data visualization skills in the A-level curriculum, underscoring the value of these foundational competencies at this stage of education.

The results for F4F8 ("Ich kann mit Programmen Grafiken erstellen, um Ergebnisse zu prÃ¤sentieren.") demonstrate a progressive relationship between educational attainment and proficiency with data visualization software. While Education5 (A-levels) is associated with a modest increase (+2.82 points), higher educational levels such as Education7 (university of applied sciences degree) and Education8 (university degree) show substantially larger effects (+7.46 and +7.04 points, respectively). These results likely reflect the greater exposure to specialized tools and training in higher education. Intriguingly, Degree2 (currently pursuing a Masterâ€™s degree) was associated with a negative effect on F4F8 (-3.20 points), potentially indicating transitional challenges or curricular variability in graduate programs. In contrast, Degree3 (state examination) showed a positive effect (+2.60 points), suggesting that this pathway may place greater emphasis on developing these skills.

For F5F8 ("Wenn ich mit umfangreichen DatensÃ¤tzen konfrontiert werde, kann ich daraus Erkenntnisse gewinnen."), a significant negative effect was observed for Age22 (-3.00 points), indicating lower self-reported confidence in extracting insights from large datasets among this age group. This result may reflect developmental factors, such as limited experience or confidence, or contextual influences unique to this stage. However, as no other items showed age-related effects, this finding should be interpreted cautiously. Additionally, while no significant educational variables were identified for F5F8, Degree3 (state examination) demonstrated a significant positive effect (+3.00 points). However, the exclusion of other degree categories raises the possibility of biased estimates, warranting further investigation.

## Limitations

The results of this study should be interpreted with several limitations in mind. The sample deviates from the general population in multiple demographic variables, potentially compromising its representativeness and generalizability. Occupational distribution among participants shows clustering in fields such as *"Gesundheit, Soziales, Lehre und Erziehung"*, *"Buchhaltung, Recht und Verwaltung"*, *"KaufmÃ¤nnische Dienstleistungen, Vertrieb, Tourismus"* and especially *"Naturwissenschaft, Geografie und Informatik"*. This indicates a selection bias, likely due to recruitment methods (who is reached) and implicitly favoring individuals more interested in data literacy.

The item pool for the questionnaire was specifically trained on this non-representative sample, which will likely affect its validity. Because it could result in the creation and measurement of a latent construct, that is specific or unique even to this particular sample.
Ideally, this approach would have beenn used with a sample, representative of the general public or citizens. That way the results would be more valid and could more likely be generalized. 

The visual analysis of the items boxplots showed several outlier in the data. Those were left in the sample for further investigation, since based on the clustering in the high categories (4 and 5), it could mean that the items were too easy for most respondents (in the sense that most respondents agreed with the items). This high concentration of responses at the upper end of the scale could indicate that the items did not adequately discriminate across a range of respondent abilities, resulting in those ceiling effects. The outliers in the low categories (0, 1, or 2) are the minority and are less likely to influence the overall conclusions. However, they do signal that a small subset of participants found the items unclear or disagreed with them and could hint at a systematic influence.
The measure was designed for citizens, potentially limiting discrimination at higher item difficulties, among more literate participants, a direction to be improved in future studies. 
Also, as data literacy is a heterogeneous construct, this complicates global instrument development and understanding across all participants. Ideally the questionnaire would incorporate a broader content to better reflect the constructs full scope, thereby increasing content validity. Expanding the questionnaire with additional items could address this need, although it would deviate from the principle of parsimony. 

These issues regarding the misfit of items and sample, as well as the heterogeneity of the construct further result in limitations regarding the reliability of the instrument. Although the McDonaldâ€™s $\omega$\textsubscript{total} would indicate good reliability, the question of reliability stretches beyond a single measure for internal consistency. 
The reliability of a measure must be evaluated in relation to its target audience, as its validity and generalizability depend on how well the sample reflects the intended population. 
Therefore, it is crucial to determine which sample the measure is based on and the population it aims to represent.

In discussing the characteristics and demographics of the sample, the randomization of the data literacy items among respondents also introduced certain limitations. 
The primary limitation lies in the unequal demographic distribution across items. 
For example, while the overall sample has an average age of 40 years, this demographic balance may not hold for each individual item.
This issue extends to other demographic variables as well, resulting in the diversity of the sample not being consistently reflected in the responses to individual items. 
Consequently, the selected items would require further and coherent testing of the item set as a whole on a diverse sample to evaluate how demographic variability influences responses.
The limited demographic balance in the items also results in limited control for confounding variables in those variables. It furthermore limits the informative value of comparisons regarding the demographic variables among the items.  

Additionally, the training and testing data sets differed in size, which could influence measurement invariance testing [@chen2007]. While the sample sizes were appropriate, they were at the lower threshold of the prior power analysis [e.g., @kass1979; @hu1999], suggesting that larger samples might have been better. Also because the 3-folding was on the lower threshold of power with this sample size and ideally, to maybe increase the stability of the solution, a larger number of folds could have helped achieving that. Lastly, it should be noted that algorithm-based item selection is a heuristic approach, rather than deterministic, and may not always yield the optimal solution [@schultze2017; @blum2003].
In this specific study, the results of the algorithm based selection with the genetic algorithm in 'stuart' appeared to be unstable across different runs. 
I tried to account for that via the $k$-folding and multiple iterations of the selection, but when the process of the data imputation was changed, from mice to FIML, different items were selected, indicating unstable selections.
This could potentially be accounted for, by using the bruteforce implementation in 'stuart'. 
But it could also be a hint at model misspecification, because of which the algorithm finds multiple local minima.  
The models yielded rather bad fit measures, when using the initially planned objective function (with the aim of optimising for model fit, reliability, and variability in the difficulty of items.
Therefor, it was changed in favor of model fit criteria exclusively.
Thus, the final objective function that was used, did not incorporate terms to optimize for reliability McDonaldâ€™s $\omega$ nor variability in the difficulty of items.  
Furthermore WLSMV could not be used, although being the appropriate estimator for rating scales, because of the ceiling effects in the items and the scarcely used lower answer categories. 
I tried to account for that by collapsing the respective answer categories, but this resulted seemingly in an introduction of better fit to the model. Because of that, I decided to use MLR as estimator and were therefore able to use FIML as imputation method, implemented in 'lavaan'.
Ideally future studies, with a sample without ceiling effects, would use WLSMV as estimator, since modified weighted least square estimators for ordered-categorical indicators (MWLS\textsubscript{C}) provide accurate estimates of the model parameters given a stable weight matrix [@wirth2007] and furthermore, WLSMV is a robust estimator which does not assume normally distributed variables and provides the best option for modelling categorical or ordered data [@brown2006]. 

## Future directions

Future research in this domain should focus on expanding the qualitative development of items to ensure comprehensive coverage of the various dimensions of data literacy. A more extensive qualitative item creation process, informed by interviews with a diverse group of individuals, would contribute to a more nuanced understanding of the construct, particularly across different fields. Such qualitative insights could help identify representative behaviors and cognitive processes, forming a more solid foundation for item development. Additionally, these insights would contribute to the establishment of a more robust theoretical model for self-perceived data literacy. Expert interviews would be particularly valuable, integrating multiple perspectives on the construct and providing a more grounded theoretical base.

Future studies should consider assessing whether items in data literacy scales are sensitive to the distinction between routine and aspirational behaviors. Additionally, capturing the progression from motivated to routine actions could provide deeper insights into how expertise shapes self-perceived data literacy.
These findings invite further refinement of the scale to better balance the measurement of skills, proficiencies, and the motivational aspects of data literacy. 
Future research should also consider whether the scale captures routine versus intentional behaviors, as this distinction may vary across samples and contexts.

This study clearly highlights that both the measurement model and the items of the scale require revision. Given that data literacy is a heterogeneous and multi-faceted construct, future research should rigorously explore the dimensionality of the scale to ensure its accuracy and utility. A thorough investigation of this dimensionality is essential for the validity of any future attempts at scale validation.
A careful and thorough development of items is necessary to address the multi-faceted nature of the construct. This is particularly important for adapting the scale to measure individual skill levels accurately, which could be achieved through methods such as Item Response Theory (IRT). An IRT-based framework could significantly enhance the scaleâ€™s precision by allowing for variability in item difficulty, thereby improving the adaptability and accuracy of the measurement. The introduction of adaptive testing, based on IRT principles, could lead to a more efficient scale, requiring fewer items while maintaining or even improving measurement precision.



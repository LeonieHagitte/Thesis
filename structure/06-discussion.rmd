# Discussion

## Summarising the results

Everything worked - what do I need to report?
First, report on the three core decisions and how the solution was created, whether it is stable, and how it was validated. The final variant is then reported exactly like a CFA (e.g. Jackson et al., 2009). To have a clear structure, answer the following questions:
How large was the original item pool and how was it created?
What is the structure of the scale?
Which algorithm was used (or bruteforce)?
Which objective function was used?
How stable is the final variant?
How was the final variant further validated (e.g. crossvalidate, k-fold)?


## What does it all mean / "Why?"

- Start with the research question
  - Maybe then towards the hypotheses
- connecting findings to the related theories
- very related/ current literature first, than broader is possible
- When discussing the why - be careful, because you didnt test that

## Limitations
- content validity
- DIF?
- psych science - authors guide to generelizability
- attempts to control for limitating factors
- dont include to general/ broad critiques, but special one for my own study

The results of this study should be interpreted with several limitations in mind. The sample deviates from the general population in multiple demographic variables, potentially compromising its representativeness and generalizability. Occupational distribution among participants shows clustering in fields such as **"Gesundheit, Soziales, Lehre und Erziehung"**, **"Buchhaltung, Recht und Verwaltung"**, **"Kaufm√§nnische Dienstleistungen, Vertrieb, Tourismus"** and especially **"Naturwissenschaft, Geografie und Informatik"**. This indicates a selection bias, likely due to recruitment methods (who is reached) and implicitly favoring individuals more interested in data literacy.

The item pool for the questionnaire was specifically trained on this non-representative sample, which may affect its validity. Data literacy is a heterogeneous construct, and the questionnaire could incorporate more aspects to better reflect its full scope, thereby increasing content validity. Expanding the questionnaire with additional items could address this need, although it would deviate from the principle of parsimony.

Additionally, the training and testing data sets differed in size, which could influence measurement invariance testing (Chen, 2007). While the sample sizes were appropriate, they were at the lower threshold of the prior power analysis (e.g., Kass and Tinsley, 1979; Hu and Bentler, 1999), suggesting that larger samples might have been ideal. Lastly, although the questionnaire showed good model fit, it should be noted that algorithm-based item selection is a heuristic approach and may not always yield the optimal solution.

## Future directions
Future research should explore adaptive testing using Item Response Theory (IRT). IRT provides a method to tailor item difficulty to respondents' ability levels in real-time, enhancing assessment efficiency and precision. This reduces the number of items required while maintaining high measurement accuracy. Implementing IRT is particularly advantageous for heterogeneous constructs like data literacy, as it ensures each participant is evaluated with items suited to their skill level. One of the significant challenges in applying IRT is the assumption of unidimensionality, where items are presumed to measure a single underlying trait. Data literacy, however, is a multi-faceted construct, and future studies should investigate the dimensionality of the scale rigorously. 
An alternative to IRT-based adaptive testing is the use of Classification and Regression Trees (CART). CART is a tree-based method that splits data into subsets based on binary decisions, optimizing for predictive accuracy. This approach could simplify adaptive testing by using binary splits to classify respondents into different levels of data literacy. The Gini index can be employed within CART to identify the optimal cutoff points for these splits, ensuring that each branch of the tree maximally distinguishes between different levels of data literacy competence.


########################################

Construct validity is evaluated through confirmatory factor analysis (CFA), using MLR as estimator, as well as correlation analyses with related constructs

- what to optimize the scale for?
- dynamic fit indices
- factors 4 and 5
    - adaptive testing/ IRT
        - dimensionality assumption and computationally intense
    - CART - tree based adaptive testing (classification trees) always binary split
        - gini index to identify the cut off 
        - POMP method - for differing number of answer formats
- residuals correlates 
- heterogeneity of construct 

The heterogeneous nature of the construct complicates global instrument development and understanding across all participants. The measure is designed for citizens, potentially limiting discrimination at higher item difficulties or among more literate participants, a direction we aim to improve in future studies.


Noteworthy is the inherent approximate, rather than deterministic, nature of metaheuristics (Schultze & Lorenz ,2023; Blum and Roli, 2003).
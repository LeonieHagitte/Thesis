# Discussion

This study meant to examine whether a pool 20 items out of the initial item pool of 71 items, would reflect the suggested measurement model within the current sample. Furthermore, the aim was to find a solution via the genetic algorithm as well as a 3-fold crossvalidation in 'stuart', that yields good model fit as well as reliability and that shows measurement invariance with a random split-sample. Additionally, correlations with other scales and constructs were to be examined to locate the construct in the nomological net. 
The model demonstrating the best overall fit was selected based on a set of 20 items representing five factors: Comprehension, Evaluation, Integration, Communication, and Statistics, each with four items and respective measurement errors. The model also included inter-factor correlations between all latent factors.
Construct validity was evaluated through CFA, using MLR as estimator, as well as correlation analyses with related constructs.

Although the RMSEA was a bit low, the initial model exhibited an acceptable to good fit, with indices suggesting good model fit (RMSEA = 0.05, SRMR = 0.08, CFI = 0.96, TLI = 0.96), according to @hu1999.
According to the 'ezCutoffs' function from the 'ezCutoffs' package [@schmalbach2019] suggests the following cutoffs: scaled CFI: .986; scaled RMSEA: .022 and SRMR: .041. So according to those simulated cutoffs from the 'ezCutoffs' package, the model exhibits non-acceptable fit. Since the simulation cutoffs tend to be more rigorous than those from the literature, it is not surprising, that the model doesn't show good fit, compared to the simulated cutoffs [@QUELLE].
The factor loadings ranged from 0.50 to 0.81 across the five factors (cf. Figure 3), and the models total McDonald’s $\omega$ was 0.92, indicating reliable internal consistency.
Regarding the first hypothesis (H1) it can be concluded, that the algorithm was able to find a solution, exhibiting good model fit in the training sample, for the suggested measurement model.
For further validation of the model was tested for measurement invariance, against the test-sample. The comparison of the training sample and the test sample regarding measurement invariance suggests that invariance levels up to scalar invariance hold (cf. Table 2).
This conclusion is drawn based on the fit indices remaining within the acceptable range proposed by @chen2007. However, given that fit indices are regarded more as rough guidelines than as precise or universally applicable cut-off values, an additional perspective were considered. Specifically, the differences between the (robust) confirmatory fit indices were examined to ensure they remained below 0.01, as recommended by @cheung2002.
In practical terms, when scalar invariance holds, the factor loadings, intercepts, and measurement scales can be considered equivalent across groups. This allows for meaningful comparisons of latent means between groups rather than being due to measurement bias or differences in how the construct is understood or measured across groups [@cheung2002; @riordan1994].
The lack of residual invariance suggests that the residuals (unexplained variance in the indicators) are not equivalent across groups. This implies that there are group-specific differences in how much of the variance in the observed variables remains unexplained by the latent factors. Meaning, this lack of residual invariance complicates the interpretation of differences between groups. While the factors themselves may be measured similarly (because scalar invariance holds), the amount of unexplained variability in the responses differs across groups, indicating potential unmodeled differences in how the groups respond to certain items.
As a result, any observed differences in the latent factors could be influenced by differing error variances across groups, making it challenging to draw definitive conclusions about true group differences. Thus, while latent means can be compared, the comparisons may be confounded by measurement error that varies across groups.
Regarding the second hypothesis (H2) it can be concluded that the latent factor structure of the initial analysis was supported by the test-sample. 
It is also to be noted that the model fit on the configural level (RMSEA = 0.041, SRMR = 0.09, CFI = 0.92), according to @hu1999, is worse than the fit of the training sample only, except for the RMSEA (cf. Table 2).

Across the versions of this models it is seen, that the CFI decreases the most and the RMSEA suffers the least.
As the CFI is comparing the tested models against a base-line model it could be that the strong decrease in this index reflects the divergence of the model respective to the differing sample and thus indicate overfitting.
The SRMR shows mediocre fit, but gets worse in the MGCFA and progressively with increasing the invariance levels. This could in part be due to the fact, that the SRMR makes no correction for parsimony, thus gets better the more paths a model contains. As we increase invariance and constrain paths to equality, it makes sense for the SRMR to increase. 
The RMSEA in turn accounts for some complexity of the model and also performs better in this case.

Further investigation of residuals revealed several correlated residuals, indicating potential specification errors, both in the training and test samples. Furthermore, the latent correlations among factors were overall strong, especially between Communication and Statistics (0.91 in the training sample and 0.84 in the test sample). 
This could indicate redundancy among the factors, that needs to be looked at in terms of items and factor specification.

The moderate to high correlations between the newly created scale and the SWE-IV-16, NFC-K and the general items of the ICT-SC25 indicate the data literacy scale measures a similar, yet distinct concept. This is in line with the expectations (H3, H4 & H5). Furthermore, the results in line with H3 and H4 suggest that the latent factors measured by the items are indeed on the side of a perceived competency, rather than a personal character trait. 
The high correlation of the data literacy scale with the NFC-K, although more on the trait side, could likely be due to the proximity to content of both scales. It seems intuitive, that people who enjoy having everything understood down to the smallest detail, also show behaviors related to examination of data and information.
The data literacy factors exhibited small to moderate correlations with personality traits, a negative correlations with openness and positive correlations with conscientiousness. This is in line with my expectations regarding conscientiousness (H7), but not in line with the expectations regarding openness to new experiences. Because data literacy is conceptually on the side of proficiencies, incorporating ones motivation to actually show the related behaviors, it was expected to correlate slightly positive with openness to new experiences. However, the opposite was true for the samples in this study. Consequently, this could mean that the items selected are on the ability or skill side rather than the proficiency side, minimizing the proportion of motivation that is measured. 
It could also be because the items and the sample do not match. Since motivation could be directly linked to people's expertise [QUELLE], it could be that the participants do not show the motivation asked about because for them this falls into a behavioral area that is less linked to motivation and is shown more routinely.

Control variable analysis revealed that educational level, pursuing different degrees as well as age had statistically significant effects on certain items. 
The significant negative effect of Education 5 (A-levels) on F2F15 ("Ich überprüfe die Qualifikation von Autor*innen, bevor ich mich auf die Informationen verlasse.") indicates that individuals whose highest educational level is A-levels are less likely to check an author’s qualifications before trusting their information. This may reflect insufficient emphasis on critical evaluation skills at this educational stage, which are often introduced or reinforced in tertiary education. This finding highlights a potential area for curriculum enhancement to foster critical appraisal skills earlier in educational trajectories. Some Schools in Germany already started to change their curricula towards a stronger emphasis on media competency [QUELLE]. 
The analysis of F3F2 ("Ich beschäftige mich mit Informationen, die meine Ansichten in Frage stellen.") was inconclusive, possibly due to singularities in the data, which suggest collinearity or insufficient variability in the predictor variables. This limitation underscores the potential inadequacy of the item F3F2 for robust statistical modeling. Refining or reformulating this item may improve its validity in future studies.
In contrast, Education5 (A-levels) exhibited a significant positive effect on F4F3 ("Ich kann Daten in Grafiken so präsentieren, dass sie für verschiedene Zielgruppen verständlich sind."), indicating that individuals at this educational level are confident in their ability to present data graphically for different target groups. This result may be attributed to the emphasis on basic communication and visualization skills within A-level curricula.
The findings for F4F8 ("Ich kann mit Programmen Grafiken erstellen, um Ergebnisse zu präsentieren.") demonstrate a progressive influence of educational attainment on self-reported proficiency with programs for creating data visualizations. While Education5 (A-levels) is associated with a 2.82-point increase, higher educational levels, such as Education7 (degree from a university of applied sciences) and Education8 (university degree), yield greater effects, with increases of 7.46 and 7.04 points, respectively. These findings suggest that higher education fosters advanced technical competencies, likely reflecting greater exposure to specialized training and tools. Interestingly, Degree2 (currently pursuing a Master’s degree) shows a significant negative effect on F4F8 (-3.20 points). This finding could stem from transitional challenges or varying curricula during graduate studies. Conversely, Degree3 (currently pursuing a state examination) demonstrates a positive effect (+2.60 points), suggesting that this pathway may emphasize or require such skills.
For F5F8 ("Wenn ich mit umfangreichen Datensätzen konfrontiert werde, kann ich daraus Erkenntnisse gewinnen."), Age22 shows a significant negative effect (-3.00 points), indicating that individuals aged 22 report lower scores compared to the reference age group. While, this could reflect developmental or contextual factors specific to this age group, such as reduced confidence or limited experience in the domain assessed by F5F8, it could also be an artifact, since no other item showed age effects. While no significant educational variables were identified for F5F8, Degree3 (pursuing state examination) demonstrates a significant positive effect (+3.00 points). However, caution is warranted in interpreting this result due to the exclusion of other degree categories, potentially leading to biased estimates.

## What does it all mean / "Why?"
- residual variances - cfi?
- item correlations high? cfi
- connecting findings to the related theories
- very related/ current literature first, than broader is possible
- 

## Limitations

- Which objective function was used?
- DIF?
- psych science - authors guide to generelizability
- attempts to control for limitating factors
- don't include to general/ broad critiques, but special one for my own study

The results of this study should be interpreted with several limitations in mind. The sample deviates from the general population in multiple demographic variables, potentially compromising its representativeness and generalizability. Occupational distribution among participants shows clustering in fields such as *"Gesundheit, Soziales, Lehre und Erziehung"*, *"Buchhaltung, Recht und Verwaltung"*, *"Kaufmännische Dienstleistungen, Vertrieb, Tourismus"* and especially *"Naturwissenschaft, Geografie und Informatik"*. This indicates a selection bias, likely due to recruitment methods (who is reached) and implicitly favoring individuals more interested in data literacy.

The item pool for the questionnaire was specifically trained on this non-representative sample, which will likely affect its validity. Because it could result in the creation and measurement of a latent construct, that is specific or unique even to this particular sample.
Ideally one would use this approach with a sample, representative of the general public or citizens. That way the results would be more valid and could more likely be generalized. 

The visual analysis of the items boxplots showed several outlier in the data. Those were left in the sample for further investigation, since based on the clustering in the high categories (4 and 5), it could be mean that the items were too easy for most respondents. This high concentration of responses at the upper end of the scale could indicate that the items did not adequately discriminate across a range of respondent abilities, resulting in those ceiling effects. The outliers in the low categories (0, 1, or 2) are the minority and are less likely to influence the overall conclusions. However, they do signal that a small subset of participants found the items unclear or disagreed with them and could hint at a systematic influence of some kind.
The measure was designed for citizens, potentially limiting discrimination at higher item difficulties or among more literate participants, a direction to be improved in future studies. 

Also, as data literacy is a heterogeneous construct, this complicates global instrument development and understanding across all participants. Ideally the questionnaire would incorporate a broader content to better reflect the constructs full scope, thereby increasing content validity. Expanding the questionnaire with additional items could address this need, although it would deviate from the principle of parsimony. 

These issues regarding the misfit of items and sample, as well as the heterogeneity of the construct further result in limitations regarding the reliability of the instrument. Although the total McDonald’s $\omega$ would indicate good reliability, the question of reliability stretches beyond a single measure for internal consistency. The reliability of a measure must be considered in the context of its target audience. The key question is: reliable for whom? It is essential to determine which sample the measure is based on and what population it makes claims about. And as highlighted above, the validity and generalizability of the measure depend heavily on how well the sample reflects the characteristics of the intended audience. 

When talking about the characteristics and demographics of the sample, the randomization of data literacy items among respondents introduced certain limitations as well. The primary limitation lies in the unequal demographic distribution across items. For example, while the overall sample may have an average age of 40 years, this demographic balance may not hold for each individual item. This issue extends to other demographic variables as well, resulting in the diversity of the sample not being consistently reflected in the responses to individual items. Consequently, the selected items would require further testing on a diverse sample to evaluate how demographic variability influences responses. The limited demographic balance in the items also results in limited control for confounding variables in those demographic variables. It furthermore limits the informative value of comparisons regarding the demographic variables among the items.  

Additionally, the training and testing data sets differed in size, which could influence measurement invariance testing [@chen2007]. While the sample sizes were appropriate, they were at the lower threshold of the prior power analysis [e.g., @kass1979; @hu1999], suggesting that larger samples might have been better. Lastly, although the questionnaire showed good model fit, it should be noted that algorithm-based item selection is a heuristic approach, rather than deterministic, and may not always yield the optimal solution[@schultze2017; @blum2003].
In this specific study the results of the algorithm based selection via the genetic algorithm in 'stuart' appeared to be unstable across different runs. I tried to account for that via the k-folding and multiple iterations of the selection, but when the process of the data imputation was changed, between mice and from mice to FIML, different items were selected, indicating unstable selections. This could potentially be accounted for, by using the bruteforce implementation in 'stuart'. But it also can be a hint at a model misspecification, because of which the algorithm finds multiple local minima.

Aside of the instability, especially when trying the imputation via the 'mice' package and prospective mean matching, the models yielded rather bad fit measures, when using the initially planned objective function (with the aim of optimising for model fit (RMSEA, SRMR, CFI), reliability (McDonald’s $\omega$), and variability in the difficulty of items (via the nu or tau matrix).
Which is why it was deviated from the initially planned objective function, in favor of model fit criteria only (RMSEA, SRMR & CFI). Thus, the final objective function, that was used, did not incorporate terms to optimize for reliability McDonald’s $\omega$ nor variability in the difficulty of items.
Furthermore WLSMV could not be used, although being the appropriate estimator for rating scales, because of the ceiling effects in the items and the sparely used lower answer categories. I tried to account for that by collapsing the respective answer categories, but this resulted seemingly in an introduction of better fit to the model. Because of that, I decided to use MLR as estimator and were therefore able to use FIML as imputation method, implemented in 'lavaan'.
Ideally future studies, with a sample without ceiling effects, would use WLSMV as estimator, since modified weighted least square estimators for ordered-categorical indicators (MWLS\textsubscript{C}) provide accurate estimates of the model parameters given a stable weight matrix [@wirth2007] and furthermore, WLSMV is a robust estimator which does not assume normally distributed variables and provides the best option for modelling categorical or ordered data [@brown2006]. 

######

- residuals correlate

## Future directions

- what to optimize the scale for?
- qualitative item creation should have been more extensive - maybe seperate study
- revisiting the model and specification, could open the possibility to include variability in item difficulty in the objective function, via the nu matrix, or the 

Future research should explore adaptive testing using Item Response Theory (IRT). IRT provides a method to tailor item difficulty to respondents' ability levels in real-time, enhancing assessment efficiency and precision. This reduces the number of items required while maintaining high measurement accuracy. Implementing IRT is particularly advantageous for heterogeneous constructs like data literacy, as it ensures each participant is evaluated with items suited to their skill level. One of the significant challenges in applying IRT is the assumption of unidimensionality, where items are presumed to measure a single underlying trait. Data literacy, however, is a multi-faceted construct, and future studies should investigate the dimensionality of the scale rigorously. 
An alternative to IRT-based adaptive testing is the use of Classification and Regression Trees (CART). CART is a tree-based method that splits data into subsets based on binary decisions, optimizing for predictive accuracy. This approach could simplify adaptive testing by using binary splits to classify respondents into different levels of data literacy. The Gini index can be employed within CART to identify the optimal cutoff points for these splits, ensuring that each branch of the tree maximally distinguishes between different levels of data literacy competence.


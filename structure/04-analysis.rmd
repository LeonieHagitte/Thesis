# Analysis

## Data Quality 
Careless or inattentive response patterns were assessed using attention-check items. Additionally, the data was visually inspected for outliers through boxplots. The data showed several outliers, that were left in the dataset. The outliers appear consistently across multiple items, which might suggest a systematic bias, where a subset of respondents consistently answers in the lower categories. This could be due to respondents finding the items too difficult or not engaging with the material in a meaningful way. Missing data, due to planned random omissions, was imputed using Full Information Maximum Likelihood (FIML) with 'stuart' in combination with 'lavaan' [@schultze2022; @rosseel2012]. FIML was also applied during model estimation to handle missing values. Aside from the planned random missings in the data literacy questions, there were non-planned missings due to incomplete questionnaire submissions. These non-planned missings were not imputed. For those participants who did not finish the questionnaire, data from their data literacy-related responses were still used in the training sample, as list-wise deletion would have resulted in too few data points for proper algorithm functioning.

## Main Analyses
Algorithm based item selection was done via the R package ‘stuart’[@schultze2022] and the confirmatory factor analysis (CFA) via the R package ‘lavaan’[@rosseel2012]. Inference criteria: Model fit was assessed using established criteria [@hu1999]. Comprising of Chi square significance testing as well as a combination of several fit indices, i.e., Root Mean Square Error of Approximation (RMSEA) < 0.05, Standardized Root Mean Square Residual (SRMR) < 0.07, Comparative Fit Index (CFI) > 0.95 [@hu1999]. Model-specific cutoff values were considered as well, using the ‘ezCutoffs’ package [@schmalbach2019].

### Rationale for Measurement Model
The design of the measurement model has to be chosen prior to item selection, which includes determining the number of items per factor in the final scale [@schultze2024]. This decision influences both the parsimonity and the evaluation of the model. The scale was designed to be as parsimonious as possible, while ensuring that a real model fit could be estimated. Statistically, a configuration with three items per factor would result in a just-identified model, leaving no degrees of freedom to test fit. To address this, four items per factor were chosen as the optimal configuration. This choice represents the most parsimonious structure that allows for the assessment of model fit.
Furthermore, the selection of four items per factor ensures that each facet can be independently utilized in practical applications while still providing 'local' indicators of fit. This design choice allows researchers to test each facet separately, without requiring the inclusion of additional constructs, thereby maintaining the flexibility and utility of the model across various contexts.

### Meta-Heuristics
Algorithm-based item selection was used to choose the most relevant items, reducing the initial item pool. In classical approaches, items are evaluated within the overall item pool and are then often selected based on their individual properties (e.g. difficulty, discrimination, item-scale-correlations).
Compared to classical approaches, algorithms are more objective and efficient in finding a good solution with regard to certain criteria [@leite2008; @olaru2015]. Furthermore, some empirical studies suggest that the use of algorithms leads to similar or better results in scale construction than traditional approaches [@sandy2014; @schroeders2016; @olaru2021]. The automated approach takes the opposite perspective to the classical approach, the one of meta heuristics, by repeatedly estimating CFAs for a multitude of possible item-combinations [@schultze2017]. Thus, a pool of items with some constraints and the goal to find the one combination that best fits the suggested purpose (e.g. equation 1) of the final scale is estimated [@schultze2017]. Hence, the selection of items and construction of a questionnaire can be viewed as a combinatorial problem, like the knapsack problem (“Choose a set of objects, each having a specific weight and monetary value, so that the value is maximized and the total weight does not exceed a predetermined limit”) [@schroeders2016, p. 4; @kerber2022; @schultze2017].
For this study, a set of 20 items from a set of 71 items was selected, to form a questionnaire. The data literacy self rating scale was optimized for model fit criteria (RMSEA, SRMR \& CFI). Those criteria were defined in the objective function (cf. Equations 1 & 2) in ‘stuart’[@schultze2022;@schultze2017]. Equation 1 is a conceptual expression indicating that $\Phi$ is a function of the fit indices RMSEA, SRMR, and CFI. It leaves the specific functional forms of $F$ undefined. Equation 2 specifies the exact transformations for these fit indices, where each $F()$ in the general formulation corresponds to a specific transformation in the computational form. This formulation reflects an objective where models with smaller RMSEA and SRMR values and higher CFI values are preferred in the optimization process.
\begin{center}
\hfill$\Phi = F(\textrm{RMSEA}) + F(\textrm{CFI}) + F(\textrm{SRMR})$ \hfill \textit{(1)}\\
\hfill$\Phi = (1 - \textrm{RMSEA}) + (1 - \textrm{SRMR}) + (1 + \textrm{CFI})$ \hfill \textit{(2)}
\end{center}

This objective function used deviated from the one that was planned in the preregistration, because the planned function did not result in acceptable model fits. Because of that, the demands on the algorithm were reduced solely in favor of model fit. 
I used the genetic algorithm of 'stuart' for the item selection. Genetic algorithms are based on Darwinian evolution principles – selection, crossover, mutation, and survival of the fittest [@holland1992; @schroeders2016]. With the genetic algorithm, the initial set of 71 items was reduced based on the evolutionary process of selection, but opposing to evolution with a goal: A near-optimal 'solution'. The survival of an item is determined by its quality (i.e. “fitness”) [@galan2013]. 

## Validation
The provided sample was split into two subsets (i.e. training data and test data) using the ‘holdout' function in ‘stuart’. The specified item-selection procedure is applied to the training data. The training data underwent k-fold cross validation ($k$=3), using the ‘kfold’ function in ‘stuart’. 
The number of folds ($k$) was determined based on the total sample size, ensuring that each fold was sufficiently large to support a CFA with adequate statistical power. To this end, a power analysis was conducted using the 'semPower' package in R [@moshagen2024] to calculate the required sample size per fold, which was determined to be $n$ = 128. This calculation ensured that dividing the training sample ($n$ = 373) into three folds would be the maximum feasible configuration.
Those $k$-folded selections were then again iterated three times, to enhance the stability of the solution even more. The other sample, the test data, is used for evaluation of the final model's performance, as well as the latent correlations with the convergent measures.
Validation with the test data is conducted (using an multi-group confirmatory factor analysis (MGCFA) in ‘lavaan’) to assess the invariance of the measurement models between the training and testing datasets. Invariance levels are assessed using the criteria of @chen2007. Invariance is necessary to claim that the scale validation has worked.

### Assessing the Nomological Net  
Criterion validity was examined through correlation analyses, using Kendall's rank correlation coefficient [@Kendall1938], between the data literacy factors and the latent factors from the other administered questionnaires. Bartlett scores [@Bartholomew2009] were used for the data literacy factors to account for substantial variation in factor loadings across items. For the factors from the other questionnaires, the respective scoring procedures outlined in their manuals were applied to ensure accurate computation of scores for the correlation analysis.

## Exploratory Analyses
Because of the complexity of the model, as well as the several objectives of the initially planned objective function, the genetic algorithm had problems converging into a stable solution, let alone a solution with sufficient model fit. Therefore, some objectives were dropped (the composite reliability and the varying item intercepts per factor), in favor of model fit. Different solutions were systematically explored, dependent on the estimator (Maximum likelihood with robust standard errors (MLR) or robust Weighted Least Squares (WLSMV) and the respective data structure (data treated as metric or ordinal data), as well as on the objective function, to lead to the best solution (cf. code in the supplementary Material ("Analysis.Rmd"; starting at line 2340)). Furthermore, the data was checked for multicollinearity because of convergence issues. 

### Analysis of Confounding Variables  
To check for possible external influences of the categorical, demographic variables (age; gender; highest educational level; the pursued degree, if studying & occupation), a dummy-coded multiple linear regression was calculated for every selected item. The reference categories for the categorical demographic variables (e.g., "Gender," "Education") are chosen automatically by R. For this analysis the 'lm()' function was used.By default, this function uses the first level of a factor variable as the reference category unless otherwise specified. 
